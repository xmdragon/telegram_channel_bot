"""
è¯­ä¹‰åˆ†æžå™¨ - åŒºåˆ†æ­£å¸¸å†…å®¹å’ŒæŽ¨å¹¿å†…å®¹
"""
import re
import logging
from typing import Tuple

logger = logging.getLogger(__name__)

class SemanticAnalyzer:
    """è¯­ä¹‰åˆ†æžå™¨ï¼Œç”¨äºŽåˆ¤æ–­æ–‡æœ¬çš„è¯­ä¹‰æ€§è´¨"""
    
    def __init__(self):
        # æ­£å¸¸å†…å®¹ç‰¹å¾
        self.normal_indicators = {
            # æ—¶é—´è¡¨è¾¾
            'time_expressions': [
                r'\d{4}å¹´\d{1,2}æœˆ', r'ç¬¬\d+å­£åº¦', r'\d+æœˆ\d+æ—¥',
                r'ä»Šå¹´', r'åŽ»å¹´', r'æœ¬æœˆ', r'ä¸Šæœˆ', r'è¿‘æœŸ', r'æœ€è¿‘'
            ],
            
            # æ•°æ®ç»Ÿè®¡
            'statistics': [
                r'\d+å', r'\d+ä¾‹', r'\d+èµ·', r'\d+ä»¶', r'\d+äºº',
                r'\d+%', r'\d+ä¸‡', r'\d+äº¿', r'\d+åƒ', r'çº¦\d+'
            ],
            
            # å®˜æ–¹æœºæž„
            'official_entities': [
                r'æ”¿åºœ', r'éƒ¨é—¨', r'å«ç”Ÿéƒ¨', r'æ•™è‚²éƒ¨', r'å…¬å®‰éƒ¨',
                r'æ³•é™¢', r'æ£€å¯Ÿé™¢', r'å§”å‘˜ä¼š', r'ç®¡ç†å±€',
                r'åŒ»é™¢', r'å­¦æ ¡', r'å¤§å­¦', r'ç ”ç©¶æ‰€'
            ],
            
            # äº‹ä»¶æè¿°
            'event_descriptions': [
                r'å‘ç”Ÿäº†', r'æŠ¥å‘Šæ˜¾ç¤º', r'æ®.*ç§°', r'æ¶ˆæ¯.*',
                r'è°ƒæŸ¥.*', r'å‘çŽ°.*', r'ç¡®è®¤.*', r'å®£å¸ƒ.*',
                r'è¡¨ç¤º.*', r'æŒ‡å‡º.*', r'å¼ºè°ƒ.*', r'è¦æ±‚.*'
            ],
            
            # æ–°é—»å…³é”®è¯
            'news_keywords': [
                r'æŠ¥é“', r'æ–°é—»', r'æ¶ˆæ¯', r'é€šæŠ¥', r'å…¬å‘Š',
                r'å£°æ˜Ž', r'é€šçŸ¥', r'å…¬å¸ƒ', r'å‘å¸ƒ'
            ]
        }
        
        # æŽ¨å¹¿å†…å®¹ç‰¹å¾
        self.promo_indicators = {
            # ç¥ˆä½¿å¥
            'imperatives': [
                r'è®¢é˜…', r'è¨‚é–±', r'å…³æ³¨', r'é—œæ³¨', r'åŠ å…¥',
                r'ç‚¹å‡»', r'æ‰«ç ', r'è”ç³»', r'è¯ç¹«', r'æŠ•ç¨¿'
            ],
            
            # è”ç³»æ–¹å¼
            'contact_info': [
                r'å¾®ä¿¡[:ï¼š]', r'QQ[:ï¼š]', r'ç”µè¯[:ï¼š]', r'æ‰‹æœº[:ï¼š]',
                r'@\w+', r't\.me/', r'telegram\.me/'
            ],
            
            # æŽ¨å¹¿ç”¨è¯
            'promo_words': [
                r'é¢‘é“', r'é »é“', r'ç¾¤ç»„', r'ç¾¤çµ„',
                r'å•†åŠ¡', r'å•†å‹™', r'åˆä½œ', r'ä»£ç†',
                r'çˆ†æ–™', r'æ›å…‰å°', r'èµ„è®¯'
            ],
            
            # çº¯ç¬¦å·
            'pure_symbols': [
                r'^[ðŸ“¢ðŸ“£ðŸ””ðŸ’¬â¤ï¸ðŸ”—â˜Žï¸ðŸ˜âœ‰ï¸ðŸ“®]+$',
                r'^[ðŸ‘‡â¬‡ï¸â†“â–¼â¤µï¸]+$',
                r'^[-=_â€”âž–â–ªâ–«â—†â—‡â– â–¡â—â—‹â€¢ï½ž~]{3,}$'
            ]
        }
    
    def analyze_content_semantics(self, text: str) -> Tuple[float, float]:
        """
        åˆ†æžæ–‡æœ¬è¯­ä¹‰
        
        Args:
            text: è¦åˆ†æžçš„æ–‡æœ¬
            
        Returns:
            (æ­£å¸¸å†…å®¹å¾—åˆ†, æŽ¨å¹¿å†…å®¹å¾—åˆ†)
        """
        logger.debug(f"ðŸ¤– è¯­ä¹‰åˆ†æžå™¨å¼€å§‹åˆ†æž - æ–‡æœ¬é•¿åº¦: {len(text) if text else 0}")
        if text:
            logger.debug(f"åˆ†æžæ–‡æœ¬: {text[:100]}{'...' if len(text) > 100 else ''}")
        
        if not text:
            logger.debug("æ–‡æœ¬ä¸ºç©ºï¼Œè¿”å›ž(0.0, 0.0)")
            return 0.0, 0.0
        
        # è®¡ç®—æ­£å¸¸å†…å®¹å¾—åˆ†
        normal_score = self._calculate_normal_score(text)
        logger.debug(f"æ­£å¸¸å†…å®¹å¾—åˆ†: {normal_score:.3f}")
        
        # è®¡ç®—æŽ¨å¹¿å†…å®¹å¾—åˆ†
        promo_score = self._calculate_promo_score(text)
        logger.debug(f"æŽ¨å¹¿å†…å®¹å¾—åˆ†: {promo_score:.3f}")
        
        logger.info(f"ðŸ“ˆ è¯­ä¹‰åˆ†æžç»“æžœ: æ­£å¸¸={normal_score:.3f}, æŽ¨å¹¿={promo_score:.3f}")
        return normal_score, promo_score
    
    def _calculate_normal_score(self, text: str) -> float:
        """è®¡ç®—æ­£å¸¸å†…å®¹å¾—åˆ†"""
        score = 0.0
        
        for category, patterns in self.normal_indicators.items():
            matches = 0
            for pattern in patterns:
                matches += len(re.findall(pattern, text, re.IGNORECASE))
            
            if matches > 0:
                if category == 'time_expressions':
                    score += min(matches * 2, 5)  # æ—¶é—´è¡¨è¾¾æƒé‡é«˜
                elif category == 'statistics':
                    score += min(matches * 3, 8)  # æ•°æ®ç»Ÿè®¡æƒé‡æœ€é«˜
                elif category == 'official_entities':
                    score += min(matches * 2, 4)
                elif category == 'event_descriptions':
                    score += min(matches * 1.5, 6)
                elif category == 'news_keywords':
                    score += min(matches * 1, 3)
        
        # æ£€æŸ¥å®Œæ•´å¥å­ç»“æž„
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]', text)
        complete_sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        if len(complete_sentences) > 0:
            score += min(len(complete_sentences) * 2, 6)
        
        return score
    
    def _calculate_promo_score(self, text: str) -> float:
        """è®¡ç®—æŽ¨å¹¿å†…å®¹å¾—åˆ†"""
        score = 0.0
        
        for category, patterns in self.promo_indicators.items():
            matches = 0
            for pattern in patterns:
                matches += len(re.findall(pattern, text, re.IGNORECASE))
            
            if matches > 0:
                if category == 'imperatives':
                    score += min(matches * 4, 10)  # ç¥ˆä½¿å¥æƒé‡æœ€é«˜
                elif category == 'contact_info':
                    score += min(matches * 3, 8)
                elif category == 'promo_words':
                    score += min(matches * 2, 6)
                elif category == 'pure_symbols':
                    score += min(matches * 1, 3)
        
        # æ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯é“¾æŽ¥å’Œç¬¦å·
        lines = text.split('\n')
        link_symbol_lines = 0
        for line in lines:
            line = line.strip()
            if not line:
                continue
            # æ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯é“¾æŽ¥ã€@ç”¨æˆ·åæˆ–ç¬¦å·
            if (re.search(r'^[ðŸ“¢ðŸ“£ðŸ””ðŸ’¬â¤ï¸ðŸ”—â˜Žï¸ðŸ˜âœ‰ï¸ðŸ“®@]', line) or
                re.search(r't\.me/', line) or
                re.search(r'^@\w+', line)):
                link_symbol_lines += 1
        
        if len(lines) > 0:
            link_ratio = link_symbol_lines / len(lines)
            if link_ratio > 0.5:  # è¶…è¿‡ä¸€åŠæ˜¯é“¾æŽ¥/ç¬¦å·è¡Œ
                score += 5
        
        return score
    
    def is_likely_normal_content(self, text: str, threshold_ratio: float = 1.2) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦æ›´å¯èƒ½æ˜¯æ­£å¸¸å†…å®¹
        
        Args:
            text: è¦åˆ†æžçš„æ–‡æœ¬
            threshold_ratio: æ­£å¸¸å†…å®¹å¾—åˆ†éœ€è¦è¶…è¿‡æŽ¨å¹¿å¾—åˆ†çš„å€æ•°
            
        Returns:
            True if likely normal content
        """
        normal_score, promo_score = self.analyze_content_semantics(text)
        
        logger.debug(f"è¯­ä¹‰åˆ†æž - æ­£å¸¸å¾—åˆ†: {normal_score:.1f}, æŽ¨å¹¿å¾—åˆ†: {promo_score:.1f}")
        
        # å¦‚æžœæŽ¨å¹¿å¾—åˆ†ä¸º0ï¼Œä¸”æ­£å¸¸å¾—åˆ†>0ï¼Œè®¤ä¸ºæ˜¯æ­£å¸¸å†…å®¹
        if promo_score == 0 and normal_score > 0:
            return True
        
        # å¦‚æžœæ­£å¸¸å¾—åˆ†æ˜Žæ˜¾é«˜äºŽæŽ¨å¹¿å¾—åˆ†ï¼Œè®¤ä¸ºæ˜¯æ­£å¸¸å†…å®¹
        if promo_score > 0:
            return normal_score > promo_score * threshold_ratio
        
        return False

# å…¨å±€å®žä¾‹
semantic_analyzer = SemanticAnalyzer()