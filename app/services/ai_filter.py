"""
AIæ™ºèƒ½è¿‡æ»¤æ¨¡å—
ä½¿ç”¨å¥å­åµŒå…¥å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯å®ç°æ™ºèƒ½çš„å¹¿å‘Šæ£€æµ‹å’Œå°¾éƒ¨è¿‡æ»¤
"""
import logging
import json
import numpy as np
from typing import Dict, List, Tuple, Optional
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN
from collections import defaultdict
import asyncio
from datetime import datetime, timedelta
import threading

logger = logging.getLogger(__name__)

class IntelligentFilter:
    """æ™ºèƒ½è¿‡æ»¤å™¨ - åŸºäºAIçš„å†…å®¹è¿‡æ»¤"""
    
    def __init__(self):
        self.model = None
        self.channel_patterns = {}  # å­˜å‚¨æ¯ä¸ªé¢‘é“çš„å°¾éƒ¨æ¨¡å¼
        self.ad_embeddings = []  # å¹¿å‘Šæ ·æœ¬çš„åµŒå…¥å‘é‡
        self.normal_embeddings = []  # æ­£å¸¸å†…å®¹çš„åµŒå…¥å‘é‡
        self.initialized = False
        self._lock = threading.RLock()  # ä¿æŠ¤å…±äº«æ•°æ®çš„é”
        
        # å°è¯•åŠ è½½æ¨¡å‹
        self._initialize()
        
        # å°è¯•åŠ è½½å·²ä¿å­˜çš„æ¨¡å¼
        if self.initialized:
            try:
                import os
                patterns_file = "data/ai_filter_patterns.json"
                if os.path.exists(patterns_file):
                    self.load_patterns(patterns_file)
                    logger.info(f"âœ… ä» {patterns_file} åŠ è½½äº†AIè¿‡æ»¤æ¨¡å¼")
            except Exception as e:
                logger.error(f"åŠ è½½AIè¿‡æ»¤æ¨¡å¼å¤±è´¥: {e}")
    
    def _initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            from sentence_transformers import SentenceTransformer
            # ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒä¸­æ–‡
            self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            self.initialized = True
            logger.info("âœ… AIè¿‡æ»¤å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            logger.warning("âš ï¸ sentence-transformers æœªå®‰è£…ï¼ŒAIè¿‡æ»¤åŠŸèƒ½æš‚ä¸å¯ç”¨")
        except Exception as e:
            logger.error(f"AIè¿‡æ»¤å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def learn_channel_pattern(self, channel_id: str, messages: List[str]) -> bool:
        """
        å­¦ä¹ ç‰¹å®šé¢‘é“çš„å°¾éƒ¨æ¨¡å¼
        
        Args:
            channel_id: é¢‘é“ID
            messages: è¯¥é¢‘é“çš„å†å²æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            æ˜¯å¦å­¦ä¹ æˆåŠŸ
        """
        if not self.initialized or not messages:
            return False
        
        try:
            # æ›´æ™ºèƒ½åœ°æå–å°¾éƒ¨å†…å®¹
            tails = []
            for msg in messages:
                tail = self._extract_real_tail(msg)
                if tail and len(tail) > 20:  # åªæ”¶é›†æœ‰æ•ˆçš„å°¾éƒ¨
                    tails.append(tail)
            
            # åŠ¨æ€åˆ¤æ–­æ ·æœ¬æ•°é‡ - ä¸å†å›ºå®šè¦æ±‚10ä¸ª
            min_samples = min(5, len(tails))  # æœ€å°‘5ä¸ªæ ·æœ¬ï¼Œæˆ–è€…æ‰€æœ‰å¯ç”¨æ ·æœ¬
            if len(tails) < min_samples:
                logger.info(f"é¢‘é“ {channel_id} æ ·æœ¬ä¸è¶³ï¼ˆ{len(tails)}ä¸ªï¼‰ï¼Œè·³è¿‡å­¦ä¹ ")
                return False
            
            # è®¡ç®—å°¾éƒ¨çš„åµŒå…¥å‘é‡
            logger.info(f"æ­£åœ¨å­¦ä¹ é¢‘é“ {channel_id} çš„å°¾éƒ¨æ¨¡å¼...")
            tail_embeddings = self.model.encode(tails)
            
            # ä½¿ç”¨DBSCANèšç±»æ‰¾å‡ºé‡å¤çš„æ¨¡å¼
            # åŠ¨æ€è°ƒæ•´èšç±»å‚æ•°ï¼šæ ·æœ¬å°‘æ—¶é™ä½è¦æ±‚
            min_cluster_size = max(2, len(tails) // 5)  # è‡³å°‘å 20%çš„æ ·æœ¬
            clustering = DBSCAN(eps=0.3, min_samples=min_cluster_size, metric='cosine')
            labels = clustering.fit_predict(tail_embeddings)
            
            # æ‰¾å‡ºæœ€å¤§çš„èšç±»ï¼ˆæœ€å¸¸è§çš„å°¾éƒ¨æ¨¡å¼ï¼‰
            label_counts = defaultdict(int)
            for label in labels:
                if label != -1:  # -1 è¡¨ç¤ºå™ªå£°ç‚¹
                    label_counts[label] += 1
            
            if not label_counts:
                logger.info(f"é¢‘é“ {channel_id} æ²¡æœ‰å‘ç°é‡å¤çš„å°¾éƒ¨æ¨¡å¼ï¼ˆå¯èƒ½æ— å›ºå®šå°¾éƒ¨ï¼‰")
                return False
            
            # è·å–æœ€å¤§èšç±»çš„ä¸­å¿ƒå‘é‡
            max_label = max(label_counts, key=label_counts.get)
            cluster_indices = [i for i, l in enumerate(labels) if l == max_label]
            cluster_embeddings = tail_embeddings[cluster_indices]
            
            # å­˜å‚¨è¯¥é¢‘é“çš„å°¾éƒ¨æ¨¡å¼ï¼ˆä½¿ç”¨èšç±»ä¸­å¿ƒï¼‰
            with self._lock:
                self.channel_patterns[channel_id] = {
                    'centroid': np.mean(cluster_embeddings, axis=0),
                    'samples': cluster_embeddings[:5],  # ä¿å­˜å‡ ä¸ªæ ·æœ¬ç”¨äºéªŒè¯
                    'threshold': 0.75,  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    'learned_at': datetime.now().isoformat(),
                    'sample_count': len(cluster_indices)
                }
            
            logger.info(f"âœ… é¢‘é“ {channel_id} å°¾éƒ¨æ¨¡å¼å­¦ä¹ å®Œæˆï¼Œå‘ç° {len(cluster_indices)} ä¸ªç›¸ä¼¼æ ·æœ¬")
            return True
            
        except Exception as e:
            logger.error(f"å­¦ä¹ é¢‘é“ {channel_id} æ¨¡å¼å¤±è´¥: {e}")
            return False
    
    def is_channel_tail(self, channel_id: str, text: str) -> Tuple[bool, float]:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºç‰¹å®šé¢‘é“çš„å°¾éƒ¨å†…å®¹
        
        Args:
            channel_id: é¢‘é“ID
            text: è¦æ£€æŸ¥çš„æ–‡æœ¬
            
        Returns:
            (æ˜¯å¦ä¸ºå°¾éƒ¨, ç›¸ä¼¼åº¦åˆ†æ•°)
        """
        if not self.initialized:
            return False, 0.0
        
        with self._lock:
            if channel_id not in self.channel_patterns:
                return False, 0.0
            pattern = self.channel_patterns[channel_id].copy()
        
        try:
            text_embedding = self.model.encode([text])[0]
            
            # è®¡ç®—ä¸é¢‘é“å°¾éƒ¨æ¨¡å¼çš„ç›¸ä¼¼åº¦
            similarity = cosine_similarity(
                text_embedding.reshape(1, -1),
                pattern['centroid'].reshape(1, -1)
            )[0][0]
            
            is_tail = similarity >= pattern['threshold']
            return is_tail, float(similarity)
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥å°¾éƒ¨å†…å®¹å¤±è´¥: {e}")
            return False, 0.0
    
    def _extract_real_tail(self, content: str) -> str:
        """
        æ™ºèƒ½æå–æ¶ˆæ¯çš„çœŸæ­£å°¾éƒ¨ï¼ˆæ¨å¹¿å†…å®¹ï¼‰
        
        Args:
            content: æ¶ˆæ¯å†…å®¹
            
        Returns:
            çœŸæ­£çš„å°¾éƒ¨å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        import re
        lines = content.split('\n')
        if len(lines) < 3:
            return ""
        
        # å¯»æ‰¾æ˜ç¡®çš„åˆ†éš”æ ‡å¿—
        separator_patterns = [
            r'^[-=_â€”â–â–ªâ–«â—†â—‡â– â–¡â—â—‹â€¢ï½~]{5,}$',  # ç¬¦å·åˆ†éš”çº¿
            r'^[ğŸ“¢ğŸ“£ğŸ””ğŸ’¬â¤ï¸ğŸ”—ğŸ”âœ‰ï¸ğŸ“®ğŸ˜]*\s*$',  # è¡¨æƒ…åˆ†éš”
            r'^\s*[-=]{3,}\s*$',  # ç®€å•åˆ†éš”çº¿
        ]
        
        # æ¨å¹¿å†…å®¹çš„ç‰¹å¾
        promo_indicators = [
            r'https?://',  # é“¾æ¥
            r'@[a-zA-Z][a-zA-Z0-9_]{4,}',  # Telegramç”¨æˆ·å
            r't\.me/',  # Telegramé“¾æ¥
            r'(?:è®¢é˜…|é—œæ³¨|æŠ•ç¨¿|å•†åŠ¡|è”ç³»|å¤±è”|å¯¼èˆª)',  # æ¨å¹¿å…³é”®è¯
            r'\[.*\]\(.*\)',  # Markdowné“¾æ¥
        ]
        
        # ä»åå‘å‰æŸ¥æ‰¾åˆ†éš”ç¬¦
        separator_index = -1
        for i in range(len(lines) - 1, max(0, len(lines) - 15), -1):
            line = lines[i].strip()
            # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†éš”ç¬¦
            for pattern in separator_patterns:
                if re.match(pattern, line):
                    # éªŒè¯åˆ†éš”ç¬¦åé¢æ˜¯å¦æœ‰æ¨å¹¿å†…å®¹
                    has_promo = False
                    for j in range(i + 1, min(i + 5, len(lines))):
                        for promo in promo_indicators:
                            if re.search(promo, lines[j], re.IGNORECASE):
                                has_promo = True
                                break
                        if has_promo:
                            break
                    
                    if has_promo:
                        separator_index = i
                        break
            
            if separator_index != -1:
                break
        
        # å¦‚æœæ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œè¿”å›åˆ†éš”ç¬¦ä¹‹åçš„å†…å®¹
        if separator_index != -1:
            tail = '\n'.join(lines[separator_index + 1:])
            return tail.strip()
        
        # å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œå¯»æ‰¾æ¨å¹¿å†…å®¹çš„èµ·å§‹ä½ç½®
        promo_start = -1
        for i in range(len(lines) - 1, max(0, len(lines) - 10), -1):
            line = lines[i]
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªæ¨å¹¿ç‰¹å¾
            promo_count = sum(1 for p in promo_indicators if re.search(p, line, re.IGNORECASE))
            if promo_count >= 2:  # è‡³å°‘2ä¸ªæ¨å¹¿ç‰¹å¾
                promo_start = i
        
        # å¦‚æœæ‰¾åˆ°æ¨å¹¿å†…å®¹ï¼Œè¿”å›ä»é‚£é‡Œå¼€å§‹çš„å†…å®¹
        if promo_start != -1 and promo_start < len(lines) - 1:
            tail = '\n'.join(lines[promo_start:])
            return tail.strip()
        
        # å¦‚æœæ—¢æ²¡æœ‰åˆ†éš”ç¬¦ä¹Ÿæ²¡æœ‰æ˜æ˜¾çš„æ¨å¹¿å†…å®¹ï¼Œè¿”å›ç©º
        return ""
    
    def filter_channel_tail(self, channel_id: str, content: str) -> str:
        """
        è¿‡æ»¤æ‰é¢‘é“ç‰¹å®šçš„å°¾éƒ¨å†…å®¹ - æ™ºèƒ½ç‰ˆæœ¬
        
        Args:
            channel_id: é¢‘é“ID
            content: åŸå§‹å†…å®¹
            
        Returns:
            è¿‡æ»¤åçš„å†…å®¹
        """
        # ä¼˜å…ˆä½¿ç”¨è§„åˆ™æ£€æµ‹ï¼Œæ‰¾åˆ°æ˜ç¡®çš„å°¾éƒ¨è¾¹ç•Œ
        rule_based_result = self._filter_by_rules(content)
        if rule_based_result != content:
            logger.info(f"è§„åˆ™æ£€æµ‹è¿‡æ»¤äº†å°¾éƒ¨: {len(content)} -> {len(rule_based_result)} å­—ç¬¦")
            return rule_based_result
        
        # å¦‚æœè§„åˆ™æ— æ³•åˆ¤æ–­ï¼Œæ‰ä½¿ç”¨AIæ¨¡å‹
        if not self.initialized:
            return content
        
        with self._lock:
            has_pattern = channel_id in self.channel_patterns
        
        if not has_pattern:
            return content
        
        lines = content.split('\n')
        if len(lines) <= 3:
            return content
        
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„è¾¹ç•Œæ£€æµ‹
        tail_boundary = self._find_tail_boundary(content, channel_id)
        
        if tail_boundary != -1 and tail_boundary < len(lines):
            filtered = '\n'.join(lines[:tail_boundary])
            keep_ratio = tail_boundary / len(lines)
            logger.info(f"AIæ£€æµ‹è¿‡æ»¤é¢‘é“ {channel_id} å°¾éƒ¨: {len(content)} -> {len(filtered)} å­—ç¬¦ (ä¿ç•™{keep_ratio*100:.0f}%)")
            return filtered.strip()
        
        return content
    
    def _filter_by_rules(self, content: str) -> str:
        """
        ä½¿ç”¨è§„åˆ™ä¼˜å…ˆè¿‡æ»¤å°¾éƒ¨æ¨å¹¿å†…å®¹
        
        Args:
            content: åŸå§‹å†…å®¹
            
        Returns:
            è¿‡æ»¤åçš„å†…å®¹
        """
        import re
        lines = content.split('\n')
        
        # åˆ†éš”ç¬¦æ¨¡å¼
        separator_patterns = [
            r'^[-=_â€”â–â–ªâ–«â—†â—‡â– â–¡â—â—‹â€¢ï½~]{5,}$',
            r'^[ğŸ“¢ğŸ“£ğŸ””ğŸ’¬â¤ï¸ğŸ”—ğŸ”âœ‰ï¸ğŸ“®ğŸ˜]{2,}.*$',
            r'^\s*[-=]{3,}\s*$',
        ]
        
        # æ¨å¹¿å†…å®¹ç‰¹å¾
        promo_indicators = [
            r'\[.*\]\(https?://.*\)',  # Markdowné“¾æ¥
            r't\.me/[a-zA-Z][a-zA-Z0-9_]{4,}',  # Telegramé“¾æ¥
            r'@[a-zA-Z][a-zA-Z0-9_]{4,}',  # Telegramç”¨æˆ·å
            r'(?:è®¢é˜…|é—œæ³¨|æŠ•ç¨¿|å•†åŠ¡|è”ç³»|å¤±è”|å¯¼èˆª).*(?:@|t\.me/)',  # æ¨å¹¿è¯+é“¾æ¥
        ]
        
        # ä»åå‘å‰æŸ¥æ‰¾åˆ†éš”ç¬¦
        for i in range(len(lines) - 1, max(0, len(lines) - 15), -1):
            line = lines[i].strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†éš”ç¬¦
            is_separator = any(re.match(p, line) for p in separator_patterns)
            
            if is_separator:
                # éªŒè¯åˆ†éš”ç¬¦åé¢æ˜¯å¦æœ‰æ¨å¹¿å†…å®¹
                has_promo_after = False
                for j in range(i + 1, min(i + 5, len(lines))):
                    if any(re.search(p, lines[j], re.IGNORECASE) for p in promo_indicators):
                        has_promo_after = True
                        break
                
                if has_promo_after:
                    # æ‰¾åˆ°äº†çœŸæ­£çš„å°¾éƒ¨è¾¹ç•Œ
                    return '\n'.join(lines[:i]).strip()
        
        return content
    
    def _find_tail_boundary(self, content: str, channel_id: str) -> int:
        """
        ä½¿ç”¨AIæ¨¡å‹æ™ºèƒ½æŸ¥æ‰¾å°¾éƒ¨è¾¹ç•Œ
        
        Args:
            content: æ¶ˆæ¯å†…å®¹
            channel_id: é¢‘é“ID
            
        Returns:
            å°¾éƒ¨å¼€å§‹çš„è¡Œå·ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å›-1
        """
        lines = content.split('\n')
        
        # åªåœ¨æœ€å30%çš„å†…å®¹ä¸­æŸ¥æ‰¾å°¾éƒ¨
        search_start = max(0, int(len(lines) * 0.7))
        
        # ä»åå‘å‰æ£€æŸ¥ï¼Œä½†é™åˆ¶èŒƒå›´
        for i in range(len(lines) - 1, search_start, -1):
            # æ£€æŸ¥ä»ç¬¬iè¡Œåˆ°ç»“å°¾çš„å†…å®¹
            test_tail = '\n'.join(lines[i:])
            is_tail, score = self.is_channel_tail(channel_id, test_tail)
            
            # æé«˜é˜ˆå€¼ï¼Œé¿å…è¯¯åˆ¤
            if is_tail and score > 0.85:
                # åŒå‘éªŒè¯
                # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨å¹¿å†…å®¹
                if not self._contains_promo_content(test_tail):
                    continue  # ä¸åŒ…å«æ¨å¹¿å†…å®¹ï¼Œè·³è¿‡
                
                # 2. æ£€æŸ¥å‰é¢çš„å†…å®¹æ˜¯å¦æ˜¯æ­£æ–‡
                if i > 0:
                    test_main = '\n'.join(lines[:i])
                    if self._is_main_content(test_main):
                        # å‰é¢ç¡®å®æ˜¯æ­£æ–‡ï¼Œè¿™é‡Œæ˜¯åˆç†çš„è¾¹ç•Œ
                        return i
                    else:
                        # å‰é¢ä¸åƒæ­£æ–‡ï¼Œå¯èƒ½æ•´ä¸ªéƒ½æ˜¯æ¨å¹¿ï¼Œç»§ç»­å‘å‰æŸ¥æ‰¾
                        continue
                else:
                    # å¦‚æœi=0ï¼Œæ„å‘³ç€è¦åˆ é™¤æ•´ä¸ªå†…å®¹ï¼Œéœ€è¦ç‰¹åˆ«è°¨æ…
                    if self._is_main_content(test_tail):
                        # å†…å®¹åŒ…å«æ­£æ–‡ç‰¹å¾ï¼Œä¸åº”è¯¥å…¨éƒ¨åˆ é™¤
                        return -1
                    else:
                        # ç¡®å®éƒ½æ˜¯æ¨å¹¿å†…å®¹
                        return i
        
        return -1
    
    def _contains_promo_content(self, text: str) -> bool:
        """
        æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æ¨å¹¿å†…å®¹ç‰¹å¾
        
        Args:
            text: è¦æ£€æŸ¥çš„æ–‡æœ¬
            
        Returns:
            æ˜¯å¦åŒ…å«æ¨å¹¿å†…å®¹
        """
        import re
        promo_patterns = [
            r'https?://',
            r't\.me/',
            r'@[a-zA-Z][a-zA-Z0-9_]{4,}',
            r'(?:è®¢é˜…|é—œæ³¨|æŠ•ç¨¿|å•†åŠ¡|è”ç³»)',
            r'\[.*\]\(.*\)',
        ]
        
        # è‡³å°‘åŒ…å«2ä¸ªæ¨å¹¿ç‰¹å¾æ‰è®¤ä¸ºæ˜¯æ¨å¹¿å†…å®¹
        promo_count = sum(1 for p in promo_patterns if re.search(p, text, re.IGNORECASE))
        return promo_count >= 2
    
    def _is_main_content(self, text: str) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦æ˜¯æ­£æ–‡å†…å®¹ï¼ˆåŒå‘éªŒè¯ï¼‰
        
        Args:
            text: è¦æ£€æŸ¥çš„æ–‡æœ¬
            
        Returns:
            æ˜¯å¦æ˜¯æ­£æ–‡å†…å®¹
        """
        import re
        
        # æ­£æ–‡å†…å®¹çš„ç‰¹å¾
        content_indicators = [
            # å™äº‹æ€§å†…å®¹
            r'(?:æˆ‘|ä½ |ä»–|å¥¹|å®ƒ|ä»¬|å’±|ä¿º|æ‚¨)',  # äººç§°ä»£è¯
            r'(?:äº†|ç€|è¿‡|çš„|åœ°|å¾—)',  # åŠ©è¯
            r'(?:æ˜¯|æœ‰|åœ¨|åˆ°|å»|æ¥|è¯´|åš|çœ‹|æƒ³|è¦)',  # å¸¸ç”¨åŠ¨è¯
            r'(?:ä»Šå¤©|æ˜¨å¤©|æ˜å¤©|ç°åœ¨|å½“æ—¶|åæ¥|ç„¶å)',  # æ—¶é—´è¯
            r'(?:å› ä¸º|æ‰€ä»¥|ä½†æ˜¯|å¯æ˜¯|å¦‚æœ|è™½ç„¶)',  # è¿è¯
            
            # æƒ…æ„Ÿè¡¨è¾¾
            r'(?:å–œæ¬¢|è®¨åŒ|é«˜å…´|éš¾è¿‡|ç”Ÿæ°”|å®³æ€•|å¸Œæœ›)',
            r'(?:ğŸ˜Š|ğŸ˜‚|ğŸ˜­|ğŸ˜|ğŸ˜¤|ğŸ˜±|ğŸ¤”|ğŸ’”)',  # æƒ…æ„Ÿè¡¨æƒ…
            
            # æ•…äº‹æ€§å†…å®¹
            r'(?:æ•…äº‹|ç»å†|å‘ç”Ÿ|é‡åˆ°|å‘ç°|è®°å¾—|æ›¾ç»)',
            r'(?:ç¬¬ä¸€|ç¬¬äºŒ|é¦–å…ˆ|å…¶æ¬¡|æœ€å|ç»ˆäº)',
            
            # è§‚ç‚¹è¡¨è¾¾
            r'(?:è®¤ä¸º|è§‰å¾—|æ„Ÿè§‰|å»ºè®®|åº”è¯¥|å¯èƒ½|ä¹Ÿè®¸)',
        ]
        
        # è®¡ç®—æ­£æ–‡ç‰¹å¾æ•°é‡
        content_score = 0
        for pattern in content_indicators:
            if re.search(pattern, text, re.IGNORECASE):
                content_score += 1
        
        # å¦‚æœåŒ…å«å¤šä¸ªæ­£æ–‡ç‰¹å¾ï¼Œè®¤ä¸ºæ˜¯æ­£æ–‡
        if content_score >= 3:
            return True
        
        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦å’Œå¥å­ç»“æ„
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        long_sentences = [s for s in sentences if len(s) > 20]
        
        # å¦‚æœæœ‰å¤šä¸ªé•¿å¥å­ï¼Œå¯èƒ½æ˜¯æ­£æ–‡
        if len(long_sentences) >= 2:
            return True
        
        return False
    
    async def train_ad_classifier(self, ad_samples: List[str], normal_samples: List[str]):
        """
        è®­ç»ƒå¹¿å‘Šåˆ†ç±»å™¨
        
        Args:
            ad_samples: å¹¿å‘Šæ ·æœ¬åˆ—è¡¨
            normal_samples: æ­£å¸¸å†…å®¹æ ·æœ¬åˆ—è¡¨
        """
        if not self.initialized:
            logger.warning("AIè¿‡æ»¤å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•è®­ç»ƒ")
            return
        
        try:
            logger.info(f"å¼€å§‹è®­ç»ƒå¹¿å‘Šåˆ†ç±»å™¨: {len(ad_samples)} ä¸ªå¹¿å‘Šæ ·æœ¬, {len(normal_samples)} ä¸ªæ­£å¸¸æ ·æœ¬")
            
            # è®¡ç®—åµŒå…¥å‘é‡
            ad_emb = self.model.encode(ad_samples) if ad_samples else []
            normal_emb = self.model.encode(normal_samples) if normal_samples else []
            
            with self._lock:
                if len(ad_emb) > 0:
                    self.ad_embeddings = ad_emb
                if len(normal_emb) > 0:
                    self.normal_embeddings = normal_emb
            
            logger.info("âœ… å¹¿å‘Šåˆ†ç±»å™¨è®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            logger.error(f"è®­ç»ƒå¹¿å‘Šåˆ†ç±»å™¨å¤±è´¥: {e}")
    
    def is_advertisement(self, text: str) -> Tuple[bool, float]:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºå¹¿å‘Š
        
        Args:
            text: è¦æ£€æŸ¥çš„æ–‡æœ¬
            
        Returns:
            (æ˜¯å¦ä¸ºå¹¿å‘Š, ç½®ä¿¡åº¦)
        """
        if not self.initialized:
            return False, 0.0
        
        with self._lock:
            ad_emb_copy = self.ad_embeddings.copy() if len(self.ad_embeddings) > 0 else []
            normal_emb_copy = self.normal_embeddings.copy() if len(self.normal_embeddings) > 0 else []
        
        if len(ad_emb_copy) == 0 and len(normal_emb_copy) == 0:
            return False, 0.0
        
        try:
            text_embedding = self.model.encode([text])[0].reshape(1, -1)
            
            # è®¡ç®—ä¸å¹¿å‘Šæ ·æœ¬çš„ç›¸ä¼¼åº¦
            ad_similarity = 0.0
            if len(ad_emb_copy) > 0:
                ad_similarities = cosine_similarity(text_embedding, ad_emb_copy)
                ad_similarity = np.max(ad_similarities)
            
            # è®¡ç®—ä¸æ­£å¸¸å†…å®¹çš„ç›¸ä¼¼åº¦
            normal_similarity = 0.0
            if len(normal_emb_copy) > 0:
                normal_similarities = cosine_similarity(text_embedding, normal_emb_copy)
                normal_similarity = np.max(normal_similarities)
            
            # å¦‚æœæ›´åƒå¹¿å‘Šï¼Œåˆ™åˆ¤å®šä¸ºå¹¿å‘Š
            is_ad = ad_similarity > normal_similarity and ad_similarity > 0.7
            confidence = float(ad_similarity) if is_ad else float(1 - ad_similarity)
            
            return is_ad, confidence
            
        except Exception as e:
            logger.error(f"å¹¿å‘Šæ£€æµ‹å¤±è´¥: {e}")
            return False, 0.0
    
    def save_patterns(self, filepath: str):
        """ä¿å­˜å­¦ä¹ çš„æ¨¡å¼åˆ°æ–‡ä»¶"""
        try:
            data = {
                'channel_patterns': {},
                'ad_sample_count': len(self.ad_embeddings),
                'normal_sample_count': len(self.normal_embeddings),
                'saved_at': datetime.now().isoformat()
            }
            
            # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            with self._lock:
                for channel_id, pattern in self.channel_patterns.items():
                    data['channel_patterns'][channel_id] = {
                        'centroid': pattern['centroid'].tolist(),
                        'threshold': pattern['threshold'],
                        'learned_at': pattern['learned_at'],
                        'sample_count': pattern['sample_count']
                    }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ¨¡å¼å·²ä¿å­˜åˆ° {filepath}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å¼å¤±è´¥: {e}")
    
    def load_patterns(self, filepath: str):
        """ä»æ–‡ä»¶åŠ è½½å­¦ä¹ çš„æ¨¡å¼"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ¢å¤numpyæ•°ç»„
            with self._lock:
                for channel_id, pattern_data in data['channel_patterns'].items():
                    self.channel_patterns[channel_id] = {
                        'centroid': np.array(pattern_data['centroid']),
                        'threshold': pattern_data['threshold'],
                        'learned_at': pattern_data['learned_at'],
                        'sample_count': pattern_data['sample_count'],
                        'samples': []  # åŠ è½½æ—¶ä¸æ¢å¤æ ·æœ¬
                    }
            
            logger.info(f"ä» {filepath} åŠ è½½äº† {len(self.channel_patterns)} ä¸ªé¢‘é“æ¨¡å¼")
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å¼å¤±è´¥: {e}")


# å…¨å±€å®ä¾‹
ai_filter = IntelligentFilter()