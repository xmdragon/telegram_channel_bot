"""
æ•´åˆçš„æ¶ˆæ¯é‡å¤æ£€æµ‹æœåŠ¡
ä¼˜å…ˆåª’ä½“å“ˆå¸Œè·¨é¢‘é“æ£€æµ‹ï¼Œå…¶æ¬¡jiebaæ–‡æœ¬ç›¸ä¼¼åº¦æ£€æµ‹
"""
import warnings
# æŠ‘åˆ¶jiebaçš„pkg_resourceså¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="jieba._compat")
warnings.filterwarnings("ignore", message=".*pkg_resources is deprecated.*")

import hashlib
import re
import logging
from typing import Optional, List, Dict, Tuple
from datetime import datetime, timedelta
from difflib import SequenceMatcher
import jieba
from sqlalchemy import select, and_, or_
from sqlalchemy.ext.asyncio import AsyncSession
from app.core.database import AsyncSessionLocal, Message

# å¯¼å…¥è§†è§‰ç›¸ä¼¼åº¦æ£€æµ‹å™¨
try:
    from app.services.visual_similarity import visual_detector
except ImportError:
    visual_detector = None

logger = logging.getLogger(__name__)

class DuplicateDetector:
    """æ•´åˆçš„æ¶ˆæ¯é‡å¤æ£€æµ‹å™¨ï¼šåª’ä½“å“ˆå¸Œ + jiebaæ–‡æœ¬ç›¸ä¼¼åº¦"""
    
    def __init__(self):
        # åª’ä½“æ£€æµ‹å‚æ•°ï¼ˆå¢åŠ æ£€æµ‹çª—å£ï¼‰
        self.media_cache_hours = 72  # åª’ä½“æ£€æµ‹72å°æ—¶çª—å£
        
        # æ–‡æœ¬æ£€æµ‹å‚æ•°ï¼ˆè°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼å’Œæ—¶é—´çª—å£ï¼‰
        self.text_similarity_threshold = 0.75  # 75%ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆæ›´ä¸¥æ ¼é¿å…è¯¯åˆ¤ï¼‰
        self.text_time_window_minutes = 2880  # 48å°æ—¶æ—¶é—´çª—å£ (2880åˆ†é’Ÿ)
        
        # æ–‡æœ¬æ¸…ç†æ­£åˆ™è¡¨è¾¾å¼
        self.common_tags = [
            r'#\w+',  # æ ‡ç­¾
            r'@\w+',  # ç”¨æˆ·å/é¢‘é“å
            r'https?://[^\s]+',  # é“¾æ¥
            r't\.me/[^\s]+',  # Telegramé“¾æ¥
            r'(?:^|\s)è®¢é˜….{0,20}(?:$|\s)',  # è®¢é˜…ç›¸å…³æ–‡å­—
            r'(?:^|\s)æŠ•ç¨¿.{0,20}(?:$|\s)',  # æŠ•ç¨¿ç›¸å…³æ–‡å­—
            r'(?:^|\s)è”ç³».{0,20}(?:$|\s)',  # è”ç³»æ–¹å¼
            r'ğŸ“¢|ğŸ“£|ğŸ“¡|ğŸ|ğŸ’°|ğŸ”¥|â¤|ğŸ˜Š|ğŸ˜|ğŸ‘‡',  # å¸¸è§è¡¨æƒ…
        ]
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.tag_patterns = [re.compile(pattern) for pattern in self.common_tags]
    
    def calculate_media_hash(self, media_data: bytes) -> str:
        """è®¡ç®—åª’ä½“æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
        return hashlib.sha256(media_data).hexdigest()
    
    def calculate_combined_hash(self, media_list: List[Dict]) -> str:
        """è®¡ç®—ç»„åˆåª’ä½“çš„å“ˆå¸Œå€¼"""
        # å°†æ‰€æœ‰åª’ä½“çš„å“ˆå¸Œå€¼ç»„åˆèµ·æ¥
        combined = ""
        for media in sorted(media_list, key=lambda x: x.get('index', 0)):
            if media.get('hash'):
                combined += media['hash']
        
        if combined:
            return hashlib.sha256(combined.encode()).hexdigest()
        return None
    
    async def is_duplicate_message(self, 
                                  source_channel: str,
                                  media_hash: Optional[str] = None, 
                                  combined_media_hash: Optional[str] = None,
                                  content: Optional[str] = None,
                                  message_time: Optional[datetime] = None,
                                  message_id: Optional[int] = None,
                                  media_data: Optional[bytes] = None,
                                  visual_hashes: Optional[dict] = None,
                                  db: Optional[AsyncSession] = None) -> Tuple[bool, Optional[int], str]:
        """
        æ•´åˆçš„é‡å¤æ¶ˆæ¯æ£€æµ‹ï¼šä¼˜å…ˆè§†è§‰ç›¸ä¼¼åº¦ï¼Œå…¶æ¬¡åª’ä½“å“ˆå¸Œï¼Œæœ€åjiebaæ–‡æœ¬ç›¸ä¼¼åº¦
        
        Args:
            source_channel: æºé¢‘é“
            media_hash: å•ä¸ªåª’ä½“çš„å“ˆå¸Œå€¼
            combined_media_hash: ç»„åˆåª’ä½“çš„å“ˆå¸Œå€¼
            content: æ¶ˆæ¯æ–‡æœ¬å†…å®¹
            message_time: æ¶ˆæ¯æ—¶é—´
            message_id: æ¶ˆæ¯ID
            media_data: åª’ä½“æ–‡ä»¶çš„äºŒè¿›åˆ¶æ•°æ®ï¼ˆç”¨äºè§†è§‰ç›¸ä¼¼åº¦æ£€æµ‹ï¼‰
            visual_hashes: é¢„è®¡ç®—çš„è§†è§‰å“ˆå¸Œå€¼
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            (is_duplicate, original_message_id, duplicate_type)
        """
        if message_time is None:
            message_time = datetime.utcnow()
        # ç¡®ä¿æ—¶é—´æ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼ˆnaive datetimeï¼‰
        if hasattr(message_time, 'tzinfo') and message_time.tzinfo is not None:
            message_time = message_time.replace(tzinfo=None)
        
        # æœ€ä¼˜å…ˆè¿›è¡Œè§†è§‰ç›¸ä¼¼åº¦æ£€æµ‹ï¼ˆå¦‚æœæœ‰å›¾ç‰‡æ•°æ®ï¼‰
        if visual_detector and (media_data or visual_hashes):
            logger.debug(f"å¼€å§‹è§†è§‰ç›¸ä¼¼åº¦æ£€æµ‹ï¼Œæ£€æµ‹çª—å£: 96å°æ—¶")
            is_visual_dup, orig_id, similarity = await self._check_visual_duplicate(
                media_data, visual_hashes, message_time, message_id, db
            )
            if is_visual_dup:
                logger.info(f"âœ… æ£€æµ‹åˆ°è§†è§‰ç›¸ä¼¼å›¾ç‰‡ï¼Œç›¸ä¼¼åº¦: {similarity:.1f}%ï¼ŒåŸæ¶ˆæ¯ID: {orig_id}")
                return True, orig_id, "visual"
            else:
                logger.debug(f"è§†è§‰ç›¸ä¼¼åº¦æ£€æµ‹æœªå‘ç°é‡å¤")
            
        # å…¶æ¬¡è¿›è¡Œåª’ä½“å“ˆå¸Œæ£€æµ‹ï¼ˆè·¨é¢‘é“ï¼‰
        if media_hash or combined_media_hash:
            logger.debug(f"å¼€å§‹åª’ä½“å“ˆå¸Œæ£€æµ‹ï¼Œæ£€æµ‹çª—å£: {self.media_cache_hours}å°æ—¶")
            is_media_dup, orig_id = await self._check_media_duplicate(
                media_hash, combined_media_hash, message_time, message_id, db
            )
            if is_media_dup:
                logger.info(f"âœ… æ£€æµ‹åˆ°åª’ä½“å“ˆå¸Œé‡å¤ï¼ŒåŸæ¶ˆæ¯ID: {orig_id}")
                return True, orig_id, "media"
            else:
                logger.debug(f"åª’ä½“å“ˆå¸Œæ£€æµ‹æœªå‘ç°é‡å¤")
        
        # å…¶æ¬¡è¿›è¡Œæ–‡æœ¬ç›¸ä¼¼åº¦æ£€æµ‹ï¼ˆè·¨é¢‘é“ï¼‰
        if content and content.strip():
            logger.debug(f"å¼€å§‹æ–‡æœ¬ç›¸ä¼¼åº¦æ£€æµ‹ï¼Œé˜ˆå€¼: {self.text_similarity_threshold:.0%}ï¼Œæ£€æµ‹çª—å£: {self.text_time_window_minutes//60}å°æ—¶")
            is_text_dup, orig_id = await self._check_text_duplicate(
                content, source_channel, message_time, message_id, db
            )
            if is_text_dup:
                logger.info(f"âœ… æ£€æµ‹åˆ°æ–‡æœ¬ç›¸ä¼¼é‡å¤ï¼ŒåŸæ¶ˆæ¯ID: {orig_id}")
                return True, orig_id, "text"
            else:
                logger.debug(f"æ–‡æœ¬ç›¸ä¼¼åº¦æ£€æµ‹æœªå‘ç°é‡å¤ï¼ˆæ£€æŸ¥äº†{len(content.strip())}å­—ç¬¦çš„å†…å®¹ï¼‰")
        
        logger.debug(f"âœ… å»é‡æ£€æµ‹å®Œæˆï¼Œæœªå‘ç°é‡å¤")
        return False, None, "none"
        
        # å¦‚æœæ²¡æœ‰ä¼ å…¥dbä¼šè¯ï¼Œåˆ›å»ºä¸€ä¸ª
        use_external_db = db is not None
        if not use_external_db:
            db = AsyncSessionLocal()
        
        try:
            result = await self.is_duplicate_message(
                source_channel, media_hash, combined_media_hash, content, message_time, db
            )
            return result[0]  # åªè¿”å›æ˜¯å¦é‡å¤
        except Exception as e:
            logger.error(f"æ£€æµ‹é‡å¤æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            return False
        finally:
            if not use_external_db:
                await db.close()
    
    async def _check_visual_duplicate(self, media_data: Optional[bytes],
                                     visual_hashes: Optional[dict],
                                     message_time: datetime,
                                     message_id: Optional[int] = None,
                                     db: Optional[AsyncSession] = None) -> Tuple[bool, Optional[int], float]:
        """
        æ£€æŸ¥è§†è§‰ç›¸ä¼¼åº¦é‡å¤
        
        Args:
            media_data: åª’ä½“æ–‡ä»¶æ•°æ®
            visual_hashes: é¢„è®¡ç®—çš„è§†è§‰å“ˆå¸Œ
            message_time: æ¶ˆæ¯æ—¶é—´
            message_id: å½“å‰æ¶ˆæ¯ID
            db: æ•°æ®åº“ä¼šè¯
            
        Returns:
            (æ˜¯å¦é‡å¤, åŸå§‹æ¶ˆæ¯ID, ç›¸ä¼¼åº¦åˆ†æ•°)
        """
        if not visual_detector:
            return False, None, 0.0
        
        # å¦‚æœæœ‰åª’ä½“æ•°æ®ä½†æ²¡æœ‰è§†è§‰å“ˆå¸Œï¼Œå…ˆè®¡ç®—
        if media_data and not visual_hashes:
            visual_hashes = visual_detector.calculate_perceptual_hashes(media_data)
        
        if not visual_hashes:
            return False, None, 0.0
        
        use_external_db = db is not None
        if not use_external_db:
            db = AsyncSessionLocal()
        
        try:
            # ç¡®ä¿æ—¶é—´æ²¡æœ‰æ—¶åŒºä¿¡æ¯
            if hasattr(message_time, 'tzinfo') and message_time.tzinfo is not None:
                message_time = message_time.replace(tzinfo=None)
            
            # æŸ¥è¯¢æ—¶é—´èŒƒå›´å†…çš„æ¶ˆæ¯ï¼ˆæ‰©å¤§è§†è§‰ç›¸ä¼¼åº¦æ£€æµ‹çª—å£ï¼‰
            time_threshold = message_time - timedelta(hours=96)  # 96å°æ—¶çª—å£
            
            conditions = [
                Message.created_at >= time_threshold,
                Message.status != "rejected",
                Message.visual_hash.isnot(None)  # åªæŸ¥è¯¢æœ‰è§†è§‰å“ˆå¸Œçš„æ¶ˆæ¯
            ]
            
            # æ’é™¤å½“å‰æ¶ˆæ¯
            if message_id is not None:
                conditions.append(Message.id != message_id)
            
            result = await db.execute(
                select(Message).where(and_(*conditions)).order_by(Message.created_at.desc())
            )
            messages = result.scalars().all()
            
            # æ£€æŸ¥æ¯ä¸ªå†å²æ¶ˆæ¯çš„è§†è§‰ç›¸ä¼¼åº¦
            for msg in messages:
                try:
                    stored_hashes = eval(msg.visual_hash)  # å°†å­—ç¬¦ä¸²è½¬æ¢å›å­—å…¸
                    is_similar, similarity = visual_detector.is_visually_similar(visual_hashes, stored_hashes)
                    if is_similar:
                        logger.info(f"å‘ç°è§†è§‰ç›¸ä¼¼å›¾ç‰‡ï¼Œæ¶ˆæ¯ID: {msg.id}, ç›¸ä¼¼åº¦: {similarity:.1f}%")
                        return True, msg.id, similarity
                except Exception as e:
                    logger.debug(f"æ¯”è¾ƒè§†è§‰å“ˆå¸Œæ—¶å‡ºé”™: {e}")
            
            return False, None, 0.0
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥è§†è§‰é‡å¤æ—¶å‡ºé”™: {e}")
            return False, None, 0.0
        finally:
            if not use_external_db:
                await db.close()
    
    async def _check_media_duplicate(self, media_hash: Optional[str], 
                                    combined_media_hash: Optional[str],
                                    message_time: datetime,
                                    message_id: Optional[int] = None,
                                    db: Optional[AsyncSession] = None) -> Tuple[bool, Optional[int]]:
        """æ£€æŸ¥åª’ä½“é‡å¤ï¼ˆè·¨é¢‘é“ï¼‰"""
        if not media_hash and not combined_media_hash:
            return False, None
            
        use_external_db = db is not None
        if not use_external_db:
            db = AsyncSessionLocal()
            
        try:
            # ç¡®ä¿æ—¶é—´æ²¡æœ‰æ—¶åŒºä¿¡æ¯
            if hasattr(message_time, 'tzinfo') and message_time.tzinfo is not None:
                message_time = message_time.replace(tzinfo=None)
            
            # æŸ¥è¯¢æ—¶é—´èŒƒå›´ï¼ˆè·¨æ‰€æœ‰é¢‘é“ï¼‰
            time_threshold = message_time - timedelta(hours=self.media_cache_hours)
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            conditions = [
                Message.created_at >= time_threshold,
                Message.status != "rejected"  # ä¸è€ƒè™‘å·²æ‹’ç»çš„æ¶ˆæ¯
            ]
            
            # æ’é™¤å½“å‰æ¶ˆæ¯æœ¬èº«
            if message_id is not None:
                conditions.append(Message.id != message_id)
            
            # åª’ä½“å“ˆå¸ŒåŒ¹é…æ¡ä»¶
            hash_conditions = []
            if media_hash:
                hash_conditions.append(Message.media_hash == media_hash)
            if combined_media_hash:
                hash_conditions.append(Message.combined_media_hash == combined_media_hash)
            
            if hash_conditions:
                conditions.append(or_(*hash_conditions))
            
            # æ‰§è¡ŒæŸ¥è¯¢
            result = await db.execute(
                select(Message).where(and_(*conditions))
            )
            existing_messages = result.scalars().all()
            
            if existing_messages:
                logger.info(f"æ£€æµ‹åˆ°åª’ä½“é‡å¤: ä¸æ¶ˆæ¯ID {existing_messages[0].id} çš„åª’ä½“ç›¸åŒ")
                return True, existing_messages[0].id
                
            return False, None
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥åª’ä½“é‡å¤æ—¶å‡ºé”™: {e}")
            return False, None
        finally:
            if not use_external_db:
                await db.close()
    
    async def _check_text_duplicate(self, content: str, source_channel: str,
                                   message_time: datetime,
                                   message_id: Optional[int] = None,
                                   db: Optional[AsyncSession] = None) -> Tuple[bool, Optional[int]]:
        """æ£€æŸ¥æ–‡æœ¬é‡å¤ï¼ˆè·¨é¢‘é“ï¼Œä½¿ç”¨jiebaåˆ†è¯ï¼‰"""
        use_external_db = db is not None
        if not use_external_db:
            db = AsyncSessionLocal()
            
        try:
            # ç¡®ä¿æ—¶é—´æ²¡æœ‰æ—¶åŒºä¿¡æ¯
            if hasattr(message_time, 'tzinfo') and message_time.tzinfo is not None:
                message_time = message_time.replace(tzinfo=None)
            
            # è®¾ç½®æ—¶é—´çª—å£
            time_start = message_time - timedelta(minutes=self.text_time_window_minutes)
            time_end = message_time + timedelta(minutes=self.text_time_window_minutes)
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            conditions = [
                Message.created_at >= time_start,
                Message.created_at <= time_end,
                Message.content.isnot(None),  # æœ‰æ–‡æœ¬å†…å®¹
                Message.status != "rejected"  # éæ‹’ç»æ¶ˆæ¯
            ]
            
            # æ’é™¤å½“å‰æ¶ˆæ¯æœ¬èº«
            if message_id is not None:
                conditions.append(Message.id != message_id)
            
            # æŸ¥è¯¢æ—¶é—´çª—å£å†…çš„æ‰€æœ‰æ¶ˆæ¯ï¼ˆåŒ…æ‹¬åŒé¢‘é“ï¼‰
            result = await db.execute(
                select(Message).where(and_(*conditions)).order_by(Message.created_at.desc())
            )
            recent_messages = result.scalars().all()
            
            # æ£€æŸ¥ç›¸ä¼¼åº¦
            for msg in recent_messages:
                if not msg.content:
                    continue
                
                # è®¡ç®—å¤šç§ç›¸ä¼¼åº¦
                text_similarity = self._calculate_text_similarity(content, msg.content)
                jieba_similarity = self._calculate_jieba_similarity(content, msg.content)
                
                # å–æœ€é«˜ç›¸ä¼¼åº¦
                max_similarity = max(text_similarity, jieba_similarity)
                
                logger.debug(f"ç›¸ä¼¼åº¦æ£€æŸ¥: {max_similarity:.2f} (æ–‡æœ¬: {text_similarity:.2f}, jieba: {jieba_similarity:.2f})")
                
                if max_similarity >= self.text_similarity_threshold:
                    logger.info(f"å‘ç°æ–‡æœ¬é‡å¤æ¶ˆæ¯ï¼Œç›¸ä¼¼åº¦: {max_similarity:.2f}")
                    return True, msg.id
            
            return False, None
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ–‡æœ¬é‡å¤æ—¶å‡ºé”™: {e}")
            return False, None
        finally:
            if not use_external_db:
                await db.close()
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤æ ‡ç­¾ã€é“¾æ¥ç­‰å¹²æ‰°å› ç´ """
        if not text:
            return ""
        
        cleaned = text
        
        # ç§»é™¤æ‰€æœ‰æ ‡ç­¾å’Œé“¾æ¥
        for pattern in self.tag_patterns:
            cleaned = pattern.sub(' ', cleaned)
        
        # å»é™¤å¤šä½™ç©ºç™½
        cleaned = ' '.join(cleaned.split())
        
        return cleaned.strip()
    
    def _extract_core_content(self, text: str) -> str:
        """æå–æ ¸å¿ƒå†…å®¹ï¼ˆä¸»è¦æ˜¯æ–°é—»å†…å®¹éƒ¨åˆ†ï¼‰"""
        if not text:
            return ""
        
        # æŒ‰è¡Œåˆ†å‰²
        lines = text.split('\n')
        
        # æ‰¾åˆ°ä¸»è¦å†…å®¹ï¼ˆé€šå¸¸æ˜¯æœ€é•¿çš„æ®µè½ï¼‰
        main_content = []
        for line in lines:
            line = line.strip()
            if len(line) > 50:  # è¶…è¿‡50ä¸ªå­—ç¬¦çš„è¡Œå¯èƒ½æ˜¯ä¸»è¦å†…å®¹
                main_content.append(line)
        
        # å¦‚æœæ²¡æœ‰é•¿æ®µè½ï¼Œä½¿ç”¨å‰å‡ è¡Œ
        if not main_content:
            main_content = [line.strip() for line in lines[:5] if line.strip()]
        
        return '\n'.join(main_content)
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨SequenceMatcherï¼‰"""
        if not text1 or not text2:
            return 0.0
        
        # æ¸…ç†æ–‡æœ¬
        clean_text1 = self._clean_text(text1)
        clean_text2 = self._clean_text(text2)
        
        # å¦‚æœæ¸…ç†åçš„æ–‡æœ¬å¤ªçŸ­ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬çš„æ ¸å¿ƒå†…å®¹
        if len(clean_text1) < 20 or len(clean_text2) < 20:
            clean_text1 = self._clean_text(self._extract_core_content(text1))
            clean_text2 = self._clean_text(self._extract_core_content(text2))
        
        return SequenceMatcher(None, clean_text1, clean_text2).ratio()
    
    def _calculate_jieba_similarity(self, text1: str, text2: str) -> float:
        """ä½¿ç”¨jiebaåˆ†è¯å’Œå“ˆå¸Œè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå¯¹ä¸­æ–‡æ›´å‹å¥½ï¼‰"""
        if not text1 or not text2:
            return 0.0
        
        # æå–æ ¸å¿ƒå†…å®¹
        core1 = self._extract_core_content(text1)
        core2 = self._extract_core_content(text2)
        
        # åˆ†è¯
        words1 = set(jieba.cut(self._clean_text(core1)))
        words2 = set(jieba.cut(self._clean_text(core2)))
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        words1 = {w for w in words1 if len(w) > 1}
        words2 = {w for w in words2 if len(w) > 1}
        
        if not words1 or not words2:
            return 0.0
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def _is_text_similar(self, text1: Optional[str], text2: Optional[str], threshold: float = 0.8) -> bool:
        """ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        if not text1 or not text2:
            # å¦‚æœéƒ½ä¸ºç©ºï¼Œè®¤ä¸ºç›¸ä¼¼
            return not text1 and not text2
        
        # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼šåŸºäºå…±åŒå­—ç¬¦çš„æ¯”ä¾‹
        text1 = text1.strip().lower()
        text2 = text2.strip().lower()
        
        if text1 == text2:
            return True
        
        # ä½¿ç”¨æ–°çš„jiebaç›¸ä¼¼åº¦ç®—æ³•
        jieba_sim = self._calculate_jieba_similarity(text1, text2)
        text_sim = self._calculate_text_similarity(text1, text2)
        return max(jieba_sim, text_sim) >= threshold
    
    async def get_similar_messages(self, 
                                  source_channel: str,
                                  media_hash: Optional[str] = None,
                                  content: Optional[str] = None,
                                  hours: int = 24) -> List[Message]:
        """
        è·å–ç›¸ä¼¼çš„å†å²æ¶ˆæ¯ï¼ˆæ”¯æŒåª’ä½“å’Œæ–‡æœ¬æ£€ç´¢ï¼‰
        
        Args:
            source_channel: æºé¢‘é“
            media_hash: åª’ä½“å“ˆå¸Œå€¼
            content: æ–‡æœ¬å†…å®¹
            hours: æŸ¥è¯¢å¤šå°‘å°æ—¶å†…çš„æ¶ˆæ¯
            
        Returns:
            ç›¸ä¼¼æ¶ˆæ¯åˆ—è¡¨
        """
        if not media_hash and not content:
            return []
        
        try:
            async with AsyncSessionLocal() as db:
                time_threshold = datetime.utcnow() - timedelta(hours=hours)
                
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                conditions = [
                    Message.created_at >= time_threshold,
                    Message.status != "rejected"
                ]
                
                # æ·»åŠ åª’ä½“æˆ–æ–‡æœ¬æ¡ä»¶
                search_conditions = []
                if media_hash:
                    search_conditions.extend([
                        Message.media_hash == media_hash,
                        Message.combined_media_hash == media_hash
                    ])
                
                if search_conditions:
                    conditions.append(or_(*search_conditions))
                
                result = await db.execute(
                    select(Message).where(and_(*conditions)).order_by(Message.created_at.desc())
                )
                
                messages = result.scalars().all()
                
                # å¦‚æœæœ‰æ–‡æœ¬å†…å®¹ï¼Œè¿›ä¸€æ­¥è¿‡æ»¤ç›¸ä¼¼æ–‡æœ¬
                if content and not media_hash:
                    similar_messages = []
                    for msg in messages:
                        if msg.content:
                            similarity = max(
                                self._calculate_text_similarity(content, msg.content),
                                self._calculate_jieba_similarity(content, msg.content)
                            )
                            if similarity >= 0.5:  # 50%ä»¥ä¸Šç›¸ä¼¼åº¦
                                similar_messages.append(msg)
                    return similar_messages
                
                return messages
                
        except Exception as e:
            logger.error(f"è·å–ç›¸ä¼¼æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            return []
    
    async def mark_as_duplicate(self, message_id: int, original_message_id: int):
        """
        å°†æ¶ˆæ¯æ ‡è®°ä¸ºé‡å¤
        
        Args:
            message_id: é‡å¤æ¶ˆæ¯çš„ID
            original_message_id: åŸå§‹æ¶ˆæ¯çš„ID
        """
        try:
            async with AsyncSessionLocal() as db:
                result = await db.execute(
                    select(Message).where(Message.id == message_id)
                )
                message = result.scalar_one_or_none()
                
                if message:
                    message.status = "rejected"
                    # é‡è¦ï¼šå»é‡ä¸æ„å‘³ç€æ˜¯å¹¿å‘Šï¼Œä¸è¦æ”¹å˜is_adçŠ¶æ€
                    message.filtered_content = f"[é‡å¤æ¶ˆæ¯ï¼ŒåŸæ¶ˆæ¯ID: {original_message_id}]"
                    message.review_time = datetime.utcnow()
                    message.reviewed_by = "DuplicateDetector"
                    
                    await db.commit()
                    logger.info(f"æ¶ˆæ¯ {message_id} å·²è¢«æ ‡è®°ä¸ºé‡å¤")
                    
        except Exception as e:
            logger.error(f"æ ‡è®°é‡å¤æ¶ˆæ¯æ—¶å‡ºé”™: {e}")