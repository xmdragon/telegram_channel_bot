"""
åŸºäºè¯­ä¹‰çš„æ™ºèƒ½å°¾éƒ¨è¿‡æ»¤å™¨
é€šè¿‡ç†è§£æ–‡æœ¬è¯­ä¹‰æ¥åˆ¤æ–­æ˜¯å¦ä¸ºæ¨å¹¿å°¾éƒ¨
"""

import re
import logging
from typing import Dict, Set, Optional, Tuple

logger = logging.getLogger(__name__)


class SemanticTailFilter:
    """åŸºäºè¯­ä¹‰çš„æ™ºèƒ½å°¾éƒ¨è¿‡æ»¤å™¨"""
    
    def __init__(self):
        # æ¨å¹¿ç›¸å…³çš„åŠ¨è¯å’ŒçŸ­è¯­
        self.promo_verbs = {
            'è®¢é˜…', 'åŠ å…¥', 'å…³æ³¨', 'æŠ•ç¨¿', 'è”ç³»', 'ç‚¹å‡»', 
            'æ·»åŠ ', 'æ‰«ç ', 'çˆ†æ–™', 'æ¾„æ¸…', 'åˆä½œ', 'å¯¹æ¥',
            'å’¨è¯¢', 'æŠ¥å', 'é¢†å–', 'å…è´¹', 'æ¬¢è¿'
        }
        
        # é¢‘é“/ç¾¤ç»„æ ‡è¯†
        self.channel_indicators = {
            'é¢‘é“', 'ç¾¤ç»„', 'ç¾¤èŠ', 'äº¤æµç¾¤', 'è®¨è®ºç¾¤', 
            'å®˜æ–¹', 'å®¢æœ', 'å•†åŠ¡', 'æ‹›å•†', 'å¤±è”å¯¼èˆª',
            'æŠ•ç¨¿', 'çˆ†æ–™', 'æ¾„æ¸…', 'åˆä½œ'
        }
        
        # è¡ŒåŠ¨å·å¬æ¨¡å¼
        self.cta_patterns = [
            r'æ¬¢è¿.{0,5}æŠ•ç¨¿',
            r'æ¬¢è¿.{0,5}çˆ†æ–™',
            r'å•†åŠ¡.{0,5}åˆä½œ', 
            r'å…è´¹.{0,5}çˆ†æ–™',
            r'ç‚¹å‡».{0,5}åŠ å…¥',
            r'æ‰«ç .{0,5}æ·»åŠ ',
            r'è®¢é˜….{0,5}é¢‘é“',
            r'å…³æ³¨.{0,5}æˆ‘ä»¬',
            r'è”ç³».{0,5}å®¢æœ',
            r'æ·»åŠ .{0,5}å¾®ä¿¡',
            r'è¿›ç¾¤.{0,5}äº¤æµ'
        ]
        
        # ç™½åå•ï¼šè¿™äº›è¯å‡ºç°æ—¶é™ä½å°¾éƒ¨åˆ¤å®šæ¦‚ç‡
        self.whitelist_terms = {
            # å­¦æœ¯å’Œå¼•ç”¨ç›¸å…³
            'å‚è€ƒæ–‡çŒ®', 'æ³¨é‡Š', 'æ¥æº', 'å¼•ç”¨', 'å‡ºå¤„', 'èµ„æ–™',
            # é€»è¾‘è¿æ¥è¯
            'å› æ­¤', 'æ‰€ä»¥', 'æ€»ä¹‹', 'ç»¼ä¸Šæ‰€è¿°', 'ç»“è®º', 'æ€»ç»“',
            'ç”±æ­¤å¯è§', 'æ¢å¥è¯è¯´', 'ä¹Ÿå°±æ˜¯è¯´', 'ç®€è€Œè¨€ä¹‹',
            # å†…å®¹å»¶ç»­
            'å¦‚ä¸‹', 'ä»¥ä¸‹', 'ä¸‹é¢', 'æ¥ä¸‹æ¥', 'ç»§ç»­',
            # è§£é‡Šè¯´æ˜
            'ä¾‹å¦‚', 'æ¯”å¦‚', 'è­¬å¦‚', 'ä¸¾ä¾‹', 'è¯´æ˜'
        }
        
        # å¼ºæ¨å¹¿ä¿¡å·è¯
        self.strong_promo_signals = {
            'å®˜æ–¹é¢‘é“', 'å®˜æ–¹ç¾¤', 'è®¢é˜…é¢‘é“', 'åŠ å…¥ç¾¤ç»„',
            'æŠ•ç¨¿çˆ†æ–™', 'å•†åŠ¡åˆä½œ', 'å…è´¹é¢†å–', 'ç‚¹å‡»é¢†å–',
            'æ‰«ç æ·»åŠ ', 'è”ç³»å®¢æœ', 'æ‹›å•†ä»£ç†'
        }
    
    def calculate_semantic_score(self, text: str, full_content: Optional[str] = None) -> float:
        """
        è®¡ç®—è¯­ä¹‰å¾—åˆ†ï¼ˆ0-1ï¼‰
        
        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬ï¼ˆå¯èƒ½çš„å°¾éƒ¨ï¼‰
            full_content: å®Œæ•´å†…å®¹ï¼ˆç”¨äºè®¡ç®—ç›¸å…³æ€§ï¼‰
            
        Returns:
            è¯­ä¹‰å¾—åˆ†ï¼Œè¶Šé«˜è¶Šå¯èƒ½æ˜¯æ¨å¹¿å°¾éƒ¨
        """
        if not text:
            return 0.0
            
        score = 0.0
        text_lower = text.lower()
        
        # 1. å¼ºä¿¡å·æ£€æµ‹ï¼ˆæƒé‡0.4ï¼‰
        strong_signal_count = sum(1 for signal in self.strong_promo_signals if signal in text)
        if strong_signal_count > 0:
            score += min(0.4, strong_signal_count * 0.2)
            logger.debug(f"å¼ºä¿¡å·å¾—åˆ†: {min(0.4, strong_signal_count * 0.2)}")
        
        # 2. æ¨å¹¿åŠ¨è¯æ£€æµ‹ï¼ˆæƒé‡0.25ï¼‰
        verb_count = sum(1 for verb in self.promo_verbs if verb in text)
        if verb_count > 0:
            verb_score = min(0.25, verb_count * 0.08)
            score += verb_score
            logger.debug(f"åŠ¨è¯å¾—åˆ†: {verb_score}")
        
        # 3. è¡ŒåŠ¨å·å¬(CTA)æ£€æµ‹ï¼ˆæƒé‡0.2ï¼‰
        cta_count = sum(1 for pattern in self.cta_patterns if re.search(pattern, text))
        if cta_count > 0:
            cta_score = min(0.2, cta_count * 0.1)
            score += cta_score
            logger.debug(f"CTAå¾—åˆ†: {cta_score}")
        
        # 4. é¢‘é“æ ‡è¯†æ£€æµ‹ï¼ˆæƒé‡0.15ï¼‰
        channel_count = sum(1 for term in self.channel_indicators if term in text)
        if channel_count > 0:
            channel_score = min(0.15, channel_count * 0.05)
            score += channel_score
            logger.debug(f"é¢‘é“æ ‡è¯†å¾—åˆ†: {channel_score}")
        
        # 5. ç™½åå•æƒ©ç½šï¼ˆå‡åˆ†ï¼‰
        whitelist_count = sum(1 for term in self.whitelist_terms if term in text)
        if whitelist_count > 0:
            penalty = min(0.3, whitelist_count * 0.1)
            score -= penalty
            logger.debug(f"ç™½åå•æƒ©ç½š: -{penalty}")
        
        # 6. è”ç³»æ–¹å¼å’Œé“¾æ¥æ£€æµ‹ï¼ˆæƒé‡0.3ï¼‰
        contact_patterns = [
            (r'@\w+', 0.1),           # Telegramç”¨æˆ·å
            (r't\.me/\w+', 0.15),     # Telegramé“¾æ¥
            (r'https?://t\.me/', 0.15), # å®Œæ•´Telegramé“¾æ¥
            (r'https?://', 0.05),     # å…¶ä»–é“¾æ¥
            (r'å¾®ä¿¡[:ï¼š]', 0.08),      # å¾®ä¿¡
            (r'QQ[:ï¼š]', 0.05)        # QQ
        ]
        
        contact_score = 0.0
        lines = text.split('\n')
        
        for pattern, weight in contact_patterns:
            matches = len(re.findall(pattern, text, re.IGNORECASE))
            if matches > 0:
                contact_score += min(weight * 2, matches * weight)
        
        # é¢å¤–åŠ åˆ†ï¼šå¤šç§è”ç³»æ–¹å¼å¹¶å­˜
        unique_patterns = sum(1 for pattern, _ in contact_patterns if re.search(pattern, text, re.IGNORECASE))
        if unique_patterns >= 2:
            contact_score += 0.1
            logger.debug(f"å¤šç§è”ç³»æ–¹å¼åŠ åˆ†: 0.1")
        
        score += min(0.3, contact_score)
        if contact_score > 0:
            logger.debug(f"è”ç³»æ–¹å¼å¾—åˆ†: {min(0.3, contact_score):.3f}")
        
        # 7. ä¸»é¢˜ç›¸å…³æ€§ï¼ˆå¦‚æœæä¾›äº†å®Œæ•´å†…å®¹ï¼‰
        if full_content:
            relevance = self.calculate_relevance(text, full_content)
            # ç›¸å…³æ€§è¶Šä½ï¼Œè¶Šå¯èƒ½æ˜¯å°¾éƒ¨ï¼ˆæƒé‡0.2ï¼‰
            relevance_score = (1 - relevance) * 0.2
            score += relevance_score
            logger.debug(f"ç›¸å…³æ€§å¾—åˆ†: {relevance_score} (ç›¸å…³æ€§: {relevance:.2f})")
        
        # ç¡®ä¿å¾—åˆ†åœ¨0-1èŒƒå›´å†…
        final_score = max(0, min(1, score))
        logger.debug(f"æœ€ç»ˆè¯­ä¹‰å¾—åˆ†: {final_score:.3f}")
        
        return final_score
    
    def calculate_relevance(self, tail: str, full_content: str) -> float:
        """
        è®¡ç®—å°¾éƒ¨ä¸æ­£æ–‡çš„ç›¸å…³æ€§ï¼ˆ0-1ï¼‰
        
        Args:
            tail: å°¾éƒ¨å†…å®¹
            full_content: å®Œæ•´å†…å®¹
            
        Returns:
            ç›¸å…³æ€§å¾—åˆ†ï¼Œè¶Šé«˜è¯´æ˜è¶Šç›¸å…³ï¼ˆä¸å¤ªå¯èƒ½æ˜¯æ¨å¹¿ï¼‰
        """
        if not tail or not full_content:
            return 0.5  # æ— æ³•åˆ¤æ–­æ—¶è¿”å›ä¸­æ€§å€¼
        
        # è·å–ä¸»è¦å†…å®¹ï¼ˆå»æ‰å°¾éƒ¨ï¼‰
        main_content = full_content.replace(tail, '').strip()
        if not main_content:
            return 0.5
        
        # æå–ä¸»è¦å†…å®¹çš„å…³é”®è¯ï¼ˆä¸­æ–‡è¯ç»„ï¼‰
        main_words = re.findall(r'[\u4e00-\u9fa5]{2,4}', main_content)
        if not main_words:
            return 0.5
        
        # è®¡ç®—è¯é¢‘
        main_word_freq = {}
        for word in main_words:
            # è¿‡æ»¤æ‰å¤ªå¸¸è§çš„è¯
            if word not in {'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'ç­‰', 'è¿™', 'é‚£', 'æœ‰', 'æ— '}:
                main_word_freq[word] = main_word_freq.get(word, 0) + 1
        
        if not main_word_freq:
            return 0.5
        
        # è·å–é«˜é¢‘è¯ï¼ˆå‰10ä¸ªï¼‰
        sorted_words = sorted(main_word_freq.items(), key=lambda x: x[1], reverse=True)
        top_words = [word for word, _ in sorted_words[:10]]
        
        # æ£€æŸ¥å°¾éƒ¨åŒ…å«å¤šå°‘é«˜é¢‘è¯
        tail_words = set(re.findall(r'[\u4e00-\u9fa5]{2,4}', tail))
        common_count = sum(1 for word in top_words if word in tail_words)
        
        # è®¡ç®—ç›¸å…³æ€§
        relevance = common_count / len(top_words) if top_words else 0
        
        # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœå°¾éƒ¨åŒ…å«æ–°é—»/æ–‡ç« çš„æ ¸å¿ƒä¸»é¢˜è¯ï¼Œæé«˜ç›¸å…³æ€§
        # æ¯”å¦‚æ­£æ–‡è®²"æŸ¬åŸ”å¯¨"ï¼Œå°¾éƒ¨ä¹Ÿæåˆ°"æŸ¬åŸ”å¯¨"ï¼Œå¯èƒ½æ˜¯ç›¸å…³å†…å®¹
        if sorted_words and sorted_words[0][1] > 5:  # æœ€é«˜é¢‘è¯å‡ºç°è¶…è¿‡5æ¬¡
            top_theme = sorted_words[0][0]
            if top_theme in tail:
                relevance = min(1.0, relevance + 0.3)
        
        return relevance
    
    def detect_topic_switch(self, main_content: str, tail: str) -> bool:
        """
        æ£€æµ‹æ˜¯å¦å­˜åœ¨ä¸»é¢˜åˆ‡æ¢
        
        Args:
            main_content: ä¸»è¦å†…å®¹
            tail: å°¾éƒ¨å†…å®¹
            
        Returns:
            Trueè¡¨ç¤ºæ£€æµ‹åˆ°ä¸»é¢˜åˆ‡æ¢
        """
        # æ£€æŸ¥æ˜¯å¦çªç„¶å‡ºç°å¤§é‡æ¨å¹¿è¯æ±‡
        main_promo_count = sum(1 for verb in self.promo_verbs if verb in main_content)
        tail_promo_count = sum(1 for verb in self.promo_verbs if verb in tail)
        
        # å¦‚æœå°¾éƒ¨çš„æ¨å¹¿è¯å¯†åº¦è¿œé«˜äºæ­£æ–‡ï¼Œè¯´æ˜æœ‰ä¸»é¢˜åˆ‡æ¢
        if len(tail) > 0:
            tail_density = tail_promo_count / len(tail)
            main_density = main_promo_count / len(main_content) if len(main_content) > 0 else 0
            
            if tail_density > main_density * 3:  # å°¾éƒ¨æ¨å¹¿è¯å¯†åº¦æ˜¯æ­£æ–‡çš„3å€ä»¥ä¸Š
                return True
        
        # æ£€æŸ¥æ˜¯å¦çªç„¶å‡ºç°è”ç³»æ–¹å¼
        contact_pattern = r'[@][\w]+|t\.me/|https?://'
        main_contacts = len(re.findall(contact_pattern, main_content))
        tail_contacts = len(re.findall(contact_pattern, tail))
        
        # æ­£æ–‡æ²¡æœ‰è”ç³»æ–¹å¼ï¼Œå°¾éƒ¨çªç„¶å‡ºç°å¤šä¸ª
        if main_contacts == 0 and tail_contacts >= 2:
            return True
        
        return False
    
    def is_likely_promotion(self, text: str, semantic_score: float) -> bool:
        """
        åŸºäºè¯­ä¹‰å¾—åˆ†åˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯æ¨å¹¿
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            semantic_score: è¯­ä¹‰å¾—åˆ†
            
        Returns:
            æ˜¯å¦å¯èƒ½æ˜¯æ¨å¹¿
        """
        # ç‰¹æ®Šæƒ…å†µï¼šéå¸¸çŸ­çš„æ–‡æœ¬ä¸å¤ªå¯èƒ½æ˜¯æœ‰æ•ˆçš„æ¨å¹¿
        if len(text) < 20:
            return False
        
        # åŸºäºå¾—åˆ†çš„é˜ˆå€¼åˆ¤æ–­
        if semantic_score > 0.7:
            return True  # é«˜ç½®ä¿¡åº¦
        elif semantic_score > 0.5:
            # ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œéœ€è¦é¢å¤–æ£€æŸ¥
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„è”ç³»æ–¹å¼
            has_contact = bool(re.search(r'@\w+|t\.me/', text))
            has_promo_verb = any(verb in text for verb in self.promo_verbs)
            return has_contact and has_promo_verb
        else:
            return False  # ä½ç½®ä¿¡åº¦
    
    def _find_extended_promo_boundary(self, lines: list, start_point: int, full_content: str) -> int:
        """
        å‘å‰æ‰©å±•æŸ¥æ‰¾æ¨å¹¿å†…å®¹çš„çœŸæ­£è¾¹ç•Œ
        
        Args:
            lines: æ¶ˆæ¯è¡Œåˆ—è¡¨
            start_point: å½“å‰æ‰¾åˆ°çš„åˆ†å‰²ç‚¹
            full_content: å®Œæ•´å†…å®¹
            
        Returns:
            æ‰©å±•åçš„åˆ†å‰²ç‚¹ï¼ˆå¯èƒ½ç­‰äºåŸåˆ†å‰²ç‚¹ï¼‰
        """
        # å‘å‰æŸ¥æ‰¾æœ€å¤š5è¡Œ
        for i in range(max(0, start_point - 5), start_point):
            line = lines[i].strip()
            if not line:  # ç©ºè¡Œï¼Œå¯èƒ½æ˜¯åˆ†éš”ç¬¦
                continue
                
            # æ£€æŸ¥è¿™è¡Œæ˜¯å¦åŒ…å«æ¨å¹¿ç‰¹å¾
            line_score = 0.0
            
            # ç‰¹æ®Šç¬¦å·å’Œè£…é¥°ï¼ˆå¦‚æ˜Ÿå·ã€ç®­å¤´ç­‰ï¼‰
            if re.search(r'[â˜…â˜†â­ğŸŒŸâœ¨ğŸ’«âš¡ğŸ”¥ğŸ¯ğŸªğŸ¨ğŸ­ğŸªğŸ””ğŸ“£ğŸ“¢ğŸºğŸ“¯]', line):
                line_score += 0.3
            if re.search(r'[ğŸš©ğŸšªğŸšªğŸ”¤]', line):  # æ¶ˆæ¯#7987ä¸­çš„ç‰¹æ®Šç¬¦å·
                line_score += 0.4
            if re.search(r'\*+', line):  # æ˜Ÿå·è£…é¥°
                line_score += 0.2
            
            # æ¨å¹¿å…³é”®è¯
            promo_keywords = ['é¢‘é“', 'ç¾¤ç»„', 'äº¤æµ', 'æŠ•ç¨¿', 'çˆ†æ–™', 'å•†åŠ¡', 'åˆä½œ', 'è®¢é˜…', 'å…³æ³¨']
            for keyword in promo_keywords:
                if keyword in line:
                    line_score += 0.2
                    
            # å¦‚æœè¿™è¡Œæœ‰è¶³å¤Ÿçš„æ¨å¹¿ç‰¹å¾ï¼Œæ‰©å±•è¾¹ç•Œ
            if line_score > 0.4:
                return i
                
        return start_point
    
    def filter_message(self, content: str, has_media: bool = False) -> tuple:
        """
        è¿‡æ»¤æ¶ˆæ¯ä¸­çš„å°¾éƒ¨å†…å®¹
        
        Args:
            content: å®Œæ•´æ¶ˆæ¯å†…å®¹
            has_media: æ˜¯å¦æœ‰åª’ä½“æ–‡ä»¶ï¼ˆå›¾ç‰‡ã€è§†é¢‘ç­‰ï¼‰
            
        Returns:
            (è¿‡æ»¤åå†…å®¹, æ˜¯å¦è¿‡æ»¤äº†å°¾éƒ¨, å°¾éƒ¨å†…å®¹, åˆ†æè¯¦æƒ…)
        """
        if not content:
            return content, False, None, {}
        
        lines = content.split('\n')
        if len(lines) < 3:
            return content, False, None, {}
        
        # ä»åå¾€å‰æ‰«æï¼Œå¯»æ‰¾æ¨å¹¿å°¾éƒ¨çš„å¼€å§‹ä½ç½®
        best_split_point = None
        best_score = 0.0
        analysis = {'scanned_lines': []}
        
        # æœ€å¤šæ£€æŸ¥æœ€å15è¡Œæˆ–å…¨éƒ¨è¡Œæ•°çš„ä¸€åŠï¼Œå–è¾ƒå°å€¼
        max_scan_lines = min(15, len(lines) // 2 + 1)
        
        for i in range(len(lines) - 1, max(0, len(lines) - max_scan_lines - 1), -1):
            # ä»ç¬¬iè¡Œå¼€å§‹åˆ°æœ«å°¾çš„å†…å®¹
            tail_candidate = '\n'.join(lines[i:])
            
            # è®¡ç®—è¯­ä¹‰å¾—åˆ†
            semantic_score = self.calculate_semantic_score(tail_candidate, content)
            
            # è®°å½•åˆ†æè¯¦æƒ…
            line_analysis = {
                'line_start': i,
                'content_preview': tail_candidate[:100] + '...' if len(tail_candidate) > 100 else tail_candidate,
                'semantic_score': semantic_score
            }
            analysis['scanned_lines'].append(line_analysis)
            
            # å¦‚æœå¾—åˆ†è¶³å¤Ÿé«˜ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªå¥½çš„åˆ†å‰²ç‚¹
            if semantic_score > 0.4 and semantic_score > best_score:
                best_score = semantic_score
                best_split_point = i
                analysis['best_split'] = i
                analysis['best_score'] = semantic_score
                
                # é¢å¤–æ£€æŸ¥ï¼šå‘å‰æ‰©å±•æŸ¥æ‰¾è¿ç»­çš„æ¨å¹¿å†…å®¹
                extended_split = self._find_extended_promo_boundary(lines, i, content)
                if extended_split < i:
                    # æ‰¾åˆ°äº†æ›´æ—©çš„æ¨å¹¿å¼€å§‹ç‚¹
                    extended_tail = '\n'.join(lines[extended_split:])
                    extended_score = self.calculate_semantic_score(extended_tail, content)
                    if extended_score > semantic_score * 0.8:  # æ‰©å±•åå¾—åˆ†ä¸åº”ä¸‹é™å¤ªå¤š
                        best_split_point = extended_split
                        best_score = extended_score
                        analysis['extended_split'] = extended_split
                        analysis['extended_score'] = extended_score
                        logger.debug(f"æ‰©å±•æ¨å¹¿è¾¹ç•Œ: {i} -> {extended_split} (å¾—åˆ†: {extended_score:.3f})")
        
        # åˆ¤æ–­æ˜¯å¦æ‰¾åˆ°å°¾éƒ¨ï¼ˆé˜ˆå€¼0.5ï¼Œæé«˜è¯†åˆ«æ•æ„Ÿåº¦ï¼‰
        if best_split_point is not None and best_score > 0.5:
            filtered_content = '\n'.join(lines[:best_split_point]).strip()
            tail_content = '\n'.join(lines[best_split_point:]).strip()
            
            # å®‰å…¨æ£€æŸ¥ï¼šè¿‡æ»¤åçš„å†…å®¹ä¸èƒ½å¤ªçŸ­ï¼ˆä½†æœ‰åª’ä½“æ—¶å…è®¸å®Œå…¨è¿‡æ»¤ï¼‰
            if len(filtered_content) < 30 and not has_media:
                # æ£€æŸ¥æ˜¯å¦æ•´æ¡éƒ½æ˜¯æ¨å¹¿
                full_score = self.calculate_semantic_score(content)
                if full_score > 0.8:
                    # å…è®¸å®Œå…¨è¿‡æ»¤çº¯æ¨å¹¿å†…å®¹
                    logger.info(f"æ£€æµ‹åˆ°çº¯æ¨å¹¿å†…å®¹ï¼Œå®Œå…¨è¿‡æ»¤: {len(content)} -> 0 å­—ç¬¦")
                    return "", True, content, analysis
                else:
                    # ä¿ç•™åŸæ–‡ï¼Œé¿å…è¯¯åˆ æœ‰ä»·å€¼çš„æ­£å¸¸å†…å®¹
                    logger.warning(f"è¿‡æ»¤åå†…å®¹è¿‡çŸ­ä¸”åŒ…å«æ­£å¸¸å†…å®¹ï¼Œä¿ç•™åŸæ–‡: {len(filtered_content)} < 30")
                    return content, False, None, analysis
            elif len(filtered_content) < 30 and has_media:
                # æœ‰åª’ä½“çš„æƒ…å†µä¸‹ï¼Œå…è®¸å®Œå…¨è¿‡æ»¤æ–‡æœ¬å†…å®¹
                logger.info(f"æœ‰åª’ä½“æ¶ˆæ¯ï¼Œå…è®¸å®Œå…¨è¿‡æ»¤æ–‡æœ¬: {len(content)} -> {len(filtered_content)} å­—ç¬¦")
            
            # è®¡ç®—è¿‡æ»¤æ¯”ä¾‹ï¼Œæœ‰åª’ä½“æ—¶ä¸é™åˆ¶è¿‡æ»¤æ¯”ä¾‹
            filter_ratio = len(tail_content) / len(content) if content else 0
            if not has_media:
                # æ²¡æœ‰åª’ä½“æ—¶æ‰æ£€æŸ¥è¿‡æ»¤æ¯”ä¾‹
                # å¦‚æœæ¨å¹¿ç‰¹å¾éå¸¸æ˜æ˜¾ï¼ˆå¾—åˆ†>0.8ï¼‰ï¼Œå…è®¸æ›´å¤§çš„è¿‡æ»¤æ¯”ä¾‹
                max_filter_ratio = 0.85 if best_score > 0.8 else 0.7
                if filter_ratio > max_filter_ratio:
                    logger.warning(f"è¿‡æ»¤æ¯”ä¾‹è¿‡å¤§ ({filter_ratio:.1%})ï¼Œè¶…è¿‡é™åˆ¶ {max_filter_ratio:.1%}ï¼Œä¿ç•™åŸæ–‡")
                    return content, False, None, analysis
            else:
                logger.debug(f"æœ‰åª’ä½“æ¶ˆæ¯ï¼Œä¸é™åˆ¶è¿‡æ»¤æ¯”ä¾‹: {filter_ratio:.1%}")
            
            logger.info(f"è¯­ä¹‰å°¾éƒ¨è¿‡æ»¤æˆåŠŸ: {len(content)} -> {len(filtered_content)} å­—ç¬¦ "
                       f"(è¿‡æ»¤{filter_ratio:.1%}ï¼Œå¾—åˆ†{best_score:.2f})")
            
            return filtered_content, True, tail_content, analysis
        
        return content, False, None, analysis


# å…¨å±€å®ä¾‹
semantic_tail_filter = SemanticTailFilter()