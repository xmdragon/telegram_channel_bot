"""
æ™ºèƒ½å°¾éƒ¨è¿‡æ»¤å™¨ - çº¯æ•°æ®é©±åŠ¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹
ä»…åŸºäºå°¾éƒ¨å†…å®¹æœ¬èº«è¿›è¡Œå­¦ä¹ ï¼Œä¸ä¾èµ–ä¸Šä¸‹æ–‡æˆ–é¢‘é“ä¿¡æ¯
"""
import re
import json
import logging
from typing import Tuple, List, Dict, Optional
import numpy as np
from collections import Counter
from pathlib import Path

logger = logging.getLogger(__name__)


class TailFeatureExtractor:
    """å°¾éƒ¨å†…å®¹ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        self.learned_keywords = Counter()  # ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ çš„å…³é”®è¯
        
    def extract_features(self, text: str) -> Dict[str, float]:
        """
        æå–æ–‡æœ¬ç‰¹å¾
        
        Returns:
            ç‰¹å¾å­—å…¸ï¼ŒåŒ…å«å„ç§ç‰¹å¾çš„æ•°å€¼
        """
        if not text:
            return {}
            
        features = {}
        lines = text.split('\n')
        text_length = len(text)
        
        # 1. é“¾æ¥ç‰¹å¾
        links = re.findall(r'https?://[^\s]+|t\.me/[^\s]+', text)
        features['link_count'] = len(links)
        features['link_density'] = len(''.join(links)) / text_length if text_length > 0 else 0
        
        # 2. @ç”¨æˆ·åç‰¹å¾
        usernames = re.findall(r'@\w+', text)
        features['username_count'] = len(usernames)
        features['username_density'] = len(usernames) / len(lines) if lines else 0
        
        # 3. Emojiç‰¹å¾
        emojis = re.findall(r'[\U0001F300-\U0001F9FF\U00002600-\U000027BF]', text)
        features['emoji_count'] = len(emojis)
        features['emoji_density'] = len(emojis) / text_length if text_length > 0 else 0
        
        # 4. ç»“æ„ç‰¹å¾
        features['line_count'] = len(lines)
        features['avg_line_length'] = sum(len(line) for line in lines) / len(lines) if lines else 0
        features['has_separator'] = 1.0 if re.search(r'^[-=*#_~]{3,}$', text, re.MULTILINE) else 0.0
        
        # 5. æ¨å¹¿å…³é”®è¯ç‰¹å¾ï¼ˆåŠ¨æ€å­¦ä¹ çš„ï¼‰
        promo_score = 0
        for keyword, weight in self.learned_keywords.most_common(20):
            if keyword in text:
                promo_score += weight
        features['promo_keyword_score'] = promo_score
        
        # 6. æ ¼å¼ç‰¹å¾
        features['has_pipe_separator'] = 1.0 if '|' in text else 0.0
        features['has_arrow'] = 1.0 if 'â†“' in text or 'â†’' in text else 0.0
        features['has_brackets'] = 1.0 if re.search(r'\[.*\]|\(.*\)', text) else 0.0
        
        return features
    
    def learn_keywords(self, tail_samples: List[str]):
        """ä»å°¾éƒ¨æ ·æœ¬ä¸­å­¦ä¹ å…³é”®è¯"""
        # æå–æ‰€æœ‰ä¸­æ–‡è¯ç»„å’Œè‹±æ–‡å•è¯
        for sample in tail_samples:
            # ä¸­æ–‡è¯ç»„ï¼ˆ2-4ä¸ªå­—ï¼‰
            chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,4}', sample)
            for word in chinese_words:
                self.learned_keywords[word] += 1
            
            # è‹±æ–‡å•è¯
            english_words = re.findall(r'\b[A-Za-z]{3,}\b', sample.lower())
            for word in english_words:
                self.learned_keywords[word] += 1
        
        logger.info(f"å­¦ä¹ äº† {len(self.learned_keywords)} ä¸ªå…³é”®è¯")


class IntelligentTailFilter:
    """æ™ºèƒ½å°¾éƒ¨è¿‡æ»¤å™¨ - åŸºäºçº¯å°¾éƒ¨æ•°æ®å­¦ä¹ """
    
    def __init__(self):
        self.feature_extractor = TailFeatureExtractor()
        self.tail_samples = []
        self.sample_features = []  # ç¼“å­˜çš„ç‰¹å¾å‘é‡
        self.feature_weights = None  # ç‰¹å¾æƒé‡
        self.threshold = 0.6  # åˆ¤å®šé˜ˆå€¼
        self._last_load_time = 0  # ä¸Šæ¬¡åŠ è½½æ—¶é—´
        self._reload_interval = 300  # 5åˆ†é’Ÿé‡è½½é—´éš”
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        self._load_training_data()
        
    def _load_training_data(self, force_reload=False):
        """åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆå¸¦ç¼“å­˜æœºåˆ¶ï¼‰"""
        import time
        current_time = time.time()
        
        # å¦‚æœä¸æ˜¯å¼ºåˆ¶é‡è½½ï¼Œä¸”åœ¨ç¼“å­˜æ—¶é—´å†…ï¼Œè·³è¿‡åŠ è½½
        if not force_reload and self._last_load_time > 0:
            if current_time - self._last_load_time < self._reload_interval:
                return
        
        from app.core.training_config import TrainingDataConfig
        
        try:
            self._last_load_time = current_time
            tail_file = TrainingDataConfig.TAIL_FILTER_SAMPLES_FILE
            if tail_file.exists():
                with open(tail_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    samples = data.get('samples', data) if isinstance(data, dict) else data
                    
                    # åªæå–tail_part
                    self.tail_samples = []
                    for sample in samples:
                        if sample.get('tail_part'):
                            self.tail_samples.append(sample['tail_part'])
                    
                    logger.info(f"åŠ è½½äº† {len(self.tail_samples)} ä¸ªå°¾éƒ¨æ ·æœ¬")
                    
                    # å­¦ä¹ å…³é”®è¯
                    self.feature_extractor.learn_keywords(self.tail_samples)
                    
                    # æå–æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾
                    self._extract_sample_features()
                    
                    # è®¡ç®—ç‰¹å¾æƒé‡
                    self._calculate_feature_weights()
                    
        except Exception as e:
            logger.error(f"åŠ è½½è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
    
    def _extract_sample_features(self):
        """æå–æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾å‘é‡"""
        self.sample_features = []
        for sample in self.tail_samples:
            features = self.feature_extractor.extract_features(sample)
            self.sample_features.append(features)
    
    def _calculate_feature_weights(self):
        """è®¡ç®—ç‰¹å¾æƒé‡ï¼ˆåŸºäºç‰¹å¾çš„åŒºåˆ†åº¦å’Œé‡è¦æ€§ï¼‰"""
        if not self.sample_features:
            return
        
        # è·å–æ‰€æœ‰ç‰¹å¾å
        all_features = set()
        for features in self.sample_features:
            all_features.update(features.keys())
        
        # åŸºç¡€æƒé‡ï¼ˆæ ¹æ®ç‰¹å¾é‡è¦æ€§é¢„è®¾ï¼‰
        base_weights = {
            'link_count': 0.25,  # é“¾æ¥æ˜¯å¼ºç‰¹å¾
            'username_count': 0.20,  # @ç”¨æˆ·åä¹Ÿæ˜¯å¼ºç‰¹å¾
            'promo_keyword_score': 0.15,  # æ¨å¹¿å…³é”®è¯
            'emoji_density': 0.10,  # emojiå¯†åº¦
            'has_separator': 0.10,  # åˆ†éš”ç¬¦
            'has_arrow': 0.08,  # ç®­å¤´ç¬¦å·
            'line_count': 0.05,  # è¡Œæ•°
            'has_pipe_separator': 0.04,  # ç®¡é“åˆ†éš”ç¬¦
            'has_brackets': 0.03  # æ‹¬å·
        }
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ ‡å‡†å·®ï¼ˆæ ‡å‡†å·®å¤§è¯´æ˜åŒºåˆ†åº¦é«˜ï¼‰
        feature_stds = {}
        for feature_name in all_features:
            values = [f.get(feature_name, 0) for f in self.sample_features]
            if values:
                feature_stds[feature_name] = np.std(values)
        
        # ç»“åˆåŸºç¡€æƒé‡å’Œæ ‡å‡†å·®
        self.feature_weights = {}
        for feature_name in all_features:
            base_w = base_weights.get(feature_name, 0.01)
            std_w = feature_stds.get(feature_name, 0)
            
            # ç»„åˆæƒé‡ï¼šåŸºç¡€æƒé‡70% + æ ‡å‡†å·®æƒé‡30%
            if sum(feature_stds.values()) > 0:
                std_normalized = std_w / sum(feature_stds.values())
            else:
                std_normalized = 1.0 / len(all_features)
            
            self.feature_weights[feature_name] = base_w * 0.7 + std_normalized * 0.3
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(self.feature_weights.values())
        if total_weight > 0:
            self.feature_weights = {
                name: weight / total_weight
                for name, weight in self.feature_weights.items()
            }
        
        logger.info(f"è®¡ç®—äº† {len(self.feature_weights)} ä¸ªç‰¹å¾çš„æƒé‡")
    
    def calculate_similarity(self, text: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬ä¸è®­ç»ƒæ ·æœ¬çš„ç›¸ä¼¼åº¦
        
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if not self.tail_samples:
            return 0.0
        
        # æå–ç‰¹å¾
        features = self.feature_extractor.extract_features(text)
        
        if not features or not self.sample_features:
            return 0.0
        
        # è®¡ç®—ä¸æ¯ä¸ªæ ·æœ¬çš„ç›¸ä¼¼åº¦
        similarities = []
        for sample_feat in self.sample_features:
            similarity = self._compute_feature_similarity(features, sample_feat)
            similarities.append(similarity)
        
        # è¿”å›æœ€å¤§ç›¸ä¼¼åº¦
        return max(similarities) if similarities else 0.0
    
    def _compute_feature_similarity(self, feat1: Dict, feat2: Dict) -> float:
        """è®¡ç®—ä¸¤ä¸ªç‰¹å¾å‘é‡çš„ç›¸ä¼¼åº¦"""
        if not self.feature_weights:
            return 0.0
        
        similarity = 0.0
        total_weight = 0.0
        
        for feature_name, weight in self.feature_weights.items():
            val1 = feat1.get(feature_name, 0)
            val2 = feat2.get(feature_name, 0)
            
            # è®¡ç®—è¯¥ç‰¹å¾çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨1-normalized_diffï¼‰
            max_val = max(abs(val1), abs(val2))
            if max_val > 0:
                feature_sim = 1 - abs(val1 - val2) / max_val
            else:
                feature_sim = 1.0 if val1 == val2 else 0.0
            
            similarity += feature_sim * weight
            total_weight += weight
        
        return similarity / total_weight if total_weight > 0 else 0.0
    
    def is_tail(self, text: str) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºå°¾éƒ¨æ¨å¹¿
        
        Args:
            text: è¦æ£€æµ‹çš„æ–‡æœ¬
            
        Returns:
            æ˜¯å¦ä¸ºå°¾éƒ¨
        """
        if not text or len(text) < 10:
            return False
        
        # æå–ç‰¹å¾
        features = self.feature_extractor.extract_features(text)
        
        # ä¸‰å±‚åˆ¤æ–­æœºåˆ¶
        
        # 1. å¿«é€Ÿåˆ¤æ–­ï¼šå¼ºç‰¹å¾ç»„åˆ
        if features.get('link_count', 0) >= 1 and features.get('username_count', 0) >= 1:
            if len(text) < 500:  # å°¾éƒ¨é€šå¸¸ä¸ä¼šå¤ªé•¿
                return True
        
        # 2. ç‰¹å¾å¾—åˆ†åˆ¤æ–­
        feature_score = self._calculate_feature_score(features)
        if feature_score > 0.7:  # ç‰¹å¾å¾—åˆ†å¾ˆé«˜
            return True
        
        # 3. ç›¸ä¼¼åº¦åˆ¤æ–­ï¼ˆä¸è®­ç»ƒæ ·æœ¬å¯¹æ¯”ï¼‰
        if feature_score > 0.3:  # æœ‰ä¸€å®šç‰¹å¾
            similarity = self.calculate_similarity(text)
            
            # åŠ¨æ€é˜ˆå€¼ï¼šç‰¹å¾è¶Šæ˜æ˜¾ï¼Œç›¸ä¼¼åº¦è¦æ±‚è¶Šä½
            dynamic_threshold = self.threshold - (feature_score * 0.2)
            
            # ç»¼åˆå¾—åˆ†
            final_score = similarity * 0.5 + feature_score * 0.5
            
            return final_score > dynamic_threshold
        
        return False
    
    def _calculate_feature_score(self, features: Dict) -> float:
        """è®¡ç®—ç‰¹å¾å¾—åˆ†ï¼ˆæ›´ç²¾ç»†çš„è¯„åˆ†ï¼‰"""
        score = 0.0
        
        # é“¾æ¥ç‰¹å¾ï¼ˆæœ€å¼ºä¿¡å·ï¼‰
        link_count = features.get('link_count', 0)
        if link_count >= 2:
            score += 0.35
        elif link_count == 1:
            score += 0.25
        
        # ç”¨æˆ·åç‰¹å¾
        username_count = features.get('username_count', 0)
        if username_count >= 2:
            score += 0.25
        elif username_count == 1:
            score += 0.15
        
        # Emojiå¯†åº¦ï¼ˆå¹¿å‘Šå¸¸ç”¨emojiï¼‰
        emoji_density = features.get('emoji_density', 0)
        if emoji_density > 0.15:
            score += 0.15
        elif emoji_density > 0.1:
            score += 0.10
        elif emoji_density > 0.05:
            score += 0.05
        
        # åˆ†éš”ç¬¦ï¼ˆæ˜æ˜¾çš„ç»“æ„ç‰¹å¾ï¼‰
        if features.get('has_separator', 0) > 0:
            score += 0.15
        
        # æ¨å¹¿å…³é”®è¯ï¼ˆä»è®­ç»ƒæ•°æ®å­¦ä¹ ï¼‰
        keyword_score = features.get('promo_keyword_score', 0)
        if keyword_score > 10:
            score += 0.20
        elif keyword_score > 5:
            score += 0.15
        elif keyword_score > 0:
            score += 0.10
        
        # ç®­å¤´å’Œç®¡é“ç¬¦å·
        if features.get('has_arrow', 0) > 0:
            score += 0.08
        if features.get('has_pipe_separator', 0) > 0:
            score += 0.07
        
        return min(score, 1.0)
    
    def filter_message(self, content: str) -> Tuple[str, bool, Optional[str]]:
        """
        è¿‡æ»¤æ¶ˆæ¯ä¸­çš„å°¾éƒ¨ï¼ˆç®€åŒ–é€»è¾‘ï¼šåªè¦åŒ¹é…å°±è¿‡æ»¤ï¼‰
        
        Args:
            content: å®Œæ•´æ¶ˆæ¯å†…å®¹
            
        Returns:
            (è¿‡æ»¤åå†…å®¹, æ˜¯å¦æœ‰å°¾éƒ¨, å°¾éƒ¨å†…å®¹)
        """
        import re
        
        if not content:
            return content, False, None
        
        lines = content.split('\n')
        
        # ç­–ç•¥1ï¼šå¿«é€Ÿæ£€æµ‹æ˜æ˜¾çš„åˆ†éš”ç¬¦
        separator_patterns = [
            r'^[-=*#_~]{3,}$',  # å¸¸è§åˆ†éš”ç¬¦
            r'^[â€”]+$',  # ä¸­æ–‡ç ´æŠ˜å·
            r'^\s*[ğŸ“£ğŸ””ğŸ˜‰ğŸ‘Œ]+\s*$'  # emojiåˆ†éš”è¡Œ
        ]
        
        separator_line = -1
        for i in range(len(lines) - 1, max(0, len(lines) - 15), -1):
            for pattern in separator_patterns:
                if re.match(pattern, lines[i].strip()):
                    separator_line = i
                    break
            if separator_line != -1:
                break
        
        # å¦‚æœæ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œä»åˆ†éš”ç¬¦å¼€å§‹æ£€æŸ¥
        if separator_line != -1:
            potential_tail = '\n'.join(lines[separator_line:])
            if self.is_tail(potential_tail):
                clean_content = '\n'.join(lines[:separator_line]).rstrip()
                # ç®€åŒ–ï¼šåªè¦æœ‰å†…å®¹å°±è¿”å›ï¼Œä¸ç®¡æ¯”ä¾‹
                if clean_content:
                    return clean_content, True, potential_tail
        
        # ç­–ç•¥2ï¼šæ™ºèƒ½æ‰«æï¼ˆæ‰¾åˆ°æœ€å¤§çš„å°¾éƒ¨èŒƒå›´ï¼‰
        best_split = -1
        best_tail = None
        
        # ä»åå¾€å‰æ‰«æï¼Œæ‰¾åˆ°æœ€æ—©çš„å°¾éƒ¨èµ·å§‹ä½ç½®
        for i in range(len(lines) - 1, 0, -1):  # ä»å€’æ•°ç¬¬äºŒè¡Œæ‰«æåˆ°ç¬¬äºŒè¡Œ
            potential_tail = '\n'.join(lines[i:])
            
            # è·³è¿‡å¤ªçŸ­çš„å†…å®¹
            if len(potential_tail) < 15:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå°¾éƒ¨
            if self.is_tail(potential_tail):
                # è®°å½•è¿™ä¸ªä½ç½®ï¼ˆç»§ç»­å‘å‰æ‰«æï¼Œæ‰¾æ›´å¤§çš„å°¾éƒ¨ï¼‰
                best_split = i
                best_tail = potential_tail
            else:
                # å¦‚æœä¸æ˜¯å°¾éƒ¨äº†ï¼Œåœæ­¢æ‰«æ
                if best_split != -1:
                    break
        
        # å¦‚æœæ‰¾åˆ°å°¾éƒ¨
        if best_split != -1 and best_tail:
            clean_content = '\n'.join(lines[:best_split]).rstrip()
            
            # ç®€åŒ–çš„åˆ¤æ–­é€»è¾‘
            # 1. å¦‚æœæœ‰æ­£æ–‡å†…å®¹ï¼ˆ>5å­—ç¬¦ï¼‰ï¼Œç›´æ¥è¿”å›
            if clean_content and len(clean_content) > 5:
                # ç¡®ä¿æ­£æ–‡æœ‰åŸºæœ¬å†…å®¹
                has_content = bool(re.search(r'[\u4e00-\u9fa5a-zA-Z0-9]+', clean_content))
                if has_content:
                    return clean_content, True, best_tail
            
            # 2. å¦‚æœæ²¡æœ‰æ­£æ–‡æˆ–æ­£æ–‡å¤ªçŸ­ï¼Œå¯èƒ½æ•´æ¡éƒ½æ˜¯æ¨å¹¿
            if best_split <= 1:  # åªå‰©ç¬¬ä¸€è¡Œæˆ–æ²¡æœ‰æ­£æ–‡
                # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦ä¹Ÿæ˜¯æ¨å¹¿
                if lines and self.is_tail(lines[0]):
                    return "", True, content  # æ•´æ¡éƒ½æ˜¯æ¨å¹¿
                elif clean_content:  # æœ‰çŸ­æ­£æ–‡
                    return clean_content, True, best_tail
        
        return content, False, None
    
    def add_training_sample(self, tail_text: str):
        """
        æ·»åŠ æ–°çš„è®­ç»ƒæ ·æœ¬
        
        Args:
            tail_text: å°¾éƒ¨æ–‡æœ¬
        """
        if tail_text and tail_text not in self.tail_samples:
            self.tail_samples.append(tail_text)
            
            # æ›´æ–°å…³é”®è¯
            self.feature_extractor.learn_keywords([tail_text])
            
            # é‡æ–°æå–ç‰¹å¾
            self._extract_sample_features()
            
            # é‡æ–°è®¡ç®—æƒé‡
            self._calculate_feature_weights()
            
            # å¼ºåˆ¶é‡æ–°åŠ è½½ä»¥è·å–æœ€æ–°æ•°æ®
            self._load_training_data(force_reload=True)
            
            logger.info(f"æ·»åŠ äº†æ–°çš„è®­ç»ƒæ ·æœ¬ï¼Œå½“å‰å…± {len(self.tail_samples)} ä¸ªæ ·æœ¬")
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_samples': len(self.tail_samples),
            'learned_keywords': len(self.feature_extractor.learned_keywords),
            'top_keywords': self.feature_extractor.learned_keywords.most_common(10),
            'feature_count': len(self.feature_weights) if self.feature_weights else 0,
            'threshold': self.threshold
        }


# å…¨å±€å®ä¾‹
intelligent_tail_filter = IntelligentTailFilter()