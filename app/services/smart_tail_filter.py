"""
æ™ºèƒ½å°¾éƒ¨è¿‡æ»¤å™¨
è¯†åˆ«å¹¶ç§»é™¤æ¶ˆæ¯å°¾éƒ¨çš„é¢‘é“æ ‡è¯†ï¼Œä¿ç•™æ­£å¸¸å†…å®¹
æ³¨æ„ï¼šå°¾éƒ¨è¿‡æ»¤æ˜¯ç§»é™¤åŸé¢‘é“æ ‡è¯†ï¼Œä¸ç®—å¹¿å‘Š
"""
import logging
import re
from typing import Tuple, Optional, List, Dict
from app.services.ad_detector import ad_detector
from app.services.ai_filter import ai_filter
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)


class SmartTailFilter:
    """æ™ºèƒ½å°¾éƒ¨è¿‡æ»¤å™¨ - AIé©±åŠ¨çš„è¯­ä¹‰ç†è§£"""
    
    def __init__(self):
        self.ad_detector = ad_detector
        self.ai_filter = ai_filter  # é›†æˆAIè¿‡æ»¤å™¨
        self.semantic_threshold = 0.5  # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
        self.known_tail_patterns = []  # å·²çŸ¥çš„å°¾éƒ¨æ¨¡å¼
        self.channel_tail_patterns = {}  # æŒ‰é¢‘é“å­˜å‚¨çš„å°¾éƒ¨æ¨¡å¼
        self._load_tail_patterns()  # åŠ è½½è®­ç»ƒçš„å°¾éƒ¨æ¨¡å¼
        
        # ä¿ç•™è§„åˆ™ä½œä¸ºfallback
        self.separator_patterns = [
            r'â”{10,}',  # æ¨ªçº¿åˆ†éš”ç¬¦
            r'â•{10,}',  # åŒçº¿åˆ†éš”ç¬¦
            r'â”€{10,}',  # ç»†çº¿åˆ†éš”ç¬¦
            r'â–¬{10,}',  # ç²—çº¿åˆ†éš”ç¬¦
            r'-{3,}',   # çŸ­æ¨ªçº¿ï¼ˆé™ä½åˆ°3ä¸ªï¼‰
            r'={3,}',   # ç­‰å·çº¿ï¼ˆé™ä½åˆ°3ä¸ªï¼‰
            r'\*{3,}',  # æ˜Ÿå·çº¿ï¼ˆé™ä½åˆ°3ä¸ªï¼‰
            r'#{3,}',   # äº•å·çº¿ï¼ˆæ–°å¢ï¼‰
            r'\.{3,}',  # ç‚¹çº¿ï¼ˆæ–°å¢ï¼‰
            r'_{3,}',   # ä¸‹åˆ’çº¿ï¼ˆæ–°å¢ï¼‰
            r'~{3,}',   # æ³¢æµªçº¿ï¼ˆæ–°å¢ï¼‰
        ]
        
        # å°¾éƒ¨æ ‡è¯†å…³é”®è¯ï¼ˆå¼ºä¿¡å·ï¼‰
        self.tail_keywords = [
            'å¤±è”å¯¼èˆª', 'è®¢é˜…é¢‘é“', 'ä¾¿æ°‘ä¿¡æ¯', 'å•†åŠ¡åˆä½œ', 'æŠ•ç¨¿çˆ†æ–™',
            'é¢‘é“æ¨è', 'äº’åŠ©ç¾¤', 'è”ç³»æ–¹å¼', 'å®˜æ–¹ç¾¤ç»„', 'å¹¿å‘Šåˆä½œ'
        ]
    
    def filter_tail_ads(self, content: str, channel_id: str = None) -> Tuple[str, bool, Optional[str]]:
        """
        è¿‡æ»¤å°¾éƒ¨é¢‘é“æ ‡è¯† - AIä¼˜å…ˆï¼Œè§„åˆ™å…œåº•
        
        Args:
            content: åŸå§‹æ¶ˆæ¯å†…å®¹
            channel_id: é¢‘é“IDï¼ˆç”¨äºAIæ¨¡å¼åŒ¹é…ï¼‰
            
        Returns:
            (è¿‡æ»¤åå†…å®¹, æ˜¯å¦åŒ…å«å°¾éƒ¨, è¢«è¿‡æ»¤çš„å°¾éƒ¨éƒ¨åˆ†)
        """
        if not content:
            return content, False, None
        
        # 0. é¦–å…ˆæ£€æŸ¥å·²çŸ¥çš„ç²¾ç¡®å°¾éƒ¨æ¨¡å¼
        result = self._filter_by_known_patterns(content, channel_id)
        if result[1]:
            logger.info(f"ç²¾ç¡®åŒ¹é…åˆ°å·²çŸ¥å°¾éƒ¨ï¼ŒåŸé•¿åº¦: {len(content)}, è¿‡æ»¤å: {len(result[0])}")
            return result
        
        # 1. ä½¿ç”¨AIè¯­ä¹‰æ£€æµ‹
        if self.ai_filter and self.ai_filter.initialized:
            result = self._filter_by_ai_semantics(content, channel_id)
            if result[1]:  # AIæ£€æµ‹åˆ°å°¾éƒ¨
                logger.info(f"AIè¯­ä¹‰æ£€æµ‹åˆ°å°¾éƒ¨ï¼ŒåŸé•¿åº¦: {len(content)}, è¿‡æ»¤å: {len(result[0])}")
                return result
        
        # AIæ— æ³•åˆ¤æ–­æ—¶ï¼Œä½¿ç”¨è§„åˆ™ä½œä¸ºfallback
        
        # 1. ç‰¹æ®Šæ ¼å¼æ£€æµ‹ï¼ˆå¦‚ -------[é“¾æ¥] | [é“¾æ¥]ï¼‰
        result = self._filter_by_special_format(content)
        if result[1]:
            logger.info(f"è§„åˆ™æ£€æµ‹åˆ°ç‰¹æ®Šæ ¼å¼å°¾éƒ¨ï¼ŒåŸé•¿åº¦: {len(content)}, è¿‡æ»¤å: {len(result[0])}")
            return result
        
        # 2. åˆ†éš”ç¬¦æ£€æµ‹
        result = self._filter_by_separator(content)
        if result[1]:
            logger.info(f"è§„åˆ™æ£€æµ‹åˆ°åˆ†éš”ç¬¦å°¾éƒ¨ï¼ŒåŸé•¿åº¦: {len(content)}, è¿‡æ»¤å: {len(result[0])}")
            return result
        
        # 3. é“¾æ¥å¯†åº¦æ£€æµ‹
        result = self._filter_by_link_density(content)
        if result[1]:
            logger.info(f"è§„åˆ™æ£€æµ‹åˆ°é“¾æ¥å¯†é›†å°¾éƒ¨ï¼ŒåŸé•¿åº¦: {len(content)}, è¿‡æ»¤å: {len(result[0])}")
            return result
        
        return content, False, None
    
    def _load_tail_patterns(self):
        """åŠ è½½è®­ç»ƒçš„å°¾éƒ¨æ¨¡å¼"""
        import json
        import os
        from app.core.training_config import TrainingDataConfig
        
        try:
            # åŠ è½½å°¾éƒ¨è¿‡æ»¤æ ·æœ¬
            tail_file = str(TrainingDataConfig.TAIL_FILTER_SAMPLES_FILE)
            if os.path.exists(tail_file):
                with open(tail_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # å¤„ç†å¯èƒ½çš„åµŒå¥—ç»“æ„
                    samples = data.get('samples', data) if isinstance(data, dict) else data
                    for sample in samples:
                        if sample.get('tail_part'):
                            # å­˜å‚¨å°¾éƒ¨æ¨¡å¼
                            tail_pattern = sample['tail_part'].strip()
                            if tail_pattern:
                                self.known_tail_patterns.append(tail_pattern)
                                
                                # æŒ‰é¢‘é“å­˜å‚¨
                                if sample.get('source'):
                                    channel = sample['source']
                                    if channel not in self.channel_tail_patterns:
                                        self.channel_tail_patterns[channel] = []
                                    self.channel_tail_patterns[channel].append(tail_pattern)
                
                logger.info(f"åŠ è½½äº† {len(self.known_tail_patterns)} ä¸ªå°¾éƒ¨æ¨¡å¼")
                
            # åŠ è½½æ‰‹åŠ¨è®­ç»ƒæ•°æ®
            manual_file = str(TrainingDataConfig.MANUAL_TRAINING_FILE)
            if os.path.exists(manual_file):
                with open(manual_file, 'r', encoding='utf-8') as f:
                    manual_data = json.load(f)
                    for channel_id, channel_data in manual_data.items():
                        if 'samples' in channel_data:
                            channel_name = channel_data.get('channel_name', '')
                            for sample in channel_data['samples']:
                                if sample.get('tail'):
                                    tail_pattern = sample['tail'].strip()
                                    if tail_pattern and tail_pattern not in self.known_tail_patterns:
                                        self.known_tail_patterns.append(tail_pattern)
                                        
                                        # æŒ‰é¢‘é“å­˜å‚¨
                                        if channel_name:
                                            if channel_name not in self.channel_tail_patterns:
                                                self.channel_tail_patterns[channel_name] = []
                                            if tail_pattern not in self.channel_tail_patterns[channel_name]:
                                                self.channel_tail_patterns[channel_name].append(tail_pattern)
                
                logger.info(f"æ€»å…±åŠ è½½äº† {len(self.known_tail_patterns)} ä¸ªå”¯ä¸€å°¾éƒ¨æ¨¡å¼")
                logger.info(f"è¦†ç›– {len(self.channel_tail_patterns)} ä¸ªé¢‘é“")
                
        except Exception as e:
            logger.error(f"åŠ è½½å°¾éƒ¨æ¨¡å¼å¤±è´¥: {e}")
    
    def _filter_by_known_patterns(self, content: str, channel_id: str = None) -> Tuple[str, bool, Optional[str]]:
        """åŸºäºå·²çŸ¥æ¨¡å¼çš„ç²¾ç¡®åŒ¹é…"""
        if not content or not self.known_tail_patterns:
            return content, False, None
        
        # ä¼˜å…ˆæ£€æŸ¥é¢‘é“ç‰¹å®šçš„æ¨¡å¼
        patterns_to_check = []
        
        # è·å–é¢‘é“ç‰¹å®šæ¨¡å¼
        if channel_id:
            # å°è¯•é€šè¿‡é¢‘é“IDæŸ¥æ‰¾
            for channel_name, patterns in self.channel_tail_patterns.items():
                if channel_id in channel_name or channel_name in str(channel_id):
                    patterns_to_check.extend(patterns)
        
        # å¦‚æœæ²¡æœ‰é¢‘é“ç‰¹å®šæ¨¡å¼ï¼Œä½¿ç”¨æ‰€æœ‰å·²çŸ¥æ¨¡å¼
        if not patterns_to_check:
            patterns_to_check = self.known_tail_patterns
        
        # æ£€æŸ¥æ¯ä¸ªæ¨¡å¼
        for pattern in patterns_to_check:
            if not pattern:
                continue
                
            # å°è¯•ç²¾ç¡®åŒ¹é…ï¼ˆåœ¨å†…å®¹æœ«å°¾ï¼‰
            if content.endswith(pattern):
                clean_content = content[:-len(pattern)].rstrip()
                return clean_content, True, pattern
            
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå…è®¸å°¾éƒ¨æœ‰å°‘é‡é¢å¤–å†…å®¹ï¼‰
            pattern_lines = pattern.split('\n')
            if len(pattern_lines) >= 2:
                # è·å–æ¨¡å¼çš„å‰ä¸¤è¡Œä½œä¸ºç‰¹å¾
                pattern_start = '\n'.join(pattern_lines[:2])
                
                # åœ¨å†…å®¹ä¸­æŸ¥æ‰¾è¿™ä¸ªç‰¹å¾
                idx = content.rfind(pattern_start)
                if idx > 0:
                    # æ‰¾åˆ°äº†æ¨¡å¼çš„å¼€å§‹ä½ç½®
                    # æ£€æŸ¥ä»è¿™ä¸ªä½ç½®åˆ°ç»“å°¾æ˜¯å¦éƒ½æ˜¯æ¨å¹¿å†…å®¹
                    potential_tail = content[idx:]
                    
                    # éªŒè¯æ˜¯å¦åŒ…å«å…³é”®å…ƒç´ ï¼ˆå¦‚@ç¬¦å·ã€é“¾æ¥ç­‰ï¼‰
                    if self._is_likely_tail(potential_tail):
                        clean_content = content[:idx].rstrip()
                        return clean_content, True, potential_tail
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å°¾éƒ¨å…³é”®è¯ç»„åˆ
        return self._filter_by_tail_keywords(content)
    
    def _filter_by_tail_keywords(self, content: str) -> Tuple[str, bool, Optional[str]]:
        """åŸºäºå°¾éƒ¨å…³é”®è¯çš„æ£€æµ‹"""
        lines = content.split('\n')
        
        # ä»åå¾€å‰æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„è¡Œ
        for i in range(len(lines) - 1, max(0, len(lines) - 10), -1):
            line = lines[i]
            
            # è®¡ç®—è¿™ä¸€è¡ŒåŒ…å«çš„å…³é”®è¯æ•°é‡
            keyword_count = sum(1 for kw in self.tail_keywords if kw in line)
            
            # å¦‚æœåŒ…å«2ä¸ªæˆ–ä»¥ä¸Šå…³é”®è¯ï¼Œå¾ˆå¯èƒ½æ˜¯å°¾éƒ¨å¼€å§‹
            if keyword_count >= 2:
                # æ£€æŸ¥è¿™ä¸€è¡ŒåŠåç»­è¡Œæ˜¯å¦éƒ½æ˜¯æ¨å¹¿å†…å®¹
                potential_tail = '\n'.join(lines[i:])
                if self._is_likely_tail(potential_tail):
                    clean_content = '\n'.join(lines[:i]).rstrip()
                    return clean_content, True, potential_tail
            
            # å•ä¸ªå¼ºå…³é”®è¯ + @ ç¬¦å·
            if keyword_count >= 1 and '@' in line:
                potential_tail = '\n'.join(lines[i:])
                if self._is_likely_tail(potential_tail):
                    clean_content = '\n'.join(lines[:i]).rstrip()
                    return clean_content, True, potential_tail
        
        return content, False, None
    
    def _is_likely_tail(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦å¯èƒ½æ˜¯å°¾éƒ¨æ¨å¹¿"""
        if not text:
            return False
        
        # ç‰¹å¾è®¡æ•°
        features = 0
        
        # 1. åŒ…å«@ç¬¦å·ï¼ˆTelegramç”¨æˆ·åï¼‰
        if '@' in text:
            features += 2
        
        # 2. åŒ…å«é“¾æ¥
        if 'http' in text or 't.me' in text:
            features += 2
        
        # 3. åŒ…å«å°¾éƒ¨å…³é”®è¯
        for kw in self.tail_keywords:
            if kw in text:
                features += 1
        
        # 4. åŒ…å«emojiè£…é¥°
        emoji_pattern = r'[ğŸ›âœ…ğŸ™‹ğŸ“£âœ‰ï¸ğŸ˜ğŸ“¢ğŸ””ğŸ’¬â¤ï¸ğŸ”—ğŸ“Œ]'
        if re.search(emoji_pattern, text):
            features += 1
        
        # 5. å¤šè¡Œä¸”æ¯è¡Œéƒ½å¾ˆçŸ­ï¼ˆå…¸å‹çš„åˆ—è¡¨æ ¼å¼ï¼‰
        lines = text.split('\n')
        if len(lines) >= 2:
            avg_line_length = sum(len(line) for line in lines) / len(lines)
            if avg_line_length < 30:  # å¹³å‡æ¯è¡Œå°‘äº30å­—ç¬¦
                features += 1
        
        # ç‰¹å¾å¾—åˆ†å¤§äºç­‰äº3è®¤ä¸ºæ˜¯å°¾éƒ¨
        return features >= 3
    
    def _filter_by_ai_semantics(self, content: str, channel_id: str = None) -> Tuple[str, bool, Optional[str]]:
        """
        ä½¿ç”¨AIè¯­ä¹‰åˆ†ææ£€æµ‹å°¾éƒ¨è¾¹ç•Œ
        ä¸ä¾èµ–å›ºå®šè§„åˆ™ï¼Œè€Œæ˜¯ç†è§£å†…å®¹çš„è¯­ä¹‰å˜åŒ–
        """
        if not self.ai_filter or not self.ai_filter.initialized:
            return content, False, None
        
        lines = content.split('\n')
        if len(lines) < 3:
            return content, False, None
        
        try:
            # 1. å°†å†…å®¹åˆ†æ®µï¼ˆæ¯3è¡Œä¸ºä¸€æ®µï¼Œä¿è¯æœ‰è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ï¼‰
            segments = []
            segment_indices = []
            for i in range(0, len(lines), 2):
                segment = '\n'.join(lines[i:min(i+3, len(lines))])
                if segment.strip():  # å¿½ç•¥ç©ºæ®µ
                    segments.append(segment)
                    segment_indices.append(i)
            
            if len(segments) < 2:
                return content, False, None
            
            # 2. è®¡ç®—æ¯ä¸ªæ®µè½çš„è¯­ä¹‰åµŒå…¥
            embeddings = self.ai_filter.model.encode(segments)
            
            # 3. è®¡ç®—ç›¸é‚»æ®µè½ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦
            similarities = []
            for i in range(len(embeddings) - 1):
                sim = cosine_similarity([embeddings[i]], [embeddings[i+1]])[0][0]
                similarities.append(sim)
            
            # 4. æ‰¾åˆ°è¯­ä¹‰çªå˜ç‚¹ï¼ˆç›¸ä¼¼åº¦çªç„¶ä¸‹é™çš„åœ°æ–¹ï¼‰
            if similarities:
                avg_similarity = np.mean(similarities)
                std_similarity = np.std(similarities)
                
                # ä»åå¾€å‰æŸ¥æ‰¾æ˜¾è‘—çš„è¯­ä¹‰å˜åŒ–
                for i in range(len(similarities) - 1, -1, -1):
                    # å¦‚æœç›¸ä¼¼åº¦æ˜æ˜¾ä½äºå¹³å‡å€¼ï¼ˆè¶…è¿‡1ä¸ªæ ‡å‡†å·®ï¼‰
                    if similarities[i] < avg_similarity - std_similarity:
                        # æ£€æŸ¥è¿™ä¸ªåˆ†å‰²ç‚¹åçš„å†…å®¹æ˜¯å¦åƒæ¨å¹¿
                        tail_start_idx = segment_indices[i+1]
                        potential_tail = '\n'.join(lines[tail_start_idx:])
                        
                        # ä½¿ç”¨AIåˆ¤æ–­æ˜¯å¦ä¸ºæ¨å¹¿å†…å®¹
                        if self._is_promotional_content(potential_tail):
                            clean_content = '\n'.join(lines[:tail_start_idx]).rstrip()
                            return clean_content, True, potential_tail
            
            # 5. å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„è¯­ä¹‰çªå˜ï¼Œæ£€æŸ¥æœ€åéƒ¨åˆ†æ˜¯å¦ä¸ºæ¨å¹¿
            # åŠ¨æ€ç¡®å®šæ£€æŸ¥èŒƒå›´ï¼ˆæœ€å20%çš„å†…å®¹ï¼‰
            check_from = max(len(lines) - max(3, len(lines) // 5), 0)
            for i in range(len(lines) - 1, check_from, -1):
                potential_tail = '\n'.join(lines[i:])
                if len(potential_tail) > 20 and self._is_promotional_content(potential_tail):
                    clean_content = '\n'.join(lines[:i]).rstrip()
                    # ç¡®ä¿ä¸ä¼šè¿‡åº¦åˆ é™¤
                    if len(clean_content) > len(content) * 0.5:
                        return clean_content, True, potential_tail
            
        except Exception as e:
            logger.error(f"AIè¯­ä¹‰æ£€æµ‹å¤±è´¥: {e}")
        
        return content, False, None
    
    def _is_promotional_content(self, text: str) -> bool:
        """
        ä½¿ç”¨AIåˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºæ¨å¹¿å†…å®¹
        ä¸ä¾èµ–å›ºå®šå…³é”®è¯ï¼Œè€Œæ˜¯ç†è§£è¯­ä¹‰
        """
        if not text or len(text) < 10:
            return False
        
        # 1. å¿«é€Ÿæ£€æŸ¥æ˜æ˜¾çš„æ¨å¹¿ç‰¹å¾
        # æ£€æŸ¥@usernameæ ¼å¼ï¼ˆTelegramç”¨æˆ·åï¼‰
        username_pattern = r'@[a-zA-Z][a-zA-Z0-9_]{4,}'
        username_count = len(re.findall(username_pattern, text))
        if username_count >= 2:  # 2ä¸ªæˆ–ä»¥ä¸Š@ç”¨æˆ·å
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªå°¾éƒ¨å…³é”®è¯
        keyword_matches = sum(1 for kw in self.tail_keywords if kw in text)
        if keyword_matches >= 2:  # åŒ…å«2ä¸ªæˆ–ä»¥ä¸Šå…³é”®è¯
            return True
        
        # å•ä¸ª@ç”¨æˆ·å + å°¾éƒ¨å…³é”®è¯
        if username_count >= 1 and keyword_matches >= 1:
            return True
        
        # 2. ä½¿ç”¨AIæ¨¡å‹åˆ¤æ–­æ˜¯å¦ä¸ºå¹¿å‘Š/æ¨å¹¿
        if self.ai_filter and self.ai_filter.initialized:
            try:
                # è®¡ç®—ä¸å·²çŸ¥æ¨å¹¿æ¨¡å¼çš„ç›¸ä¼¼åº¦
                if hasattr(self.ai_filter, 'ad_embeddings') and self.ai_filter.ad_embeddings:
                    text_embedding = self.ai_filter.model.encode([text])[0]
                    
                    # ä¸å¹¿å‘Šæ ·æœ¬æ¯”è¾ƒ
                    ad_similarities = []
                    for ad_emb in self.ai_filter.ad_embeddings[:10]:  # æ¯”è¾ƒå‰10ä¸ªæ ·æœ¬
                        sim = cosine_similarity([text_embedding], [ad_emb])[0][0]
                        ad_similarities.append(sim)
                    
                    if ad_similarities and max(ad_similarities) > 0.7:
                        return True
                
                # ä½¿ç”¨AIå¹¿å‘Šæ£€æµ‹å™¨
                is_ad, confidence = self.ai_filter.is_advertisement(text)
                if confidence > 0.5:  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼
                    return is_ad
                    
            except Exception as e:
                logger.debug(f"AIæ¨å¹¿åˆ¤æ–­å¤±è´¥: {e}")
        
        # 3. åŸºæœ¬ç‰¹å¾æ£€æŸ¥ï¼ˆä½œä¸ºè¡¥å……ï¼‰
        # è®¡ç®—é“¾æ¥å¯†åº¦
        link_count = len(re.findall(r'https?://|t\.me/', text))
        if link_count >= 2:  # å¤šä¸ªé“¾æ¥é€šå¸¸æ˜¯æ¨å¹¿
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢‘é“åˆ—è¡¨ç‰¹å¾
        if '|' in text and (link_count >= 1 or username_count >= 1):  # ç”¨|åˆ†éš”çš„é“¾æ¥æˆ–ç”¨æˆ·å
            return True
        
        # æ£€æŸ¥emojiè£…é¥° + é“¾æ¥/ç”¨æˆ·åçš„ç»„åˆ
        emoji_pattern = r'[ğŸ›âœ…ğŸ™‹ğŸ“£âœ‰ï¸ğŸ˜ğŸ“¢ğŸ””ğŸ’¬â¤ï¸ğŸ”—ğŸ“Œ]'
        has_emoji = bool(re.search(emoji_pattern, text))
        if has_emoji and (link_count >= 1 or username_count >= 1):
            return True
        
        return False
    
    def _filter_by_special_format(self, content: str) -> Tuple[str, bool, Optional[str]]:
        """æ£€æµ‹ç‰¹æ®Šæ ¼å¼çš„å°¾éƒ¨æ ‡è¯†ï¼ˆå¦‚ -------[é“¾æ¥] | [é“¾æ¥]ï¼‰"""
        lines = content.split('\n')
        
        for i in range(len(lines) - 1, -1, -1):
            line = lines[i].strip()
            if not line:
                continue
            
            # æ£€æµ‹æ¨¡å¼ï¼šåˆ†éš”ç¬¦åç´§è·Ÿé“¾æ¥ï¼ˆåœ¨åŒä¸€è¡Œï¼‰
            # ä¾‹å¦‚ï¼š-------[ä¸œå—äºšæ— https://t.me/...] | [åšé—»èµ„è®¯](https://...)
            if re.search(r'^[-=*#._~]{3,}.*(\[.*\]|\(.*\)|https?://|t\.me/|@)', line):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é“¾æ¥
                if re.search(r'https?://|t\.me/|@\w+', line):
                    # ä»è¿™ä¸€è¡Œå¼€å§‹åˆ°ç»“å°¾éƒ½æ˜¯å°¾éƒ¨
                    potential_ad = '\n'.join(lines[i:])
                    clean_content = '\n'.join(lines[:i]).rstrip()
                    
                    # å¦‚æœæ¸…ç†åçš„å†…å®¹ä¸ä¸ºç©ºï¼Œè®¤ä¸ºæ‰¾åˆ°äº†å°¾éƒ¨
                    if clean_content:
                        return clean_content, True, potential_ad
            
            # æ£€æµ‹æ¨¡å¼ï¼š[åç§°](é“¾æ¥) | [åç§°](é“¾æ¥) æ ¼å¼
            # æˆ–è€…å¤šä¸ªé“¾æ¥ç”¨ | åˆ†éš”
            if '|' in line and (line.count('http') >= 2 or line.count('t.me') >= 2):
                # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰åˆ†éš”ç¬¦
                has_separator = False
                if i > 0:
                    prev_line = lines[i-1].strip()
                    if self._is_separator_line(prev_line):
                        has_separator = True
                        # ä»åˆ†éš”ç¬¦å¼€å§‹éƒ½æ˜¯å°¾éƒ¨
                        potential_ad = '\n'.join(lines[i-1:])
                        clean_content = '\n'.join(lines[:i-1]).rstrip()
                    else:
                        # ä»å½“å‰è¡Œå¼€å§‹éƒ½æ˜¯å°¾éƒ¨
                        potential_ad = '\n'.join(lines[i:])
                        clean_content = '\n'.join(lines[:i]).rstrip()
                else:
                    # ä»å½“å‰è¡Œå¼€å§‹éƒ½æ˜¯å°¾éƒ¨
                    potential_ad = '\n'.join(lines[i:])
                    clean_content = '\n'.join(lines[:i]).rstrip()
                
                if clean_content:
                    return clean_content, True, potential_ad
        
        return content, False, None
    
    def _filter_by_separator(self, content: str) -> Tuple[str, bool, Optional[str]]:
        """é€šè¿‡åˆ†éš”ç¬¦æ£€æµ‹å¹¶è¿‡æ»¤å°¾éƒ¨æ ‡è¯†"""
        # æŸ¥æ‰¾æ‰€æœ‰åˆ†éš”ç¬¦ä½ç½®
        separator_positions = []
        
        for pattern in self.separator_patterns:
            matches = list(re.finditer(pattern, content))
            for match in matches:
                separator_positions.append({
                    'pos': match.start(),
                    'pattern': pattern,
                    'text': match.group()
                })
        
        if not separator_positions:
            return content, False, None
        
        # æŒ‰ä½ç½®æ’åº
        separator_positions.sort(key=lambda x: x['pos'])
        
        # ä»æœ€åä¸€ä¸ªåˆ†éš”ç¬¦å¼€å§‹æ£€æŸ¥
        for sep in reversed(separator_positions):
            pos = sep['pos']
            
            # è·å–åˆ†éš”ç¬¦åçš„å†…å®¹
            after_separator = content[pos:].strip()
            
            # æ£€æŸ¥åˆ†éš”ç¬¦åçš„å†…å®¹æ˜¯å¦ä¸ºå¹¿å‘Š
            if self._is_ad_section(after_separator):
                # è¿”å›åˆ†éš”ç¬¦å‰çš„å†…å®¹
                clean_content = content[:pos].rstrip()
                ad_part = content[pos:]
                return clean_content, True, ad_part
        
        return content, False, None
    
    def _filter_by_semantic_split(self, content: str) -> Tuple[str, bool, Optional[str]]:
        """é€šè¿‡è¯­ä¹‰åˆ†å‰²æ£€æµ‹å°¾éƒ¨æ ‡è¯†"""
        if not self.ad_detector.initialized:
            return content, False, None
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        
        if len(paragraphs) <= 1:
            return content, False, None
        
        # åˆ†ææ®µè½é—´çš„è¯­ä¹‰è¿è´¯æ€§
        for i in range(len(paragraphs) - 1, 0, -1):
            # æ£€æŸ¥æœ€åiä¸ªæ®µè½æ˜¯å¦ä¸ºå¹¿å‘Š
            potential_ad = '\n\n'.join(paragraphs[i:])
            main_content = '\n\n'.join(paragraphs[:i])
            
            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
            if main_content and potential_ad:
                try:
                    # ä½¿ç”¨ad_detectorçš„è¯­ä¹‰æ£€æŸ¥åŠŸèƒ½
                    coherence = self.ad_detector.check_semantic_coherence(
                        main_content,
                        [potential_ad]
                    )
                    
                    # å¦‚æœç›¸ä¼¼åº¦å¾ˆä½ï¼Œä¸”åŒ…å«å¹¿å‘Šç‰¹å¾
                    if coherence < 0.3 and self._has_ad_features(potential_ad):  # é™ä½é˜ˆå€¼ä»0.4åˆ°0.3
                        logger.debug(f"è¯­ä¹‰åˆ†å‰²æ£€æµ‹åˆ°å°¾éƒ¨æ ‡è¯†ï¼Œç›¸ä¼¼åº¦: {coherence:.3f}")
                        return main_content, True, potential_ad
                
                except Exception as e:
                    logger.error(f"è¯­ä¹‰åˆ†å‰²æ£€æµ‹å¤±è´¥: {e}")
        
        return content, False, None
    
    def _filter_by_link_density(self, content: str) -> Tuple[str, bool, Optional[str]]:
        """é€šè¿‡é“¾æ¥å¯†åº¦æ£€æµ‹å°¾éƒ¨æ ‡è¯†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        lines = content.split('\n')
        
        # ç‰¹æ®Šå¤„ç†ï¼šæ£€æµ‹å•è¡Œå¤šé“¾æ¥çš„æƒ…å†µ
        for i in range(len(lines) - 1, -1, -1):
            line = lines[i].strip()
            if not line:
                continue
            
            # æ£€æµ‹å•è¡Œæ˜¯å¦åŒ…å«å¤šä¸ªé“¾æ¥
            link_count = len(re.findall(r'https?://[^\s]+|t\.me/[^\s]+|@\w+', line))
            link_density = self._calculate_line_link_density(line)
            
            # å•è¡ŒåŒ…å«2ä¸ªä»¥ä¸Šé“¾æ¥ï¼Œæˆ–é“¾æ¥å¯†åº¦è¶…è¿‡0.3
            if link_count >= 2 or link_density > 0.3:
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†éš”ç¬¦åœ¨å‰é¢
                has_separator_before = False
                for j in range(max(0, i-1), i):
                    if self._is_separator_line(lines[j]):
                        has_separator_before = True
                        break
                
                # å¦‚æœæœ‰åˆ†éš”ç¬¦ï¼Œæˆ–è€…é“¾æ¥æ•°é‡>=3ï¼Œè®¤ä¸ºæ˜¯å°¾éƒ¨
                if has_separator_before or link_count >= 3:
                    potential_ad = '\n'.join(lines[i:])
                    if self._is_ad_section(potential_ad):
                        clean_content = '\n'.join(lines[:i]).rstrip()
                        return clean_content, True, potential_ad
        
        # åŸæœ‰çš„è¿ç»­å¤šè¡Œæ£€æµ‹é€»è¾‘ï¼ˆé™ä½é˜ˆå€¼ï¼‰
        ad_start_idx = -1
        consecutive_link_lines = 0
        max_consecutive = 0
        
        for i in range(len(lines) - 1, -1, -1):
            line = lines[i].strip()
            
            if not line:
                continue
            
            # è®¡ç®—è¯¥è¡Œçš„é“¾æ¥å¯†åº¦
            link_density = self._calculate_line_link_density(line)
            
            if link_density > 0.3:  # é“¾æ¥å¯†åº¦é˜ˆå€¼é™ä½åˆ°0.3
                consecutive_link_lines += 1
                if consecutive_link_lines > max_consecutive:
                    max_consecutive = consecutive_link_lines
                    ad_start_idx = i
            else:
                # å¦‚æœå·²ç»æ‰¾åˆ°è¿ç»­çš„é“¾æ¥å¯†é›†è¡Œï¼ˆé™ä½åˆ°2è¡Œï¼‰
                if consecutive_link_lines >= 2:
                    # éªŒè¯æ˜¯å¦ä¸ºå¹¿å‘Š
                    potential_ad = '\n'.join(lines[ad_start_idx:])
                    if self._is_ad_section(potential_ad):
                        clean_content = '\n'.join(lines[:ad_start_idx]).rstrip()
                        return clean_content, True, potential_ad
                
                consecutive_link_lines = 0
        
        # æ£€æŸ¥æœ€åçš„è¿ç»­é“¾æ¥è¡Œ
        if consecutive_link_lines >= 2 and ad_start_idx >= 0:
            potential_ad = '\n'.join(lines[ad_start_idx:])
            if self._is_ad_section(potential_ad):
                clean_content = '\n'.join(lines[:ad_start_idx]).rstrip()
                return clean_content, True, potential_ad
        
        return content, False, None
    
    def _is_separator_line(self, line: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†éš”ç¬¦è¡Œ"""
        line = line.strip()
        if not line:
            return False
        
        for pattern in self.separator_patterns:
            if re.match(pattern, line):
                return True
        return False
    
    def _calculate_line_link_density(self, line: str) -> float:
        """è®¡ç®—å•è¡Œçš„é“¾æ¥å¯†åº¦"""
        if not line:
            return 0.0
        
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        url_pattern = r'https?://[^\s]+|t\.me/[^\s]+|@\w+'
        urls = re.findall(url_pattern, line)
        
        if not urls:
            return 0.0
        
        # è®¡ç®—é“¾æ¥å­—ç¬¦å æ¯”
        url_chars = sum(len(url) for url in urls)
        total_chars = len(line)
        
        return url_chars / total_chars if total_chars > 0 else 0.0
    
    def _is_ad_section(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ®µæ˜¯å¦ä¸ºå¹¿å‘Šï¼ˆä¼˜åŒ–ç‰ˆï¼Œé™ä½è¯¯åˆ¤ï¼‰"""
        if not text:
            return False
        
        # 1. ä½¿ç”¨AIæ£€æµ‹
        if self.ad_detector.initialized:
            is_ad, confidence = self.ad_detector.is_advertisement_ai(text)
            if confidence > 0.7:
                return is_ad
        
        # 2. æ£€æŸ¥å¹¿å‘Šç‰¹å¾ï¼ˆè°ƒæ•´æƒé‡ï¼‰
        ad_score = 0.0
        
        # é“¾æ¥æ•°é‡ï¼ˆé™ä½æƒé‡ï¼‰
        url_count = len(re.findall(r'https?://[^\s]+|t\.me/[^\s]+', text))
        if url_count >= 3:
            ad_score += 0.3  # é™ä½æƒé‡
        elif url_count >= 2:
            ad_score += 0.2
        
        # è¡¨æƒ…ç¬¦å·å¯†åº¦
        emoji_count = len(re.findall(r'[\U0001F300-\U0001F9FF]', text))
        if emoji_count > 5:
            ad_score += 0.2
        
        # é¢‘é“æ¨å¹¿æ¨¡å¼
        if re.search(r'[\U0001F300-\U0001F9FF]\s*\([^)]+\)', text):
            ad_score += 0.3
        
        # è”ç³»æ–¹å¼
        if re.search(r'[@][\w]+|t\.me/\+?\w+', text):
            ad_score += 0.2
        
        # ç‰¹æ®Šæ¨¡å¼ï¼šåˆ†éš”ç¬¦åç´§è·Ÿé“¾æ¥ï¼ˆå¼ºç‰¹å¾ï¼‰
        if re.search(r'^[-=*#._~]{3,}.*https?://', text, re.MULTILINE):
            ad_score += 0.4
        
        # åŒ…å«å¤šä¸ªé¢‘é“é“¾æ¥ç”¨ | åˆ†éš”ï¼ˆå¼ºç‰¹å¾ï¼‰
        if '|' in text and text.count('http') >= 2:
            ad_score += 0.3
        
        return ad_score >= 0.5  # é™ä½é˜ˆå€¼åˆ°0.5
    
    async def learn_from_user_filter(self, channel_id: str, original: str, filtered: str):
        """
        ä»ç”¨æˆ·çš„æ‰‹åŠ¨è¿‡æ»¤ç»“æœä¸­å­¦ä¹ 
        è®°å½•ç”¨æˆ·è®¤ä¸ºçš„å°¾éƒ¨æ¨¡å¼ï¼Œç”¨äºæ”¹è¿›AIåˆ¤æ–­
        """
        if not self.ai_filter or not self.ai_filter.initialized:
            return
        
        try:
            # æå–è¢«ç”¨æˆ·è¿‡æ»¤æ‰çš„å°¾éƒ¨
            if len(filtered) < len(original):
                # æ‰¾åˆ°å°¾éƒ¨å¼€å§‹çš„ä½ç½®
                tail_start = original.find(filtered) + len(filtered) if filtered in original else len(filtered)
                removed_tail = original[tail_start:].strip()
                
                if removed_tail and len(removed_tail) > 10:
                    logger.info(f"å­¦ä¹ ç”¨æˆ·è¿‡æ»¤çš„å°¾éƒ¨æ¨¡å¼ï¼ˆé¢‘é“ {channel_id}ï¼‰")
                    
                    # è®©AIå­¦ä¹ è¿™ä¸ªå°¾éƒ¨æ¨¡å¼
                    if channel_id:
                        # æ”¶é›†è¯¥é¢‘é“çš„å°¾éƒ¨æ ·æœ¬
                        samples = [removed_tail]
                        # å¯ä»¥ä»æ•°æ®åº“è·å–æ›´å¤šè¯¥é¢‘é“çš„å†å²å°¾éƒ¨æ ·æœ¬
                        await self.ai_filter.learn_channel_pattern(channel_id, samples)
                    
                    # æ›´æ–°å¹¿å‘Šæ ·æœ¬åº“
                    if hasattr(self.ai_filter, 'ad_embeddings'):
                        tail_embedding = self.ai_filter.model.encode([removed_tail])[0]
                        self.ai_filter.ad_embeddings.append(tail_embedding)
                        # é™åˆ¶æ ·æœ¬æ•°é‡
                        if len(self.ai_filter.ad_embeddings) > 100:
                            self.ai_filter.ad_embeddings = self.ai_filter.ad_embeddings[-100:]
                    
                    logger.info(f"å·²å­¦ä¹ æ–°çš„å°¾éƒ¨æ¨¡å¼ï¼Œé•¿åº¦: {len(removed_tail)} å­—ç¬¦")
                    
        except Exception as e:
            logger.error(f"å­¦ä¹ ç”¨æˆ·è¿‡æ»¤å¤±è´¥: {e}")
    
    def _has_ad_features(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«å¹¿å‘Šç‰¹å¾"""
        if not text:
            return False
        
        features = {
            'has_links': bool(re.search(r'https?://|t\.me/', text)),
            'has_contact': bool(re.search(r'@\w+|è”ç³»|æŠ•ç¨¿|å’¨è¯¢', text)),
            'has_emojis': len(re.findall(r'[\U0001F300-\U0001F9FF]', text)) > 3,
            'has_channel_list': bool(re.search(r'[\U0001F300-\U0001F9FF]\s*\([^)]+\)', text))
        }
        
        # å¦‚æœæœ‰å¤šä¸ªç‰¹å¾ï¼Œå¯èƒ½æ˜¯å¹¿å‘Š
        feature_count = sum(1 for v in features.values() if v)
        return feature_count >= 2
    
    def analyze_tail_content(self, content: str) -> Dict:
        """åˆ†æå°¾éƒ¨å†…å®¹çš„è¯¦ç»†ä¿¡æ¯"""
        result = {
            'has_tail_ad': False,
            'ad_type': None,
            'confidence': 0.0,
            'ad_position': -1,
            'ad_length': 0
        }
        
        # æ‰§è¡Œè¿‡æ»¤
        clean_content, has_ad, ad_part = self.filter_tail_ads(content)
        
        if has_ad and ad_part:
            result['has_tail_ad'] = True
            result['ad_position'] = len(clean_content)
            result['ad_length'] = len(ad_part)
            
            # åˆ†æå¹¿å‘Šç±»å‹
            if re.search(r'â”{10,}|â•{10,}|â”€{10,}', ad_part):
                result['ad_type'] = 'structured'  # ç»“æ„åŒ–å¹¿å‘Š
            elif len(re.findall(r'https?://|t\.me/', ad_part)) >= 3:
                result['ad_type'] = 'link_cluster'  # é“¾æ¥èšé›†å¹¿å‘Š
            else:
                result['ad_type'] = 'embedded'  # åµŒå…¥å¼å¹¿å‘Š
            
            # è®¡ç®—ç½®ä¿¡åº¦
            if self.ad_detector.initialized:
                _, confidence = self.ad_detector.is_advertisement_ai(ad_part)
                result['confidence'] = confidence
        
        return result


# å…¨å±€å®ä¾‹
smart_tail_filter = SmartTailFilter()