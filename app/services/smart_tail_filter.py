"""
æ™ºèƒ½å°¾éƒ¨è¿‡æ»¤å™¨
è¯†åˆ«å¹¶ç§»é™¤æ¶ˆæ¯å°¾éƒ¨çš„é¢‘é“æ ‡è¯†ï¼Œä¿ç•™æ­£å¸¸å†…å®¹
æ³¨æ„ï¼šå°¾éƒ¨è¿‡æ»¤æ˜¯ç§»é™¤åŸé¢‘é“æ ‡è¯†ï¼Œä¸ç®—å¹¿å‘Š
"""
import logging
import re
from typing import Tuple, Optional, List, Dict
from app.services.ad_detector import ad_detector
from app.services.ai_filter import ai_filter
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)


class SmartTailFilter:
    """æ™ºèƒ½å°¾éƒ¨è¿‡æ»¤å™¨ - çº¯æ•°æ®é©±åŠ¨çš„æœºå™¨å­¦ä¹ """
    
    def __init__(self):
        # ä½¿ç”¨æ–°çš„æ™ºèƒ½è¿‡æ»¤å™¨
        from app.services.intelligent_tail_filter import intelligent_tail_filter
        self.intelligent_filter = intelligent_tail_filter
        
        # ä¿ç•™è¿™äº›ä»¥ä¿æŒå…¼å®¹æ€§
        self.ad_detector = ad_detector
        self.ai_filter = ai_filter
        self.known_tail_patterns = []  # å…¼å®¹æ—§ä»£ç 
        self._load_tail_patterns()  # åŠ è½½è®­ç»ƒæ•°æ®
    
    def filter_tail_ads(self, content: str, channel_id: str = None) -> Tuple[str, bool, Optional[str]]:
        """
        è¿‡æ»¤å°¾éƒ¨é¢‘é“æ ‡è¯† - ä½¿ç”¨æ™ºèƒ½å¼•æ“
        
        Args:
            content: åŸå§‹æ¶ˆæ¯å†…å®¹
            channel_id: é¢‘é“IDï¼ˆä¸å†ä½¿ç”¨ï¼Œä»…ä¸ºå…¼å®¹ï¼‰
            
        Returns:
            (è¿‡æ»¤åå†…å®¹, æ˜¯å¦åŒ…å«å°¾éƒ¨, è¢«è¿‡æ»¤çš„å°¾éƒ¨éƒ¨åˆ†)
        """
        if not content:
            return content, False, None
        
        # ä½¿ç”¨æ™ºèƒ½è¿‡æ»¤å™¨
        try:
            result = self.intelligent_filter.filter_message(content)
            if result[1]:  # å¦‚æœæ£€æµ‹åˆ°å°¾éƒ¨
                logger.info(f"æ™ºèƒ½è¿‡æ»¤å™¨æ£€æµ‹åˆ°å°¾éƒ¨ï¼ŒåŸé•¿åº¦: {len(content)}, è¿‡æ»¤å: {len(result[0])}")
                return result
        except Exception as e:
            logger.error(f"æ™ºèƒ½è¿‡æ»¤å™¨å¼‚å¸¸: {e}")
        
        # 0. ä¼˜å…ˆæ£€æŸ¥æ˜æ˜¾çš„emojiåˆ†éš”ç¬¦ï¼ˆæœ€å¯é çš„æ ‡è¯†ï¼‰
        emoji_separators = [
            r'ğŸ˜‰{5,}',  # è¿ç»­çš„ç¬‘è„¸
            r'ğŸ‘‘{5,}',  # è¿ç»­çš„çš‡å† 
            r'ğŸ”¥{5,}',  # è¿ç»­çš„ç«ç„°
            r'[ğŸ˜‰â˜ºï¸]{10,}',  # æ··åˆè¡¨æƒ…
            r'[ğŸ“£ğŸ”—âœ…ğŸ’¬ğŸ˜]{3,}.*è®¢é˜…',  # è¡¨æƒ…+è®¢é˜…ç»„åˆ
        ]
        
        for pattern in emoji_separators:
            import re
            match = re.search(pattern, content)
            if match:
                # æ‰¾åˆ°emojiåˆ†éš”ç¬¦ï¼Œä»è¿™é‡Œå¼€å§‹éƒ½æ˜¯å°¾éƒ¨
                tail_start = match.start()
                if tail_start > len(content) * 0.3:  # ç¡®ä¿ä¸ä¼šè¿‡åº¦è£å‰ª
                    clean_content = content[:tail_start].rstrip()
                    tail_part = content[tail_start:]
                    # éªŒè¯å°¾éƒ¨ç¡®å®åŒ…å«æ¨å¹¿å†…å®¹
                    if self._is_likely_tail(tail_part):
                        logger.info(f"é€šè¿‡emojiåˆ†éš”ç¬¦æ£€æµ‹åˆ°å°¾éƒ¨ï¼ŒåŸé•¿åº¦: {len(content)}, è¿‡æ»¤å: {len(clean_content)}")
                        return clean_content, True, tail_part
        
        # 1. ç„¶åæ£€æŸ¥å·²çŸ¥çš„ç²¾ç¡®å°¾éƒ¨æ¨¡å¼
        result = self._filter_by_known_patterns(content, channel_id)
        if result[1]:
            # æ·»åŠ å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœè¿‡æ»¤åå†…å®¹å¤ªå°‘ï¼Œå¯èƒ½æ˜¯é”™è¯¯åŒ¹é…
            if len(result[0]) < len(content) * 0.3 and len(content) > 200:
                logger.warning(f"è¿‡æ»¤ç»“æœå¯èƒ½è¿‡åº¦è£å‰ªï¼Œè·³è¿‡æ­¤åŒ¹é…")
            else:
                logger.info(f"ç²¾ç¡®åŒ¹é…åˆ°å·²çŸ¥å°¾éƒ¨ï¼ŒåŸé•¿åº¦: {len(content)}, è¿‡æ»¤å: {len(result[0])}")
                return result
        
        # 1. ä½¿ç”¨æ··åˆæ™ºèƒ½è¿‡æ»¤å™¨ï¼ˆè¯­ä¹‰+ç»“æ„ï¼‰
        from app.services.hybrid_tail_filter import hybrid_tail_filter
        result = hybrid_tail_filter.filter_message(content)
        if result[1]:
            logger.info(f"æ··åˆæ™ºèƒ½è¿‡æ»¤å™¨æ£€æµ‹åˆ°å°¾éƒ¨ï¼ŒåŸé•¿åº¦: {len(content)}, è¿‡æ»¤å: {len(result[0])}")
            return result
        
        # å¦‚æœæ··åˆè¿‡æ»¤å™¨æ²¡æœ‰æ£€æµ‹åˆ°ï¼Œå°è¯•AIï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.ai_filter and self.ai_filter.initialized:
            result = self._filter_by_ai_semantics(content, channel_id)
            if result[1]:
                logger.info(f"AIè¯­ä¹‰æ£€æµ‹åˆ°å°¾éƒ¨ï¼ŒåŸé•¿åº¦: {len(content)}, è¿‡æ»¤å: {len(result[0])}")
                return result
        
        # æ™ºèƒ½è¿‡æ»¤æ— æ³•åˆ¤æ–­æ—¶ï¼Œä½¿ç”¨è§„åˆ™ä½œä¸ºfallback
        
        # 1. ç‰¹æ®Šæ ¼å¼æ£€æµ‹ï¼ˆå¦‚ -------[é“¾æ¥] | [é“¾æ¥]ï¼‰
        result = self._filter_by_special_format(content)
        if result[1]:
            logger.info(f"è§„åˆ™æ£€æµ‹åˆ°ç‰¹æ®Šæ ¼å¼å°¾éƒ¨ï¼ŒåŸé•¿åº¦: {len(content)}, è¿‡æ»¤å: {len(result[0])}")
            return result
        
        # 3. é“¾æ¥å¯†åº¦æ£€æµ‹
        result = self._filter_by_link_density(content)
        if result[1]:
            logger.info(f"è§„åˆ™æ£€æµ‹åˆ°é“¾æ¥å¯†é›†å°¾éƒ¨ï¼ŒåŸé•¿åº¦: {len(content)}, è¿‡æ»¤å: {len(result[0])}")
            return result
        
        return content, False, None
    
    def _load_tail_patterns(self):
        """åŠ è½½è®­ç»ƒçš„å°¾éƒ¨æ¨¡å¼ï¼ˆä¸»è¦ç”¨äºå‘æ™ºèƒ½è¿‡æ»¤å™¨æ·»åŠ æ ·æœ¬ï¼‰"""
        import json
        import os
        from app.core.training_config import TrainingDataConfig
        
        try:
            # åŠ è½½å°¾éƒ¨è¿‡æ»¤æ ·æœ¬
            tail_file = str(TrainingDataConfig.TAIL_FILTER_SAMPLES_FILE)
            if os.path.exists(tail_file):
                with open(tail_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    samples = data.get('samples', data) if isinstance(data, dict) else data
                    
                    # åªæå–tail_partä¾›æ™ºèƒ½è¿‡æ»¤å™¨å­¦ä¹ 
                    tail_count = 0
                    for sample in samples:
                        if sample.get('tail_part'):
                            tail_pattern = sample['tail_part'].strip()
                            if tail_pattern:
                                self.known_tail_patterns.append(tail_pattern)
                                # æ·»åŠ åˆ°æ™ºèƒ½è¿‡æ»¤å™¨
                                self.intelligent_filter.add_training_sample(tail_pattern)
                                tail_count += 1
                
                logger.info(f"åŠ è½½äº† {tail_count} ä¸ªå°¾éƒ¨æ¨¡å¼åˆ°æ™ºèƒ½è¿‡æ»¤å™¨")
                
            # åŠ è½½æ‰‹åŠ¨è®­ç»ƒæ•°æ®
            manual_file = str(TrainingDataConfig.MANUAL_TRAINING_FILE)
            if os.path.exists(manual_file):
                with open(manual_file, 'r', encoding='utf-8') as f:
                    manual_data = json.load(f)
                    for channel_id, channel_data in manual_data.items():
                        if 'samples' in channel_data:
                            channel_name = channel_data.get('channel_name', '')
                            for sample in channel_data['samples']:
                                if sample.get('tail'):
                                    tail_pattern = sample['tail'].strip()
                                    if tail_pattern and tail_pattern not in self.known_tail_patterns:
                                        self.known_tail_patterns.append(tail_pattern)
                                        
                                        # æŒ‰é¢‘é“å­˜å‚¨
                                        if channel_name:
                                            if channel_name not in self.channel_tail_patterns:
                                                self.channel_tail_patterns[channel_name] = []
                                            if tail_pattern not in self.channel_tail_patterns[channel_name]:
                                                self.channel_tail_patterns[channel_name].append(tail_pattern)
                
                logger.info(f"æ€»å…±åŠ è½½äº† {len(self.known_tail_patterns)} ä¸ªå”¯ä¸€å°¾éƒ¨æ¨¡å¼")
                logger.info(f"è¦†ç›– {len(self.channel_tail_patterns)} ä¸ªé¢‘é“")
                
        except Exception as e:
            logger.error(f"åŠ è½½å°¾éƒ¨æ¨¡å¼å¤±è´¥: {e}")
    
    def _filter_by_known_patterns(self, content: str, channel_id: str = None) -> Tuple[str, bool, Optional[str]]:
        """åŸºäºå·²çŸ¥æ¨¡å¼çš„æ™ºèƒ½åŒ¹é…ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if not content or not self.known_tail_patterns:
            return content, False, None
        
        # æ·»åŠ å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœå†…å®¹å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›
        if len(content) < 100:
            return content, False, None
        
        # ä¼˜å…ˆæ£€æŸ¥é¢‘é“ç‰¹å®šçš„æ¨¡å¼
        patterns_to_check = []
        
        # è·å–é¢‘é“ç‰¹å®šæ¨¡å¼
        if channel_id:
            # å°è¯•é€šè¿‡é¢‘é“IDæŸ¥æ‰¾
            for channel_name, patterns in self.channel_tail_patterns.items():
                if channel_id in channel_name or channel_name in str(channel_id):
                    patterns_to_check.extend(patterns)
        
        # å¦‚æœæ²¡æœ‰é¢‘é“ç‰¹å®šæ¨¡å¼ï¼Œä½¿ç”¨æ‰€æœ‰å·²çŸ¥æ¨¡å¼
        if not patterns_to_check:
            patterns_to_check = self.known_tail_patterns
        
        # æ£€æŸ¥æ¯ä¸ªæ¨¡å¼
        for pattern in patterns_to_check:
            if not pattern:
                continue
                
            # 1. å°è¯•ç²¾ç¡®åŒ¹é…ï¼ˆåœ¨å†…å®¹æœ«å°¾ï¼‰
            if content.endswith(pattern):
                clean_content = content[:-len(pattern)].rstrip()
                return clean_content, True, pattern
            
            # 2. æ™ºèƒ½éƒ¨åˆ†åŒ¹é…ï¼ˆå¤„ç†ç»†å¾®å·®å¼‚ï¼‰
            # å°è¯•å¿½ç•¥ç©ºæ ¼å’Œæ¢è¡Œçš„å·®å¼‚
            pattern_normalized = re.sub(r'\s+', ' ', pattern.strip())
            content_tail = content[-len(pattern)*2:] if len(content) > len(pattern)*2 else content
            content_normalized = re.sub(r'\s+', ' ', content_tail.strip())
            
            if pattern_normalized in content_normalized:
                # æ‰¾åˆ°åŒ¹é…ä½ç½®
                idx = content.rfind(pattern_normalized.split()[0] if pattern_normalized.split() else pattern_normalized)
                if idx > 0:
                    potential_tail = content[idx:]
                    clean_content = content[:idx].rstrip()
                    return clean_content, True, potential_tail
            
            # 2.5. å…³é”®è¯åŒ¹é…ï¼ˆå¦‚"åšé—»èµ„è®¯"ï¼‰
            # æ£€æŸ¥æ¨¡å¼ä¸­çš„å…³é”®ç‰¹å¾è¯
            key_phrases = ['åšé—»èµ„è®¯', 'ä¸œå—äºšåƒç“œ', 'è®¢é˜…é¢‘é“', 'ç‚¹å‡»è¿›ç¾¤']
            for phrase in key_phrases:
                if phrase in pattern and phrase in content:
                    # æ‰¾åˆ°å…³é”®è¯çš„ä½ç½®
                    phrase_idx = content.rfind(phrase)
                    if phrase_idx > len(content) * 0.5:  # åœ¨ååŠéƒ¨åˆ†
                        # å‘å‰æŸ¥æ‰¾å¯èƒ½çš„å¼€å§‹ä½ç½®ï¼ˆåˆ†éš”ç¬¦æˆ–æ¢è¡Œï¼‰
                        start_idx = phrase_idx
                        for back_idx in range(phrase_idx - 1, max(0, phrase_idx - 50), -1):
                            if content[back_idx:back_idx+1] in ['\n', '|', 'â”', 'â•', 'â”€', 'â–¬']:
                                start_idx = back_idx
                                break
                        potential_tail = content[start_idx:]
                        if self._is_likely_tail(potential_tail):
                            clean_content = content[:start_idx].rstrip()
                            return clean_content, True, potential_tail
            
            # 3. å…³é”®è¯åºåˆ—åŒ¹é…ï¼ˆæå–æ¨¡å¼ä¸­çš„å…³é”®è¯ï¼‰
            # æå–æ¨¡å¼ä¸­çš„å…³é”®è¯ï¼ˆä¸­æ–‡è¯ã€è‹±æ–‡å•è¯ã€é“¾æ¥ã€ç”¨æˆ·åï¼‰
            keywords = []
            # æå–ä¸­æ–‡è¯ï¼ˆ2ä¸ªå­—ä»¥ä¸Šï¼‰
            chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,}', pattern)
            keywords.extend(chinese_words)
            # æå–è‹±æ–‡å•è¯
            english_words = re.findall(r'\b[A-Za-z]{3,}\b', pattern)
            keywords.extend(english_words)
            # æå–é“¾æ¥å’Œç”¨æˆ·å
            pattern_links = re.findall(r'https?://[^\s\)]+|t\.me/[^\s\)]+', pattern)
            pattern_usernames = re.findall(r'@\w+', pattern)
            keywords.extend(pattern_links)
            keywords.extend(pattern_usernames)
            
            if len(keywords) >= 2:  # è‡³å°‘æœ‰2ä¸ªå…³é”®è¯
                # æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«è¿™äº›å…³é”®è¯çš„åºåˆ—
                matched_count = 0
                last_match_idx = -1
                
                for keyword in keywords:
                    if keyword in content:
                        keyword_idx = content.rfind(keyword)
                        if keyword_idx > last_match_idx:  # ç¡®ä¿é¡ºåº
                            matched_count += 1
                            if last_match_idx == -1:
                                last_match_idx = keyword_idx
                
                # å¦‚æœåŒ¹é…äº†80%ä»¥ä¸Šçš„å…³é”®è¯
                if matched_count >= len(keywords) * 0.8 and last_match_idx > 0:
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå…³é”®è¯çš„ä½ç½®ä½œä¸ºå°¾éƒ¨å¼€å§‹
                    first_keyword_idx = content.rfind(keywords[0])
                    # å‘å‰æŸ¥æ‰¾å¯èƒ½çš„åˆ†éš”ç¬¦
                    search_start = max(0, first_keyword_idx - 100)
                    search_section = content[search_start:first_keyword_idx]
                    
                    # æŸ¥æ‰¾åˆ†éš”ç¬¦
                    sep_patterns = [r'[-=*#_~ã€‚.]{3,}', r'\n{2,}', r'\|{2,}']
                    for sep_pattern in sep_patterns:
                        sep_match = re.search(sep_pattern, search_section)
                        if sep_match:
                            actual_start = search_start + sep_match.start()
                            potential_tail = content[actual_start:]
                            if self._is_likely_tail(potential_tail):
                                clean_content = content[:actual_start].rstrip()
                                return clean_content, True, potential_tail
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œä»ç¬¬ä¸€ä¸ªå…³é”®è¯å¼€å§‹
                    potential_tail = content[first_keyword_idx:]
                    if self._is_likely_tail(potential_tail):
                        clean_content = content[:first_keyword_idx].rstrip()
                        return clean_content, True, potential_tail
            
            # 4. ç»“æ„åŒ–åŒ¹é…ï¼ˆåŸºäºåˆ†éš”ç¬¦ï¼‰
            # æŸ¥æ‰¾æ¨¡å¼ä¸­çš„åˆ†éš”ç¬¦
            separator_matches = re.findall(r'[-=*#_~ã€‚.]{3,}|\|{2,}', pattern)
            for separator in separator_matches:
                # åœ¨å†…å®¹ä¸­æŸ¥æ‰¾ç›¸åŒæˆ–ç›¸ä¼¼çš„åˆ†éš”ç¬¦
                # å…è®¸åˆ†éš”ç¬¦é•¿åº¦æœ‰å·®å¼‚
                sep_char = separator[0]
                flexible_sep_pattern = re.escape(sep_char) + '{3,}'
                
                for match in re.finditer(flexible_sep_pattern, content):
                    idx = match.start()
                    potential_tail = content[idx:]
                    # éªŒè¯æ˜¯å¦ä¸ºæ¨å¹¿å†…å®¹
                    if self._is_likely_tail(potential_tail):
                        clean_content = content[:idx].rstrip()
                        return clean_content, True, potential_tail
        
        # æ²¡æœ‰åŒ¹é…åˆ°å·²çŸ¥æ¨¡å¼
        return content, False, None
    
    
    def _is_likely_tail(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦å¯èƒ½æ˜¯å°¾éƒ¨æ¨å¹¿"""
        if not text:
            return False
        
        # ç‰¹å¾è®¡æ•°
        features = 0
        
        # 1. åŒ…å«@ç¬¦å·ï¼ˆTelegramç”¨æˆ·åï¼‰
        if '@' in text:
            features += 2
        
        # 2. åŒ…å«é“¾æ¥
        if 'http' in text or 't.me' in text:
            features += 2
        
        # 3. åŒ…å«å¤šä¸ªè¡¨æƒ…ç¬¦å·ï¼ˆæ¨å¹¿å†…å®¹å¸¸ç”¨è¡¨æƒ…è£…é¥°ï¼‰
        emoji_count = len(re.findall(r'[\U0001F300-\U0001F9FF]', text))
        if emoji_count > 5:
            features += 1
        
        # 4. åŒ…å«emojiè£…é¥°
        emoji_pattern = r'[ğŸ›âœ…ğŸ™‹ğŸ“£âœ‰ï¸ğŸ˜ğŸ“¢ğŸ””ğŸ’¬â¤ï¸ğŸ”—ğŸ“Œ]'
        if re.search(emoji_pattern, text):
            features += 1
        
        # 5. å¤šè¡Œä¸”æ¯è¡Œéƒ½å¾ˆçŸ­ï¼ˆå…¸å‹çš„åˆ—è¡¨æ ¼å¼ï¼‰
        lines = text.split('\n')
        if len(lines) >= 2:
            avg_line_length = sum(len(line) for line in lines) / len(lines)
            if avg_line_length < 30:  # å¹³å‡æ¯è¡Œå°‘äº30å­—ç¬¦
                features += 1
        
        # ç‰¹å¾å¾—åˆ†å¤§äºç­‰äº3è®¤ä¸ºæ˜¯å°¾éƒ¨
        return features >= 3
    
    def _filter_by_ai_semantics(self, content: str, channel_id: str = None) -> Tuple[str, bool, Optional[str]]:
        """
        ä½¿ç”¨AIè¯­ä¹‰åˆ†ææ£€æµ‹å°¾éƒ¨è¾¹ç•Œ
        ä¸ä¾èµ–å›ºå®šè§„åˆ™ï¼Œè€Œæ˜¯ç†è§£å†…å®¹çš„è¯­ä¹‰å˜åŒ–
        """
        if not self.ai_filter or not self.ai_filter.initialized:
            return content, False, None
        
        lines = content.split('\n')
        if len(lines) < 3:
            return content, False, None
        
        try:
            # 1. å°†å†…å®¹åˆ†æ®µï¼ˆæ¯3è¡Œä¸ºä¸€æ®µï¼Œä¿è¯æœ‰è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ï¼‰
            segments = []
            segment_indices = []
            for i in range(0, len(lines), 2):
                segment = '\n'.join(lines[i:min(i+3, len(lines))])
                if segment.strip():  # å¿½ç•¥ç©ºæ®µ
                    segments.append(segment)
                    segment_indices.append(i)
            
            if len(segments) < 2:
                return content, False, None
            
            # 2. è®¡ç®—æ¯ä¸ªæ®µè½çš„è¯­ä¹‰åµŒå…¥
            embeddings = self.ai_filter.model.encode(segments)
            
            # 3. è®¡ç®—ç›¸é‚»æ®µè½ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦
            similarities = []
            for i in range(len(embeddings) - 1):
                sim = cosine_similarity([embeddings[i]], [embeddings[i+1]])[0][0]
                similarities.append(sim)
            
            # 4. æ‰¾åˆ°è¯­ä¹‰çªå˜ç‚¹ï¼ˆç›¸ä¼¼åº¦çªç„¶ä¸‹é™çš„åœ°æ–¹ï¼‰
            if similarities:
                avg_similarity = np.mean(similarities)
                std_similarity = np.std(similarities)
                
                # ä»åå¾€å‰æŸ¥æ‰¾æ˜¾è‘—çš„è¯­ä¹‰å˜åŒ–
                for i in range(len(similarities) - 1, -1, -1):
                    # å¦‚æœç›¸ä¼¼åº¦æ˜æ˜¾ä½äºå¹³å‡å€¼ï¼ˆè¶…è¿‡1ä¸ªæ ‡å‡†å·®ï¼‰
                    if similarities[i] < avg_similarity - std_similarity:
                        # æ£€æŸ¥è¿™ä¸ªåˆ†å‰²ç‚¹åçš„å†…å®¹æ˜¯å¦åƒæ¨å¹¿
                        tail_start_idx = segment_indices[i+1]
                        potential_tail = '\n'.join(lines[tail_start_idx:])
                        
                        # ä½¿ç”¨AIåˆ¤æ–­æ˜¯å¦ä¸ºæ¨å¹¿å†…å®¹
                        if self._is_promotional_content(potential_tail):
                            clean_content = '\n'.join(lines[:tail_start_idx]).rstrip()
                            return clean_content, True, potential_tail
            
            # 5. å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„è¯­ä¹‰çªå˜ï¼Œæ£€æŸ¥æœ€åéƒ¨åˆ†æ˜¯å¦ä¸ºæ¨å¹¿
            # åŠ¨æ€ç¡®å®šæ£€æŸ¥èŒƒå›´ï¼ˆæœ€å20%çš„å†…å®¹ï¼‰
            check_from = max(len(lines) - max(3, len(lines) // 5), 0)
            for i in range(len(lines) - 1, check_from, -1):
                potential_tail = '\n'.join(lines[i:])
                if len(potential_tail) > 20 and self._is_promotional_content(potential_tail):
                    clean_content = '\n'.join(lines[:i]).rstrip()
                    # ç¡®ä¿ä¸ä¼šè¿‡åº¦åˆ é™¤
                    if len(clean_content) > len(content) * 0.5:
                        return clean_content, True, potential_tail
            
        except Exception as e:
            logger.error(f"AIè¯­ä¹‰æ£€æµ‹å¤±è´¥: {e}")
        
        return content, False, None
    
    def _is_promotional_content(self, text: str) -> bool:
        """
        ä½¿ç”¨AIåˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºæ¨å¹¿å†…å®¹
        ä¸ä¾èµ–å›ºå®šå…³é”®è¯ï¼Œè€Œæ˜¯ç†è§£è¯­ä¹‰
        """
        if not text or len(text) < 10:
            return False
        
        # 1. å¿«é€Ÿæ£€æŸ¥æ˜æ˜¾çš„æ¨å¹¿ç‰¹å¾
        # æ£€æŸ¥@usernameæ ¼å¼ï¼ˆTelegramç”¨æˆ·åï¼‰
        username_pattern = r'@[a-zA-Z][a-zA-Z0-9_]{4,}'
        username_count = len(re.findall(username_pattern, text))
        if username_count >= 2:  # 2ä¸ªæˆ–ä»¥ä¸Š@ç”¨æˆ·å
            return True
        
        # æ£€æŸ¥é“¾æ¥å¯†åº¦
        link_count = len(re.findall(r'https?://|t\.me/', text))
        text_length = len(text)
        if text_length > 0:
            link_density = link_count * 100 / text_length  # é“¾æ¥å­—ç¬¦å æ¯”
            if link_density > 5:  # é“¾æ¥å¯†åº¦è¶…è¿‡5%
                return True
        
        # @ç”¨æˆ·å + é“¾æ¥çš„ç»„åˆï¼ˆå…¸å‹æ¨å¹¿æ¨¡å¼ï¼‰
        if username_count >= 1 and link_count >= 1:
            return True
        
        # 2. ä½¿ç”¨AIæ¨¡å‹åˆ¤æ–­æ˜¯å¦ä¸ºå¹¿å‘Š/æ¨å¹¿
        if self.ai_filter and self.ai_filter.initialized:
            try:
                # è®¡ç®—ä¸å·²çŸ¥æ¨å¹¿æ¨¡å¼çš„ç›¸ä¼¼åº¦
                if hasattr(self.ai_filter, 'ad_embeddings') and self.ai_filter.ad_embeddings:
                    text_embedding = self.ai_filter.model.encode([text])[0]
                    
                    # ä¸å¹¿å‘Šæ ·æœ¬æ¯”è¾ƒ
                    ad_similarities = []
                    for ad_emb in self.ai_filter.ad_embeddings[:10]:  # æ¯”è¾ƒå‰10ä¸ªæ ·æœ¬
                        sim = cosine_similarity([text_embedding], [ad_emb])[0][0]
                        ad_similarities.append(sim)
                    
                    if ad_similarities and max(ad_similarities) > 0.7:
                        return True
                
                # ä½¿ç”¨AIå¹¿å‘Šæ£€æµ‹å™¨
                is_ad, confidence = self.ai_filter.is_advertisement(text)
                if confidence > 0.5:  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼
                    return is_ad
                    
            except Exception as e:
                logger.debug(f"AIæ¨å¹¿åˆ¤æ–­å¤±è´¥: {e}")
        
        # 3. åŸºæœ¬ç‰¹å¾æ£€æŸ¥ï¼ˆä½œä¸ºè¡¥å……ï¼‰
        # è®¡ç®—é“¾æ¥å¯†åº¦
        link_count = len(re.findall(r'https?://|t\.me/', text))
        if link_count >= 2:  # å¤šä¸ªé“¾æ¥é€šå¸¸æ˜¯æ¨å¹¿
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢‘é“åˆ—è¡¨ç‰¹å¾
        if '|' in text and (link_count >= 1 or username_count >= 1):  # ç”¨|åˆ†éš”çš„é“¾æ¥æˆ–ç”¨æˆ·å
            return True
        
        # æ£€æŸ¥emojiè£…é¥° + é“¾æ¥/ç”¨æˆ·åçš„ç»„åˆ
        emoji_pattern = r'[ğŸ›âœ…ğŸ™‹ğŸ“£âœ‰ï¸ğŸ˜ğŸ“¢ğŸ””ğŸ’¬â¤ï¸ğŸ”—ğŸ“Œ]'
        has_emoji = bool(re.search(emoji_pattern, text))
        if has_emoji and (link_count >= 1 or username_count >= 1):
            return True
        
        return False
    
    def _filter_by_special_format(self, content: str) -> Tuple[str, bool, Optional[str]]:
        """æ£€æµ‹ç‰¹æ®Šæ ¼å¼çš„å°¾éƒ¨æ ‡è¯†ï¼ˆå¦‚ -------[é“¾æ¥] | [é“¾æ¥]ï¼‰"""
        lines = content.split('\n')
        
        for i in range(len(lines) - 1, -1, -1):
            line = lines[i].strip()
            if not line:
                continue
            
            # æ£€æµ‹æ¨¡å¼ï¼šåˆ†éš”ç¬¦åç´§è·Ÿé“¾æ¥ï¼ˆåœ¨åŒä¸€è¡Œï¼‰
            # ä¾‹å¦‚ï¼š-------[ä¸œå—äºšæ— https://t.me/...] | [åšé—»èµ„è®¯](https://...)
            if re.search(r'^[-=*#._~]{3,}.*(\[.*\]|\(.*\)|https?://|t\.me/|@)', line):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é“¾æ¥
                if re.search(r'https?://|t\.me/|@\w+', line):
                    # ä»è¿™ä¸€è¡Œå¼€å§‹åˆ°ç»“å°¾éƒ½æ˜¯å°¾éƒ¨
                    potential_ad = '\n'.join(lines[i:])
                    clean_content = '\n'.join(lines[:i]).rstrip()
                    
                    # å¦‚æœæ¸…ç†åçš„å†…å®¹ä¸ä¸ºç©ºï¼Œè®¤ä¸ºæ‰¾åˆ°äº†å°¾éƒ¨
                    if clean_content:
                        return clean_content, True, potential_ad
            
            # æ£€æµ‹æ¨¡å¼ï¼š[åç§°](é“¾æ¥) | [åç§°](é“¾æ¥) æ ¼å¼
            # æˆ–è€…å¤šä¸ªé“¾æ¥ç”¨ | åˆ†éš”
            if '|' in line and (line.count('http') >= 2 or line.count('t.me') >= 2):
                # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰åˆ†éš”ç¬¦
                has_separator = False
                if i > 0:
                    prev_line = lines[i-1].strip()
                    if self._is_separator_line(prev_line):
                        has_separator = True
                        # ä»åˆ†éš”ç¬¦å¼€å§‹éƒ½æ˜¯å°¾éƒ¨
                        potential_ad = '\n'.join(lines[i-1:])
                        clean_content = '\n'.join(lines[:i-1]).rstrip()
                    else:
                        # ä»å½“å‰è¡Œå¼€å§‹éƒ½æ˜¯å°¾éƒ¨
                        potential_ad = '\n'.join(lines[i:])
                        clean_content = '\n'.join(lines[:i]).rstrip()
                else:
                    # ä»å½“å‰è¡Œå¼€å§‹éƒ½æ˜¯å°¾éƒ¨
                    potential_ad = '\n'.join(lines[i:])
                    clean_content = '\n'.join(lines[:i]).rstrip()
                
                if clean_content:
                    return clean_content, True, potential_ad
        
        return content, False, None
    
    
    def _filter_by_semantic_split(self, content: str) -> Tuple[str, bool, Optional[str]]:
        """é€šè¿‡è¯­ä¹‰åˆ†å‰²æ£€æµ‹å°¾éƒ¨æ ‡è¯†"""
        if not self.ad_detector.initialized:
            return content, False, None
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        
        if len(paragraphs) <= 1:
            return content, False, None
        
        # åˆ†ææ®µè½é—´çš„è¯­ä¹‰è¿è´¯æ€§
        for i in range(len(paragraphs) - 1, 0, -1):
            # æ£€æŸ¥æœ€åiä¸ªæ®µè½æ˜¯å¦ä¸ºå¹¿å‘Š
            potential_ad = '\n\n'.join(paragraphs[i:])
            main_content = '\n\n'.join(paragraphs[:i])
            
            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
            if main_content and potential_ad:
                try:
                    # ä½¿ç”¨ad_detectorçš„è¯­ä¹‰æ£€æŸ¥åŠŸèƒ½
                    coherence = self.ad_detector.check_semantic_coherence(
                        main_content,
                        [potential_ad]
                    )
                    
                    # å¦‚æœç›¸ä¼¼åº¦å¾ˆä½ï¼Œä¸”åŒ…å«å¹¿å‘Šç‰¹å¾
                    if coherence < 0.3 and self._has_ad_features(potential_ad):  # é™ä½é˜ˆå€¼ä»0.4åˆ°0.3
                        logger.debug(f"è¯­ä¹‰åˆ†å‰²æ£€æµ‹åˆ°å°¾éƒ¨æ ‡è¯†ï¼Œç›¸ä¼¼åº¦: {coherence:.3f}")
                        return main_content, True, potential_ad
                
                except Exception as e:
                    logger.error(f"è¯­ä¹‰åˆ†å‰²æ£€æµ‹å¤±è´¥: {e}")
        
        return content, False, None
    
    def _filter_by_link_density(self, content: str) -> Tuple[str, bool, Optional[str]]:
        """é€šè¿‡é“¾æ¥å¯†åº¦æ£€æµ‹å°¾éƒ¨æ ‡è¯†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        lines = content.split('\n')
        
        # ç‰¹æ®Šå¤„ç†ï¼šæ£€æµ‹å•è¡Œå¤šé“¾æ¥çš„æƒ…å†µ
        for i in range(len(lines) - 1, -1, -1):
            line = lines[i].strip()
            if not line:
                continue
            
            # æ£€æµ‹å•è¡Œæ˜¯å¦åŒ…å«å¤šä¸ªé“¾æ¥
            link_count = len(re.findall(r'https?://[^\s]+|t\.me/[^\s]+|@\w+', line))
            link_density = self._calculate_line_link_density(line)
            
            # å•è¡ŒåŒ…å«2ä¸ªä»¥ä¸Šé“¾æ¥ï¼Œæˆ–é“¾æ¥å¯†åº¦è¶…è¿‡0.3
            if link_count >= 2 or link_density > 0.3:
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†éš”ç¬¦åœ¨å‰é¢
                has_separator_before = False
                for j in range(max(0, i-1), i):
                    if self._is_separator_line(lines[j]):
                        has_separator_before = True
                        break
                
                # å¦‚æœæœ‰åˆ†éš”ç¬¦ï¼Œæˆ–è€…é“¾æ¥æ•°é‡>=3ï¼Œè®¤ä¸ºæ˜¯å°¾éƒ¨
                if has_separator_before or link_count >= 3:
                    potential_ad = '\n'.join(lines[i:])
                    if self._is_ad_section(potential_ad):
                        clean_content = '\n'.join(lines[:i]).rstrip()
                        return clean_content, True, potential_ad
        
        # åŸæœ‰çš„è¿ç»­å¤šè¡Œæ£€æµ‹é€»è¾‘ï¼ˆé™ä½é˜ˆå€¼ï¼‰
        ad_start_idx = -1
        consecutive_link_lines = 0
        max_consecutive = 0
        
        for i in range(len(lines) - 1, -1, -1):
            line = lines[i].strip()
            
            if not line:
                continue
            
            # è®¡ç®—è¯¥è¡Œçš„é“¾æ¥å¯†åº¦
            link_density = self._calculate_line_link_density(line)
            
            if link_density > 0.3:  # é“¾æ¥å¯†åº¦é˜ˆå€¼é™ä½åˆ°0.3
                consecutive_link_lines += 1
                if consecutive_link_lines > max_consecutive:
                    max_consecutive = consecutive_link_lines
                    ad_start_idx = i
            else:
                # å¦‚æœå·²ç»æ‰¾åˆ°è¿ç»­çš„é“¾æ¥å¯†é›†è¡Œï¼ˆé™ä½åˆ°2è¡Œï¼‰
                if consecutive_link_lines >= 2:
                    # éªŒè¯æ˜¯å¦ä¸ºå¹¿å‘Š
                    potential_ad = '\n'.join(lines[ad_start_idx:])
                    if self._is_ad_section(potential_ad):
                        clean_content = '\n'.join(lines[:ad_start_idx]).rstrip()
                        return clean_content, True, potential_ad
                
                consecutive_link_lines = 0
        
        # æ£€æŸ¥æœ€åçš„è¿ç»­é“¾æ¥è¡Œ
        if consecutive_link_lines >= 2 and ad_start_idx >= 0:
            potential_ad = '\n'.join(lines[ad_start_idx:])
            if self._is_ad_section(potential_ad):
                clean_content = '\n'.join(lines[:ad_start_idx]).rstrip()
                return clean_content, True, potential_ad
        
        return content, False, None
    
    def _is_separator_line(self, line: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†éš”ç¬¦è¡Œï¼ˆåŸºäºå¸¸è§æ¨¡å¼ï¼‰"""
        line = line.strip()
        if not line:
            return False
        
        # ç›´æ¥æ£€æŸ¥å¸¸è§çš„åˆ†éš”ç¬¦æ¨¡å¼
        import re
        separator_patterns = [
            r'^[-=*#_~â”â•â”€â–¬]{3,}$',  # å„ç§åˆ†éš”ç¬¦
            r'^[ğŸ˜‰â˜ºï¸ğŸ‘‘ğŸ”¥]{5,}$',    # emojiåˆ†éš”ç¬¦
        ]
        
        for pattern in separator_patterns:
            if re.match(pattern, line):
                return True
        return False
    
    def _calculate_line_link_density(self, line: str) -> float:
        """è®¡ç®—å•è¡Œçš„é“¾æ¥å¯†åº¦"""
        if not line:
            return 0.0
        
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        url_pattern = r'https?://[^\s]+|t\.me/[^\s]+|@\w+'
        urls = re.findall(url_pattern, line)
        
        if not urls:
            return 0.0
        
        # è®¡ç®—é“¾æ¥å­—ç¬¦å æ¯”
        url_chars = sum(len(url) for url in urls)
        total_chars = len(line)
        
        return url_chars / total_chars if total_chars > 0 else 0.0
    
    def _is_ad_section(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ®µæ˜¯å¦ä¸ºå¹¿å‘Šï¼ˆä¼˜åŒ–ç‰ˆï¼Œé™ä½è¯¯åˆ¤ï¼‰"""
        if not text:
            return False
        
        # 1. ä½¿ç”¨AIæ£€æµ‹
        if self.ad_detector.initialized:
            is_ad, confidence = self.ad_detector.is_advertisement_ai(text)
            if confidence > 0.7:
                return is_ad
        
        # 2. æ£€æŸ¥å¹¿å‘Šç‰¹å¾ï¼ˆè°ƒæ•´æƒé‡ï¼‰
        ad_score = 0.0
        
        # é“¾æ¥æ•°é‡ï¼ˆé™ä½æƒé‡ï¼‰
        url_count = len(re.findall(r'https?://[^\s]+|t\.me/[^\s]+', text))
        if url_count >= 3:
            ad_score += 0.3  # é™ä½æƒé‡
        elif url_count >= 2:
            ad_score += 0.2
        
        # è¡¨æƒ…ç¬¦å·å¯†åº¦
        emoji_count = len(re.findall(r'[\U0001F300-\U0001F9FF]', text))
        if emoji_count > 5:
            ad_score += 0.2
        
        # é¢‘é“æ¨å¹¿æ¨¡å¼
        if re.search(r'[\U0001F300-\U0001F9FF]\s*\([^)]+\)', text):
            ad_score += 0.3
        
        # è”ç³»æ–¹å¼
        if re.search(r'[@][\w]+|t\.me/\+?\w+', text):
            ad_score += 0.2
        
        # ç‰¹æ®Šæ¨¡å¼ï¼šåˆ†éš”ç¬¦åç´§è·Ÿé“¾æ¥ï¼ˆå¼ºç‰¹å¾ï¼‰
        if re.search(r'^[-=*#._~]{3,}.*https?://', text, re.MULTILINE):
            ad_score += 0.4
        
        # åŒ…å«å¤šä¸ªé¢‘é“é“¾æ¥ç”¨ | åˆ†éš”ï¼ˆå¼ºç‰¹å¾ï¼‰
        if '|' in text and text.count('http') >= 2:
            ad_score += 0.3
        
        return ad_score >= 0.5  # é™ä½é˜ˆå€¼åˆ°0.5
    
    async def learn_from_user_filter(self, channel_id: str, original: str, filtered: str):
        """
        ä»ç”¨æˆ·çš„æ‰‹åŠ¨è¿‡æ»¤ç»“æœä¸­å­¦ä¹ 
        è®°å½•ç”¨æˆ·è®¤ä¸ºçš„å°¾éƒ¨æ¨¡å¼ï¼Œç”¨äºæ”¹è¿›AIåˆ¤æ–­
        """
        if not self.ai_filter or not self.ai_filter.initialized:
            return
        
        try:
            # æå–è¢«ç”¨æˆ·è¿‡æ»¤æ‰çš„å°¾éƒ¨
            if len(filtered) < len(original):
                # æ‰¾åˆ°å°¾éƒ¨å¼€å§‹çš„ä½ç½®
                tail_start = original.find(filtered) + len(filtered) if filtered in original else len(filtered)
                removed_tail = original[tail_start:].strip()
                
                if removed_tail and len(removed_tail) > 10:
                    logger.info(f"å­¦ä¹ ç”¨æˆ·è¿‡æ»¤çš„å°¾éƒ¨æ¨¡å¼ï¼ˆé¢‘é“ {channel_id}ï¼‰")
                    
                    # è®©AIå­¦ä¹ è¿™ä¸ªå°¾éƒ¨æ¨¡å¼
                    if channel_id:
                        # æ”¶é›†è¯¥é¢‘é“çš„å°¾éƒ¨æ ·æœ¬
                        samples = [removed_tail]
                        # å¯ä»¥ä»æ•°æ®åº“è·å–æ›´å¤šè¯¥é¢‘é“çš„å†å²å°¾éƒ¨æ ·æœ¬
                        await self.ai_filter.learn_channel_pattern(channel_id, samples)
                    
                    # æ›´æ–°å¹¿å‘Šæ ·æœ¬åº“
                    if hasattr(self.ai_filter, 'ad_embeddings'):
                        tail_embedding = self.ai_filter.model.encode([removed_tail])[0]
                        self.ai_filter.ad_embeddings.append(tail_embedding)
                        # é™åˆ¶æ ·æœ¬æ•°é‡
                        if len(self.ai_filter.ad_embeddings) > 100:
                            self.ai_filter.ad_embeddings = self.ai_filter.ad_embeddings[-100:]
                    
                    logger.info(f"å·²å­¦ä¹ æ–°çš„å°¾éƒ¨æ¨¡å¼ï¼Œé•¿åº¦: {len(removed_tail)} å­—ç¬¦")
                    
        except Exception as e:
            logger.error(f"å­¦ä¹ ç”¨æˆ·è¿‡æ»¤å¤±è´¥: {e}")
    
    def _has_ad_features(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«å¹¿å‘Šç‰¹å¾"""
        if not text:
            return False
        
        features = {
            'has_links': bool(re.search(r'https?://|t\.me/', text)),
            'has_contact': bool(re.search(r'@\w+|è”ç³»|æŠ•ç¨¿|å’¨è¯¢', text)),
            'has_emojis': len(re.findall(r'[\U0001F300-\U0001F9FF]', text)) > 3,
            'has_channel_list': bool(re.search(r'[\U0001F300-\U0001F9FF]\s*\([^)]+\)', text))
        }
        
        # å¦‚æœæœ‰å¤šä¸ªç‰¹å¾ï¼Œå¯èƒ½æ˜¯å¹¿å‘Š
        feature_count = sum(1 for v in features.values() if v)
        return feature_count >= 2
    
    def analyze_tail_content(self, content: str) -> Dict:
        """åˆ†æå°¾éƒ¨å†…å®¹çš„è¯¦ç»†ä¿¡æ¯"""
        result = {
            'has_tail_ad': False,
            'ad_type': None,
            'confidence': 0.0,
            'ad_position': -1,
            'ad_length': 0
        }
        
        # æ‰§è¡Œè¿‡æ»¤
        clean_content, has_ad, ad_part = self.filter_tail_ads(content)
        
        if has_ad and ad_part:
            result['has_tail_ad'] = True
            result['ad_position'] = len(clean_content)
            result['ad_length'] = len(ad_part)
            
            # åˆ†æå¹¿å‘Šç±»å‹
            if re.search(r'â”{10,}|â•{10,}|â”€{10,}', ad_part):
                result['ad_type'] = 'structured'  # ç»“æ„åŒ–å¹¿å‘Š
            elif len(re.findall(r'https?://|t\.me/', ad_part)) >= 3:
                result['ad_type'] = 'link_cluster'  # é“¾æ¥èšé›†å¹¿å‘Š
            else:
                result['ad_type'] = 'embedded'  # åµŒå…¥å¼å¹¿å‘Š
            
            # è®¡ç®—ç½®ä¿¡åº¦
            if self.ad_detector.initialized:
                _, confidence = self.ad_detector.is_advertisement_ai(ad_part)
                result['confidence'] = confidence
        
        return result


# å…¨å±€å®ä¾‹
smart_tail_filter = SmartTailFilter()