"""
æ¶ˆæ¯ç»„åˆå¤„ç†å™¨ - å¤„ç†Telegramçš„æ¶ˆæ¯ç»„åˆåŠŸèƒ½
"""
import logging
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from app.utils.timezone import get_current_time
from sqlalchemy import select
from app.core.database import AsyncSessionLocal, Message

logger = logging.getLogger(__name__)

class MessageGrouper:
    """æ¶ˆæ¯ç»„åˆå¤„ç†å™¨"""
    
    def __init__(self):
        self.pending_groups: Dict[str, List[Dict]] = {}  # å¾…å¤„ç†çš„æ¶ˆæ¯ç»„
        self.completed_groups: Dict[str, Dict] = {}  # å·²å®Œæˆçš„ç»„åˆæ¶ˆæ¯æ•°æ®
        self.group_timers: Dict[str, asyncio.Task] = {}  # ç»„åˆè¶…æ—¶å®šæ—¶å™¨
        self.group_timeout = 10  # æ¶ˆæ¯ç»„åˆè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰- ç»Ÿä¸€ä½¿ç”¨10ç§’
        self.telegram_messages: Dict[str, Any] = {}  # ä¿å­˜åŸå§‹Telegramæ¶ˆæ¯å¯¹è±¡ï¼Œç”¨äºå¼‚æ­¥ä¸‹è½½
    
    async def process_message(self, message, channel_id: str, media_info: Optional[Dict] = None, filtered_content: Optional[str] = None, is_ad: bool = False, is_batch: bool = False) -> Optional[Dict]:
        """
        å¤„ç†æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ä¸å…¶ä»–æ¶ˆæ¯ç»„åˆ
        è¿”å›å®Œæ•´çš„ç»„åˆæ¶ˆæ¯æˆ–Noneï¼ˆå¦‚æœæ¶ˆæ¯è¿˜åœ¨ç­‰å¾…ç»„åˆï¼‰
        
        Args:
            is_batch: æ˜¯å¦ä¸ºæ‰¹é‡å¤„ç†æ¨¡å¼ï¼ˆå¦‚å†å²æ¶ˆæ¯é‡‡é›†ï¼‰ï¼Œæ‰¹é‡æ¨¡å¼ä¸‹ä¼šç«‹å³å¤„ç†å®Œæ•´ä¸ªç»„
        """
        try:
            # æå–æ¶ˆæ¯åŸºæœ¬ä¿¡æ¯
            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„filtered_contentä½œä¸ºå†…å®¹ï¼ˆå·²ç»åŒ…å«captionï¼‰
            original_content = filtered_content if filtered_content is not None else ""
            
            # å¦‚æœæ²¡æœ‰ä¼ å…¥filtered_contentï¼Œå†ä»æ¶ˆæ¯å¯¹è±¡æå–
            if original_content == "" and hasattr(message, 'text'):
                original_content = message.text or message.raw_text or ""
                # æ£€æŸ¥caption
                if not original_content and hasattr(message, 'caption'):
                    original_content = message.caption or ""
            
            message_data = {
                'message_id': message.id,
                'content': original_content,  # ä½¿ç”¨æå–æˆ–ä¼ å…¥çš„å†…å®¹
                'filtered_content': filtered_content if filtered_content is not None else original_content,
                'is_ad': is_ad,
                'media_info': media_info,
                'date': message.date or get_current_time(),
                'grouped_id': str(getattr(message, 'grouped_id', None)) if getattr(message, 'grouped_id', None) else None
            }
            
            # å¦‚æœæ²¡æœ‰grouped_idï¼Œè¯´æ˜æ˜¯ç‹¬ç«‹æ¶ˆæ¯
            if not message_data['grouped_id']:
                return await self._create_single_message(message_data, channel_id)
            
            # æœ‰grouped_idï¼Œéœ€è¦å¤„ç†æ¶ˆæ¯ç»„åˆ
            if is_batch:
                # æ‰¹é‡æ¨¡å¼ï¼Œä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶æˆ–ç«‹å³å¤„ç†
                return await self._handle_grouped_message_batch(message_data, channel_id)
            else:
                # å®æ—¶æ¨¡å¼ï¼Œä½¿ç”¨æ­£å¸¸çš„è¶…æ—¶æœºåˆ¶
                return await self._handle_grouped_message(message_data, channel_id)
            
        except Exception as e:
            logger.error(f"å¤„ç†æ¶ˆæ¯ç»„åˆæ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶è¿”å›å•ç‹¬æ¶ˆæ¯
            return await self._create_single_message(message_data if 'message_data' in locals() else {
                'message_id': message.id,
                'content': filtered_content if filtered_content is not None else (message.text or message.caption if hasattr(message, 'caption') else ""),
                'filtered_content': filtered_content,
                'is_ad': is_ad,
                'media_info': media_info,
                'date': message.date or get_current_time(),
                'grouped_id': None
            }, channel_id)
    
    async def _create_single_message(self, message_data: Dict, channel_id: str) -> Dict:
        """åˆ›å»ºå•ç‹¬æ¶ˆæ¯"""
        # å¦‚æœæœ‰åª’ä½“ä¿¡æ¯ï¼Œä¿å­˜æœ¬åœ°æ–‡ä»¶è·¯å¾„
        media_url = None
        if message_data.get('media_info'):
            media_url = message_data['media_info']['file_path']
        
        return {
            'message_id': message_data['message_id'],
            'content': message_data.get('content', ''),
            'filtered_content': message_data.get('filtered_content', message_data.get('content', '')),
            'is_ad': message_data.get('is_ad', False),
            'media_type': message_data['media_info']['media_type'] if message_data.get('media_info') else None,
            'media_url': media_url,
            'grouped_id': str(message_data.get('grouped_id')) if message_data.get('grouped_id') else None,
            'is_combined': False,
            'combined_messages': None,
            'media_group': None,
            'date': message_data.get('date', get_current_time())
        }
    
    async def _handle_grouped_message_batch(self, message_data: Dict, channel_id: str) -> Optional[Dict]:
        """æ‰¹é‡æ¨¡å¼ä¸‹å¤„ç†ç»„åˆæ¶ˆæ¯ï¼ˆç”¨äºå†å²æ¶ˆæ¯é‡‡é›†ï¼‰"""
        grouped_id = str(message_data['grouped_id']) if message_data.get('grouped_id') else None
        if not grouped_id:
            return await self._create_single_message(message_data, channel_id)
            
        group_key = f"{channel_id}_{grouped_id}"
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™ä¸ªæ¶ˆæ¯ç»„
        existing_combined = await self._get_existing_combined_message(channel_id, grouped_id)
        if existing_combined:
            logger.debug(f"æ¶ˆæ¯ç»„ {grouped_id} å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†")
            return None
        
        # å°†æ¶ˆæ¯æ·»åŠ åˆ°å¾…å¤„ç†ç»„
        if group_key not in self.pending_groups:
            self.pending_groups[group_key] = []
            # æ‰¹é‡æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ç»Ÿä¸€çš„è¶…æ—¶æ—¶é—´
            asyncio.create_task(self._process_batch_group_after_timeout(group_key, channel_id, self.group_timeout))
        
        self.pending_groups[group_key].append(message_data)
        logger.debug(f"æ‰¹é‡æ¨¡å¼ï¼šæ¶ˆæ¯ç»„ {grouped_id} å½“å‰æœ‰ {len(self.pending_groups[group_key])} æ¡æ¶ˆæ¯")
        
        # æ‰¹é‡æ¨¡å¼ä¸‹è¿”å›Noneï¼Œç­‰å¾…è¶…æ—¶åå¤„ç†
        return None
    
    async def _process_batch_group_after_timeout(self, group_key: str, channel_id: str, timeout: float):
        """æ‰¹é‡æ¨¡å¼ä¸‹çš„è¶…æ—¶å¤„ç†"""
        try:
            await asyncio.sleep(timeout)
            
            if group_key not in self.pending_groups:
                return
            
            messages = self.pending_groups[group_key]
            if not messages:
                return
            
            logger.info(f"æ‰¹é‡å¤„ç†æ¶ˆæ¯ç»„ {group_key}ï¼Œå…± {len(messages)} æ¡æ¶ˆæ¯")
            
            # åˆ›å»ºç»„åˆæ¶ˆæ¯
            combined_message = await self._create_combined_message(messages, channel_id)
            
            # ä¿å­˜ç»„åˆæ¶ˆæ¯åˆ°æ•°æ®åº“
            await self._save_combined_message(combined_message, channel_id)
            
            # æ¸…ç†
            del self.pending_groups[group_key]
                
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†æ¶ˆæ¯ç»„ {group_key} æ—¶å‡ºé”™: {e}")
    
    async def _handle_grouped_message(self, message_data: Dict, channel_id: str) -> Optional[Dict]:
        """å¤„ç†ç»„åˆæ¶ˆæ¯"""
        grouped_id = str(message_data['grouped_id']) if message_data.get('grouped_id') else None
        if not grouped_id:
            return await self._create_single_message(message_data, channel_id)
            
        group_key = f"{channel_id}_{grouped_id}"
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™ä¸ªæ¶ˆæ¯ç»„
        existing_combined = await self._get_existing_combined_message(channel_id, grouped_id)
        if existing_combined:
            logger.info(f"æ¶ˆæ¯ç»„ {grouped_id} å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†")
            return None
        
        # æ£€æŸ¥è¿™æ¡æ¶ˆæ¯æ˜¯å¦å·²ç»è¢«ä½œä¸ºå•ç‹¬æ¶ˆæ¯ä¿å­˜è¿‡
        existing_single = await self._get_existing_single_message(channel_id, message_data['message_id'])
        if existing_single:
            logger.info(f"æ¶ˆæ¯ {message_data['message_id']} å·²ä½œä¸ºå•ç‹¬æ¶ˆæ¯å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†")
            return None
        
        # å°†æ¶ˆæ¯æ·»åŠ åˆ°å¾…å¤„ç†ç»„
        if group_key not in self.pending_groups:
            self.pending_groups[group_key] = []
        
        self.pending_groups[group_key].append(message_data)
        
        # å–æ¶ˆä¹‹å‰çš„å®šæ—¶å™¨
        if group_key in self.group_timers:
            self.group_timers[group_key].cancel()
        
        # è®¾ç½®æ–°çš„å®šæ—¶å™¨
        self.group_timers[group_key] = asyncio.create_task(
            self._process_group_after_timeout(group_key, channel_id)
        )
        
        logger.info(f"æ¶ˆæ¯ç»„ {grouped_id} å½“å‰æœ‰ {len(self.pending_groups[group_key])} æ¡æ¶ˆæ¯")
        
        # æš‚æ—¶è¿”å›Noneï¼Œç­‰å¾…è¶…æ—¶åå¤„ç†
        return None
    
    async def _process_group_after_timeout(self, group_key: str, channel_id: str):
        """è¶…æ—¶åå¤„ç†æ¶ˆæ¯ç»„"""
        try:
            await asyncio.sleep(self.group_timeout)
            
            if group_key not in self.pending_groups:
                return
            
            messages = self.pending_groups[group_key]
            if not messages:
                return
            
            logger.info(f"å¤„ç†æ¶ˆæ¯ç»„ {group_key}ï¼Œå…± {len(messages)} æ¡æ¶ˆæ¯")
            
            # åˆ›å»ºç»„åˆæ¶ˆæ¯
            combined_message = await self._create_combined_message(messages, channel_id)
            
            # å‡†å¤‡ç»„åˆæ¶ˆæ¯æ•°æ®
            processed_data = await self._save_combined_message(combined_message, channel_id)
            
            # å°†å¤„ç†åçš„æ•°æ®å­˜å‚¨ï¼Œä¾›åç»­è·å–
            if processed_data:
                self.completed_groups[group_key] = processed_data
            
            # æ¸…ç†
            del self.pending_groups[group_key]
            if group_key in self.group_timers:
                del self.group_timers[group_key]
                
        except Exception as e:
            logger.error(f"å¤„ç†æ¶ˆæ¯ç»„è¶…æ—¶æ—¶å‡ºé”™: {e}")
            # æ¸…ç†èµ„æº
            if group_key in self.pending_groups:
                del self.pending_groups[group_key]
            if group_key in self.group_timers:
                del self.group_timers[group_key]
    
    async def _create_combined_message(self, messages: List[Dict], channel_id: str) -> Dict:
        """åˆ›å»ºç»„åˆæ¶ˆæ¯"""
        # æŒ‰æ—¶é—´æ’åº
        messages.sort(key=lambda x: x['date'])
        
        # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨è¿‡æ»¤åçš„å†…å®¹ï¼‰
        all_texts = []
        all_filtered_texts = []
        is_ad = False
        
        for msg in messages:
            content = msg.get('content') or ''
            filtered_content = msg.get('filtered_content')
            
            # å§‹ç»ˆä¿å­˜åŸå§‹å†…å®¹
            if content.strip():
                all_texts.append(content)
            
            # å¦‚æœæœ‰è¿‡æ»¤åçš„å†…å®¹ï¼Œä½¿ç”¨è¿‡æ»¤åçš„ï¼›å¦åˆ™ä½¿ç”¨åŸå§‹å†…å®¹
            if filtered_content and filtered_content.strip():
                all_filtered_texts.append(filtered_content)
            elif content.strip():
                all_filtered_texts.append(content)
            
            # å¦‚æœç»„å†…ä»»ä½•ä¸€æ¡æ¶ˆæ¯è¢«åˆ¤å®šä¸ºå¹¿å‘Šï¼Œæ•´ç»„éƒ½æ ‡è®°ä¸ºå¹¿å‘Š
            if msg.get('is_ad'):
                is_ad = True
                logger.info(f"ğŸš« æ¶ˆæ¯ç»„ä¸­æ£€æµ‹åˆ°å¹¿å‘Šï¼Œæ•´ç»„æ ‡è®°ä¸ºå¹¿å‘Š")
        
        combined_content = '\n'.join(all_texts) if all_texts else ""
        combined_filtered_content = '\n'.join(all_filtered_texts) if all_filtered_texts else ""
        
        # æå–æ‰€æœ‰åª’ä½“ä¿¡æ¯
        media_group = []
        media_types = set()
        
        for msg in messages:
            if msg.get('media_info'):
                media_info = msg['media_info']
                media_group.append({
                    'message_id': msg['message_id'],
                    'media_type': media_info['media_type'],
                    'file_path': media_info.get('file_path'),  # å¯èƒ½ä¸ºNoneï¼ˆä¸‹è½½å¤±è´¥ï¼‰
                    'file_size': media_info.get('file_size'),
                    'mime_type': media_info.get('mime_type'),
                    'download_failed': media_info.get('download_failed', False),
                    'error': media_info.get('error'),
                    'visual_hashes': media_info.get('visual_hashes')  # ä¿ç•™è§†è§‰å“ˆå¸Œ
                })
                # åªæœ‰æˆåŠŸä¸‹è½½çš„åª’ä½“æ‰è®¡å…¥ç±»å‹ç»Ÿè®¡
                if not media_info.get('download_failed'):
                    media_types.add(media_info['media_type'])
        
        # ç¡®å®šä¸»è¦åª’ä½“ç±»å‹
        if len(media_types) == 1:
            main_media_type = list(media_types)[0]
        elif 'photo' in media_types:
            main_media_type = 'photo'
        elif 'video' in media_types:
            main_media_type = 'video'
        else:
            main_media_type = 'mixed'
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¶ˆæ¯çš„ä¿¡æ¯ä½œä¸ºä¸»æ¶ˆæ¯
        first_message = messages[0]
        
        # ä¸ºç»„åˆæ¶ˆæ¯ä¿å­˜ä¸»åª’ä½“æ–‡ä»¶è·¯å¾„
        main_media_url = None
        if media_group:
            main_media_url = media_group[0]['file_path']
        
        return {
            'message_id': first_message['message_id'],
            'content': combined_content,
            'filtered_content': combined_filtered_content,
            'is_ad': is_ad,
            'media_type': main_media_type if media_group else None,
            'media_url': main_media_url,
            'grouped_id': str(first_message['grouped_id']) if first_message.get('grouped_id') else None,
            'is_combined': True,
            'combined_messages': [
                {
                    'message_id': msg['message_id'],
                    'content': msg['content'],
                    'media_info': msg.get('media_info')
                }
                for msg in messages
            ],
            'media_group': media_group if media_group else None,
            'date': first_message['date']
        }
    
    async def _save_combined_message(self, combined_message: Dict, channel_id: str):
        """å‡†å¤‡ç»„åˆæ¶ˆæ¯æ•°æ®ï¼ˆä¸å†ç›´æ¥ä¿å­˜ï¼‰"""
        try:
            # è¿”å›å¤„ç†åçš„ç»„åˆæ¶ˆæ¯æ•°æ®
            return await self._trigger_combined_message_event(combined_message, channel_id)
            
        except Exception as e:
            logger.error(f"å‡†å¤‡ç»„åˆæ¶ˆæ¯æ•°æ®æ—¶å‡ºé”™: {e}")
            return None
    
    async def _trigger_combined_message_event(self, combined_message: Dict, channel_id: str):
        """è¿”å›ç»„åˆæ¶ˆæ¯æ•°æ®ï¼ˆä¸å†ç›´æ¥ä¿å­˜ï¼‰"""
        try:
            from datetime import datetime
            
            # ä½¿ç”¨å·²ç»è¿‡æ»¤çš„å†…å®¹ï¼ˆåœ¨åˆ›å»ºç»„åˆæ¶ˆæ¯æ—¶å·²ç»å¤„ç†ï¼‰
            is_ad = combined_message.get('is_ad', False)
            filtered_content = combined_message.get('filtered_content', combined_message['content'])
            
            # é¦–å…ˆåˆ é™¤å·²ç»å•ç‹¬ä¿å­˜çš„ç»„å†…æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            await self._cleanup_individual_messages(channel_id, combined_message)
            
            # å¤„ç†JSONåºåˆ—åŒ– - æ¸…ç†åŒ…å«datetimeçš„å¯¹è±¡
            def serialize_for_json(obj):
                """é€’å½’å¤„ç†å¯¹è±¡ï¼Œå°†datetimeè½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
                if isinstance(obj, datetime):
                    return obj.isoformat()
                elif isinstance(obj, dict):
                    return {k: serialize_for_json(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [serialize_for_json(item) for item in obj]
                else:
                    return obj
            
            # æ¸…ç†combined_messageså’Œmedia_groupä¸­çš„datetimeå¯¹è±¡
            clean_combined_messages = serialize_for_json(combined_message.get('combined_messages'))
            clean_media_group = serialize_for_json(combined_message.get('media_group'))
            
            # æå–ç»„åˆæ¶ˆæ¯çš„è§†è§‰å“ˆå¸Œ
            combined_visual_hashes = []
            if clean_media_group:
                for media_item in clean_media_group:
                    if media_item.get('visual_hashes'):
                        combined_visual_hashes.append(media_item['visual_hashes'])
            
            # å¦‚æœæœ‰è§†è§‰å“ˆå¸Œï¼Œå­˜å‚¨ä¸ºå­—ç¬¦ä¸²
            visual_hash = str(combined_visual_hashes) if combined_visual_hashes else None
            
            # è®¡ç®—ç»„åˆåª’ä½“å“ˆå¸Œ
            combined_media_hash = None
            if clean_media_group:
                import hashlib
                hashes = []
                for media_item in clean_media_group:
                    if media_item.get('hash'):
                        hashes.append(media_item['hash'])
                if hashes:
                    combined_media_hash = hashlib.sha256(''.join(sorted(hashes)).encode()).hexdigest()
            
            # è¿”å›å¤„ç†åçš„æ¶ˆæ¯æ•°æ®ï¼Œç”±ç»Ÿä¸€å¤„ç†å™¨ä¿å­˜
            logger.info(f"ç»„åˆæ¶ˆæ¯å‡†å¤‡å®Œæˆ: grouped_id={combined_message['grouped_id']}, åŒ…å« {len(combined_message.get('combined_messages', []))} æ¡æ¶ˆæ¯")
            
            return {
                'message_id': combined_message['message_id'],
                'content': combined_message['content'],
                'filtered_content': filtered_content,
                'is_ad': is_ad,
                'media_type': combined_message['media_type'],
                'media_url': combined_message['media_url'],
                'grouped_id': combined_message.get('grouped_id'),
                'is_combined': True,
                'combined_messages': clean_combined_messages,
                'media_group': clean_media_group,
                'visual_hash': visual_hash,
                'combined_media_hash': combined_media_hash,
                'date': combined_message.get('date', get_current_time())
            }
                
        except Exception as e:
            logger.error(f"å¤„ç†ç»„åˆæ¶ˆæ¯æ•°æ®æ—¶å‡ºé”™: {e}")
            return None
    
    async def force_complete_all_groups(self):
        """å¼ºåˆ¶å®Œæˆæ‰€æœ‰å¾…å¤„ç†çš„æ¶ˆæ¯ç»„ï¼ˆç”¨äºå†å²é‡‡é›†ç»“æŸæ—¶ï¼‰"""
        try:
            logger.info(f"å¼ºåˆ¶å®Œæˆæ‰€æœ‰å¾…å¤„ç†çš„æ¶ˆæ¯ç»„ï¼Œå½“å‰æœ‰ {len(self.pending_groups)} ä¸ªç»„")
            
            # å–æ¶ˆæ‰€æœ‰å®šæ—¶å™¨
            for timer in self.group_timers.values():
                timer.cancel()
            self.group_timers.clear()
            
            # å¤„ç†æ‰€æœ‰å¾…å¤„ç†çš„ç»„
            groups_to_process = list(self.pending_groups.keys())
            for group_key in groups_to_process:
                messages = self.pending_groups.get(group_key, [])
                if messages:
                    # ä»group_keyä¸­æå–channel_id
                    # group_keyæ ¼å¼: channel_id_grouped_id
                    # channel_idå¯èƒ½æ˜¯è´Ÿæ•°ï¼Œå¦‚ -1001969693044
                    last_underscore = group_key.rfind('_')
                    if last_underscore > 0:
                        channel_id = group_key[:last_underscore]
                    else:
                        # å¦‚æœæ‰¾ä¸åˆ°ä¸‹åˆ’çº¿ï¼Œæ•´ä¸ªkeyå°±æ˜¯channel_id
                        channel_id = group_key
                    
                    logger.info(f"å¼ºåˆ¶å¤„ç†æ¶ˆæ¯ç»„ {group_key}ï¼Œå…± {len(messages)} æ¡æ¶ˆæ¯")
                    
                    # åˆ›å»ºç»„åˆæ¶ˆæ¯
                    combined_message = await self._create_combined_message(messages, channel_id)
                    
                    # ä¿å­˜ç»„åˆæ¶ˆæ¯åˆ°æ•°æ®åº“
                    await self._save_combined_message(combined_message, channel_id)
            
            # æ¸…ç†æ‰€æœ‰å¾…å¤„ç†çš„ç»„
            self.pending_groups.clear()
            
            logger.info("æ‰€æœ‰å¾…å¤„ç†çš„æ¶ˆæ¯ç»„å·²å®Œæˆ")
            
        except Exception as e:
            logger.error(f"å¼ºåˆ¶å®Œæˆæ¶ˆæ¯ç»„æ—¶å‡ºé”™: {e}")
    
    async def _get_existing_combined_message(self, channel_id: str, grouped_id: str) -> Optional[Message]:
        """æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç»„åˆæ¶ˆæ¯"""
        try:
            async with AsyncSessionLocal() as db:
                result = await db.execute(
                    select(Message).where(
                        Message.source_channel == channel_id,
                        Message.grouped_id == grouped_id,
                        Message.is_combined == True
                    )
                )
                return result.scalar_one_or_none()
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç°æœ‰ç»„åˆæ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            return None
    
    async def _get_existing_single_message(self, channel_id: str, message_id: int) -> Optional[Message]:
        """æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å•ç‹¬æ¶ˆæ¯"""
        try:
            async with AsyncSessionLocal() as db:
                result = await db.execute(
                    select(Message).where(
                        Message.source_channel == channel_id,
                        Message.message_id == message_id
                    )
                )
                return result.scalar_one_or_none()
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç°æœ‰å•ç‹¬æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            return None
    
    async def _cleanup_individual_messages(self, channel_id: str, combined_message: Dict):
        """æ¸…ç†å·²ç»è¢«ç»„åˆçš„å•ç‹¬æ¶ˆæ¯"""
        try:
            if not combined_message.get('combined_messages'):
                return
            
            async with AsyncSessionLocal() as db:
                # è·å–æ‰€æœ‰ç›¸å…³çš„å•ç‹¬æ¶ˆæ¯ID
                message_ids = [msg['message_id'] for msg in combined_message['combined_messages']]
                
                # æŸ¥æ‰¾å¹¶åˆ é™¤è¿™äº›å•ç‹¬æ¶ˆæ¯
                result = await db.execute(
                    select(Message).where(
                        Message.source_channel == channel_id,
                        Message.message_id.in_(message_ids),
                        Message.is_combined == False  # åªåˆ é™¤å•ç‹¬æ¶ˆæ¯ï¼Œä¸åˆ é™¤ç»„åˆæ¶ˆæ¯
                    )
                )
                
                messages_to_delete = result.scalars().all()
                
                if messages_to_delete:
                    for msg in messages_to_delete:
                        logger.info(f"åˆ é™¤å·²è¢«ç»„åˆçš„å•ç‹¬æ¶ˆæ¯: ID={msg.id}, message_id={msg.message_id}")
                        await db.delete(msg)
                    
                    await db.commit()
                    logger.info(f"å·²æ¸…ç† {len(messages_to_delete)} æ¡è¢«ç»„åˆçš„å•ç‹¬æ¶ˆæ¯")
                
        except Exception as e:
            logger.error(f"æ¸…ç†å•ç‹¬æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
    
    async def cleanup_expired_groups(self):
        """æ¸…ç†è¿‡æœŸçš„æ¶ˆæ¯ç»„"""
        try:
            expired_keys = []
            current_time = get_current_time()
            
            for group_key, messages in self.pending_groups.items():
                if not messages:
                    expired_keys.append(group_key)
                    continue
                
                # æ£€æŸ¥æœ€æ—§æ¶ˆæ¯çš„æ—¶é—´
                oldest_time = min(msg['date'] for msg in messages)
                if current_time - oldest_time > timedelta(minutes=5):  # 5åˆ†é’Ÿè¶…æ—¶
                    expired_keys.append(group_key)
            
            for key in expired_keys:
                if key in self.pending_groups:
                    del self.pending_groups[key]
                if key in self.group_timers:
                    self.group_timers[key].cancel()
                    del self.group_timers[key]
                    
            if expired_keys:
                logger.info(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸæ¶ˆæ¯ç»„")
                
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸæ¶ˆæ¯ç»„æ—¶å‡ºé”™: {e}")

# å…¨å±€æ¶ˆæ¯ç»„åˆå™¨å®ä¾‹
message_grouper = MessageGrouper()