"""
ç»Ÿä¸€çš„æ¶ˆæ¯å¤„ç†å™¨
å°†å®æ—¶æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯çš„å¤„ç†æµç¨‹ç»Ÿä¸€ï¼Œç¡®ä¿ä¸€è‡´æ€§å’Œå¯ç»´æŠ¤æ€§
"""
import logging
import os
from typing import Optional, Dict, Any, Tuple
from datetime import datetime
from app.utils.timezone import get_current_time, parse_telegram_time, format_for_api
from telethon.tl.types import Message as TLMessage

from app.core.database import AsyncSessionLocal, Message
from app.services.content_filter import ContentFilter
from app.services.media_handler import media_handler
from app.services.message_grouper import message_grouper
from app.services.duplicate_detector import DuplicateDetector
from app.services.message_processor import MessageProcessor
from app.core.config import db_settings

logger = logging.getLogger(__name__)

class UnifiedMessageProcessor:
    """ç»Ÿä¸€çš„æ¶ˆæ¯å¤„ç†å™¨ - å¤„ç†æ‰€æœ‰æ¥æºçš„æ¶ˆæ¯"""
    
    def __init__(self):
        self.content_filter = ContentFilter()
        self.duplicate_detector = DuplicateDetector()
        self.message_processor = MessageProcessor()
        
    async def process_telegram_message(
        self, 
        message: TLMessage, 
        channel_id: str, 
        is_history: bool = False
    ) -> Optional[Message]:
        """
        ç»Ÿä¸€çš„æ¶ˆæ¯å¤„ç†å…¥å£
        
        Args:
            message: Telegramæ¶ˆæ¯å¯¹è±¡
            channel_id: é¢‘é“IDï¼ˆå·²æ ¼å¼åŒ–ï¼‰
            is_history: æ˜¯å¦ä¸ºå†å²æ¶ˆæ¯
            
        Returns:
            å¤„ç†åçš„æ•°æ®åº“æ¶ˆæ¯å¯¹è±¡ï¼Œå¦‚æœæ¶ˆæ¯è¢«è¿‡æ»¤åˆ™è¿”å›None
        """
        try:
            # æ­¥éª¤1: é¦–å…ˆæå–åŸå§‹å†…å®¹å¹¶ä¿å­˜
            original_content = await self._extract_original_content(message)
            
            # æ­¥éª¤2: é€šç”¨å¤„ç†ï¼ˆæå–å†…å®¹ã€ä¸‹è½½åª’ä½“ã€è¿‡æ»¤å¹¿å‘Šï¼‰
            processed_data = await self._common_message_processing(message, channel_id, is_history)
            if not processed_data:
                logger.info(f"ğŸ“­ æ¶ˆæ¯ #{message.id} åœ¨é€šç”¨å¤„ç†é˜¶æ®µè¢«è¿‡æ»¤")
                return None  # æ¶ˆæ¯è¢«è¿‡æ»¤
            
            # ç¡®ä¿åŸå§‹å†…å®¹è¢«ä¿ç•™
            processed_data['original_content'] = original_content
            
            # æ­¥éª¤3: ç»„åˆæ¶ˆæ¯æ£€æµ‹
            combined_message = await message_grouper.process_message(
                message, 
                channel_id, 
                processed_data.get('media_info'),
                filtered_content=processed_data['filtered_content'],
                is_ad=processed_data['is_ad'],
                is_batch=is_history  # å†å²æ¶ˆæ¯ä½¿ç”¨æ‰¹é‡æ¨¡å¼
            )
            
            # å¦‚æœè¿”å›Noneï¼Œè¯´æ˜æ¶ˆæ¯æ­£åœ¨ç­‰å¾…ç»„åˆ
            if combined_message is None:
                logger.info(f"â³ æ¶ˆæ¯ #{message.id} æ­£åœ¨ç­‰å¾…ç»„åˆ")
                return None
            
            # æ­¥éª¤4: å‡†å¤‡ä¿å­˜æ•°æ®
            save_data = await self._prepare_save_data(
                combined_message, 
                channel_id, 
                processed_data,
                is_history
            )
            
            # æ­¥éª¤5: å»é‡æ£€æµ‹
            duplicate_info = await self._check_duplicate_with_details(save_data, channel_id)
            if duplicate_info:
                logger.info(f"ğŸ”„ {'å†å²' if is_history else 'å®æ—¶'}æ¶ˆæ¯è¢«å»é‡æ£€æµ‹æ‹’ç»: {duplicate_info['reason']}")
                # ä¿å­˜è¢«å»é‡æ‹’ç»çš„æ¶ˆæ¯åˆ°æ•°æ®åº“ï¼ŒçŠ¶æ€ä¸ºrejected
                save_data['status'] = 'rejected'
                save_data['reject_reason'] = f"å»é‡æ£€æµ‹: {duplicate_info['reason']} (åŸæ¶ˆæ¯ID: {duplicate_info.get('original_id', 'N/A')})"
                save_data['filter_reason'] = duplicate_info['reason']
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                db_message = await self.message_processor.process_new_message(save_data)
                if db_message:
                    logger.info(f"âŒ æœ€ç»ˆå¤„ç†ç»“æœ: æ¶ˆæ¯ #{message.id} -> æ•°æ®åº“ID #{db_message.id} [çŠ¶æ€: rejected] [åŸå› : å»é‡æ£€æµ‹]")
                
                # æ¸…ç†åª’ä½“æ–‡ä»¶ï¼ˆå¦‚æœä¸æƒ³ä¿ç•™çš„è¯ï¼‰
                # await self._cleanup_media_files(save_data)
                return db_message
            
            # æ­¥éª¤6: ä¿å­˜åˆ°æ•°æ®åº“
            db_message = await self.message_processor.process_new_message(save_data)
            
            if not db_message:
                logger.info(f"ğŸ’¥ æ¶ˆæ¯ #{message.id} ä¿å­˜å¤±è´¥æˆ–è¢«æ‹’ç»")
                await self._cleanup_media_files(save_data)
                return None
            
            # æ­¥éª¤7: è½¬å‘åˆ°å®¡æ ¸ç¾¤ï¼ˆæ ¹æ®é…ç½®å†³å®šï¼‰
            if await self._should_forward_to_review(is_history):
                await self._forward_to_review(db_message)
            
            # æ­¥éª¤8: å¹¿æ’­åˆ°WebSocketï¼ˆæ‰€æœ‰æ–°æ¶ˆæ¯éƒ½å¹¿æ’­ï¼Œè®©webç«¯èƒ½çœ‹åˆ°ï¼‰
            # ä¸å†åŒºåˆ†æ˜¯å¦å†å²æ¶ˆæ¯ï¼Œæ‰€æœ‰æˆåŠŸä¿å­˜çš„æ¶ˆæ¯éƒ½å¹¿æ’­åˆ°webç«¯
            await self._broadcast_new_message(db_message)
            
            # æœ€ç»ˆå¤„ç†ç»“æœæ—¥å¿—
            status_emoji = {
                'pending': 'â³',
                'approved': 'âœ…', 
                'rejected': 'âŒ',
                'auto_forwarded': 'ğŸ¤–'
            }.get(db_message.status, 'â“')
            
            logger.info(f"{status_emoji} æœ€ç»ˆå¤„ç†ç»“æœ: æ¶ˆæ¯ #{message.id} -> æ•°æ®åº“ID #{db_message.id} [çŠ¶æ€: {db_message.status}] [å¹¿å‘Š: {'æ˜¯' if db_message.is_ad else 'å¦'}]")
            
            return db_message
            
        except Exception as e:
            logger.error(f"ç»Ÿä¸€æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            # æ¸…ç†å¯èƒ½å·²ä¸‹è½½çš„åª’ä½“
            if 'processed_data' in locals() and processed_data:
                media_info = processed_data.get('media_info')
                if media_info and media_info.get('file_path'):
                    await media_handler.cleanup_file(media_info['file_path'])
            return None
    
    async def _extract_original_content(self, message: TLMessage) -> str:
        """
        æå–æ¶ˆæ¯çš„åŸå§‹å†…å®¹ï¼Œç¡®ä¿ä¸ä¸¢å¤±ä»»ä½•æ–‡æœ¬
        
        Args:
            message: Telegramæ¶ˆæ¯å¯¹è±¡
            
        Returns:
            åŸå§‹å†…å®¹å­—ç¬¦ä¸²
        """
        # å°è¯•å¤šç§æ–¹å¼æå–å†…å®¹
        content = ""
        
        # 1. ä¼˜å…ˆä½¿ç”¨textå±æ€§
        if hasattr(message, 'text') and message.text:
            content = message.text
        # 2. å°è¯•raw_text
        elif hasattr(message, 'raw_text') and message.raw_text:
            content = message.raw_text
        # 3. å°è¯•messageå±æ€§
        elif hasattr(message, 'message') and message.message:
            content = message.message
        # 4. å¯¹äºåª’ä½“æ¶ˆæ¯ï¼Œå°è¯•caption
        elif hasattr(message, 'media') and message.media:
            if hasattr(message, 'caption') and message.caption:
                content = message.caption
        
        # è®°å½•åŸå§‹å†…å®¹æå–æƒ…å†µ
        if content:
            logger.info(f"ğŸ“ æå–åˆ°åŸå§‹å†…å®¹: {len(content)} å­—ç¬¦")
            logger.debug(f"åŸå§‹å†…å®¹å‰100å­—ç¬¦: {content[:100]}...")
        else:
            logger.debug(f"ğŸ“ æ¶ˆæ¯æ— æ–‡æœ¬å†…å®¹ï¼ˆçº¯åª’ä½“ï¼‰")
        
        return content
    
    async def _common_message_processing(
        self, 
        message: TLMessage, 
        channel_id: str, 
        is_history: bool
    ) -> Optional[Dict[str, Any]]:
        """
        é€šç”¨æ¶ˆæ¯å¤„ç†é€»è¾‘
        æå–å†…å®¹ã€ä¸‹è½½åª’ä½“ã€è¿‡æ»¤å¹¿å‘Š
        """
        try:
            # æå–æ¶ˆæ¯å†…å®¹
            content = message.text or message.raw_text or message.message or ""
            
            # å¯¹äºåª’ä½“æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰caption
            if not content and message.media:
                if hasattr(message, 'caption'):
                    content = message.caption or ""
                elif hasattr(message, 'raw_text'):
                    content = message.raw_text or ""
            
            # å†æ¬¡å°è¯•è·å–
            if not content and hasattr(message, 'message') and message.message:
                content = message.message
                logger.debug(f"ğŸ“ ä»messageå±æ€§æå–åˆ°å†…å®¹")
            
            # è®°å½•å†…å®¹æå–ç»“æœ
            if content:
                logger.info(f"ğŸ“ æå–åˆ°æ¶ˆæ¯å†…å®¹: {content[:100]}...")
            else:
                logger.debug(f"ğŸ“ æ¶ˆæ¯æ— æ–‡æœ¬å†…å®¹ï¼ˆçº¯åª’ä½“ï¼‰")
            
            # å¤„ç†åª’ä½“
            media_info = None
            if message.media:
                media_info = await self._process_media(message, channel_id)
            
            # å‡†å¤‡åª’ä½“æ–‡ä»¶åˆ—è¡¨ç”¨äºOCRå¤„ç†
            media_files = []
            if media_info and media_info.get('file_path'):
                media_files.append(media_info['file_path'])
            
            # æå–æ¶ˆæ¯å®ä½“ï¼ˆåŒ…æ‹¬éšè—é“¾æ¥ï¼‰
            from app.services.structural_ad_detector import structural_detector
            entities = structural_detector.extract_entity_data(message)
            
            # ç§»é™¤éšè—é“¾æ¥ï¼ˆæ ¹æ®é…ç½®ï¼‰
            removed_hidden_links = []
            from app.services.config_manager import config_manager
            hidden_link_action = await config_manager.get_config('filter.hidden_link_action')
            if hidden_link_action == 'remove' or hidden_link_action is None:  # é»˜è®¤ç§»é™¤
                clean_entities, removed_hidden_links = structural_detector.remove_hidden_links(message)
                if removed_hidden_links:
                    logger.info(f"ğŸ”— ç§»é™¤äº† {len(removed_hidden_links)} ä¸ªéšè—é“¾æ¥")
            
            # å†…å®¹è¿‡æ»¤ï¼ˆæ™ºèƒ½å»å°¾éƒ¨ + ç»“æ„åŒ–å¹¿å‘Šæ£€æµ‹ + AIå¹¿å‘Šæ£€æµ‹ + OCRå›¾ç‰‡æ–‡å­—æå–ï¼‰
            is_ad, filtered_content, filter_reason, ocr_result = await self.content_filter.filter_message(
                content, 
                channel_id=channel_id,
                message_obj=message,  # ä¼ é€’æ¶ˆæ¯å¯¹è±¡ç”¨äºç»“æ„åŒ–æ£€æµ‹
                media_files=media_files  # ä¼ é€’åª’ä½“æ–‡ä»¶ç”¨äºOCRå¤„ç†
            )
            
            # è®°å½•è¿‡æ»¤æ•ˆæœ
            if content != filtered_content:
                original_len = len(content)
                filtered_len = len(filtered_content)
                logger.info(f"ğŸ“ å†…å®¹è¿‡æ»¤: {original_len} -> {filtered_len} å­—ç¬¦ (å‡å°‘ {original_len - filtered_len})")
            
            if is_ad:
                logger.info(f"ğŸš« æ£€æµ‹åˆ°å¹¿å‘Š: {filter_reason}")
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥å®Œå…¨æ‹’ç»çº¯å¹¿å‘Šæ¶ˆæ¯
                should_reject, reject_reason = self._should_reject_pure_ad(
                    is_ad, filter_reason, filtered_content, content, media_info, ocr_result
                )
                
                if should_reject:
                    logger.warning(f"ğŸš¨ æ‹’ç»çº¯å¹¿å‘Šæ¶ˆæ¯: {reject_reason}")
                    
                    # ä¿å­˜è¢«æ‹’ç»çš„OCRæ ·æœ¬ï¼ˆå¦‚æœæœ‰åª’ä½“æ–‡ä»¶ï¼‰
                    if media_info and media_info.get('file_path') and ocr_result:
                        try:
                            from app.services.ocr_service import ocr_service
                            import hashlib
                            
                            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
                            with open(media_info['file_path'], 'rb') as f:
                                file_hash = hashlib.md5(f.read()).hexdigest()
                            
                            # å¼‚æ­¥ä¿å­˜æ ·æœ¬
                            asyncio.create_task(ocr_service._save_ocr_sample(
                                image_path=media_info['file_path'],
                                image_hash=file_hash,
                                texts=ocr_result.get('texts', []),
                                qr_codes=[qr.get('data', '') for qr in ocr_result.get('qr_codes', []) if qr.get('data')],
                                ad_score=ocr_result.get('ad_score', 0),
                                is_ad=True,
                                keywords_detected=ocr_result.get('ad_indicators', []),
                                auto_rejected=True,
                                rejection_reason=reject_reason
                            ))
                        except Exception as e:
                            logger.debug(f"ä¿å­˜æ‹’ç»æ ·æœ¬å¤±è´¥: {e}")
                    
                    # æ¸…ç†åª’ä½“æ–‡ä»¶
                    if media_info and media_info.get('file_path'):
                        await media_handler.cleanup_file(media_info['file_path'])
                    return None
                
                # å¦‚æœé…ç½®äº†è‡ªåŠ¨è¿‡æ»¤å¹¿å‘Šï¼Œç›´æ¥è¿”å›None
                if await db_settings.get_auto_filter_ads():
                    logger.info(f"ğŸš« è‡ªåŠ¨è¿‡æ»¤å¹¿å‘Šæ¶ˆæ¯: {filter_reason}")
                    if media_info and media_info.get('file_path'):
                        await media_handler.cleanup_file(media_info['file_path'])
                    return None
            
            # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
            # å¦‚æœæ—¢æ²¡æœ‰åª’ä½“ï¼Œfiltered_contentåˆä¸ºç©ºï¼Œåˆ™æ‹’ç»è¿™æ¡æ¶ˆæ¯
            if not media_info and not filtered_content:
                logger.warning(f"âŒ æ¶ˆæ¯æ—¢æ— åª’ä½“åˆæ— æœ‰æ•ˆå†…å®¹ï¼Œæ‹’ç»å¤„ç† (åŸå†…å®¹é•¿åº¦: {len(content)})")
                return None
            
            return {
                'content': content,
                'filtered_content': filtered_content,
                'is_ad': is_ad,
                'filter_reason': filter_reason,
                'media_info': media_info,
                'ocr_result': ocr_result,  # åŒ…å«OCRæå–ç»“æœ
                'entities': entities,  # æ‰€æœ‰å®ä½“ä¿¡æ¯
                'removed_hidden_links': removed_hidden_links  # è¢«ç§»é™¤çš„éšè—é“¾æ¥
            }
            
        except Exception as e:
            logger.error(f"é€šç”¨æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            return None
    
    async def _process_media(self, message: TLMessage, channel_id: str) -> Optional[Dict]:
        """å¤„ç†åª’ä½“ä¸‹è½½"""
        try:
            media_type = None
            if hasattr(message.media, 'photo'):
                media_type = "photo"
                timeout = 30.0
            elif hasattr(message.media, 'document'):
                media_type = "document"
                document = message.media.document
                mime_type = document.mime_type or ""
                timeout = 120.0 if mime_type.startswith("video/") else 60.0
            else:
                return None
            
            # è·å–Telegramå®¢æˆ·ç«¯
            from app.telegram.bot import telegram_bot
            if not telegram_bot or not telegram_bot.client:
                logger.warning("Telegramå®¢æˆ·ç«¯æœªè¿æ¥ï¼Œæ— æ³•ä¸‹è½½åª’ä½“")
                return None
            
            # ä¸‹è½½åª’ä½“ï¼ˆéœ€è¦ä¼ é€’clientå’Œmessage_idï¼‰
            media_info = await media_handler.download_media(
                telegram_bot.client,
                message, 
                message.id,
                timeout=timeout
            )
            
            if not media_info or not media_info.get('file_path'):
                logger.warning(f"åª’ä½“ä¸‹è½½å¤±è´¥æˆ–è¶…æ—¶")
                return None
            
            # è¿”å›åª’ä½“ä¿¡æ¯ï¼ˆmedia_handlerå·²ç»è®¡ç®—äº†å“ˆå¸Œå’Œè§†è§‰å“ˆå¸Œï¼‰
            return media_info
            
        except Exception as e:
            logger.error(f"åª’ä½“å¤„ç†å¤±è´¥: {e}")
            return None
    
    async def _prepare_save_data(
        self, 
        message_data: dict, 
        channel_id: str,
        processed_data: dict,
        is_history: bool
    ) -> dict:
        """å‡†å¤‡ä¿å­˜åˆ°æ•°æ®åº“çš„æ•°æ®"""
        # æå–åª’ä½“å“ˆå¸Œ
        media_hash = None
        combined_media_hash = None
        visual_hash = None
        
        if message_data.get('is_combined'):
            # ç»„åˆæ¶ˆæ¯çš„å“ˆå¸Œå¤„ç†
            if message_data.get('media_group'):
                hashes = []
                visual_hashes = []
                for media_item in message_data['media_group']:
                    if media_item.get('hash'):
                        hashes.append(media_item['hash'])
                    if media_item.get('visual_hashes'):
                        visual_hashes.append(media_item['visual_hashes'])
                
                if hashes:
                    combined_media_hash = hashlib.sha256(''.join(sorted(hashes)).encode()).hexdigest()
                if visual_hashes:
                    visual_hash = str(visual_hashes)
        else:
            # å•ç‹¬æ¶ˆæ¯çš„å“ˆå¸Œ
            media_info = processed_data.get('media_info')
            if media_info:
                media_hash = media_info.get('hash')
                if media_info.get('visual_hashes'):
                    visual_hash = str(media_info['visual_hashes'])
        
        # å¤„ç†æ—¶é—´æˆ³ï¼Œç¡®ä¿æ˜¯æ— æ—¶åŒºçš„UTC datetime
        created_at = parse_telegram_time(message_data.get('date'))
        
        # å¤„ç†OCRç»“æœ
        ocr_result = processed_data.get('ocr_result', {})
        ocr_text = None
        qr_codes = None
        ocr_ad_score = 0
        ocr_processed = False
        
        if ocr_result:
            # å°†OCRæ–‡å­—è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å­˜å‚¨
            if ocr_result.get('texts'):
                import json
                ocr_text = json.dumps(ocr_result['texts'], ensure_ascii=False)
            
            # å°†äºŒç»´ç ä¿¡æ¯è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å­˜å‚¨
            if ocr_result.get('qr_codes'):
                qr_codes = json.dumps(ocr_result['qr_codes'], ensure_ascii=False)
            
            ocr_ad_score = int(ocr_result.get('ad_score', 0))
            ocr_processed = bool(ocr_result.get('processed_files', 0) > 0)
        
        return {
            'source_channel': channel_id,
            'message_id': message_data.get('message_id', message_data.get('id')),
            'content': processed_data.get('original_content', message_data.get('content', processed_data['content'])),  # ä¼˜å…ˆä½¿ç”¨åŸå§‹å†…å®¹
            'filtered_content': message_data.get('filtered_content', processed_data['filtered_content']),
            'is_ad': message_data.get('is_ad', processed_data['is_ad']),
            'media_type': message_data.get('media_type'),
            'media_url': message_data.get('media_url'),
            'media_hash': media_hash,
            # æ–°å¢OCRç›¸å…³å­—æ®µ
            'ocr_text': ocr_text,
            'qr_codes': qr_codes,
            'ocr_ad_score': ocr_ad_score,
            'ocr_processed': ocr_processed,
            # æ–°å¢å®ä½“ç›¸å…³å­—æ®µ
            'entities': processed_data.get('entities'),
            'removed_hidden_links': processed_data.get('removed_hidden_links'),
            'combined_media_hash': combined_media_hash,
            'visual_hash': visual_hash,
            'grouped_id': str(message_data.get('grouped_id')) if message_data.get('grouped_id') else None,
            'is_combined': message_data.get('is_combined', False),
            'combined_messages': message_data.get('combined_messages'),
            'media_group': message_data.get('media_group'),
            'status': 'pending',  # æ‰€æœ‰æ¶ˆæ¯éƒ½å…ˆè®¾ä¸ºpendingçŠ¶æ€ï¼Œç­‰å¾…å®¡æ ¸
            'created_at': created_at
        }
    
    async def _check_duplicate_with_details(self, save_data: dict, channel_id: str) -> Optional[dict]:
        """æ£€æŸ¥æ˜¯å¦é‡å¤å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯"""
        try:
            # æå–è§†è§‰å“ˆå¸Œï¼ˆå¦‚æœæœ‰ï¼‰
            visual_hashes = None
            media_info = save_data.get('media_info')
            if media_info and media_info.get('visual_hashes'):
                visual_hashes = media_info['visual_hashes']
            else:
                # å…¼å®¹æ—§æ ¼å¼
                try:
                    import json
                    if save_data.get('visual_hash'):
                        visual_hashes = json.loads(save_data['visual_hash'])
                except:
                    pass
            
            is_duplicate, orig_id, dup_type = await self.duplicate_detector.is_duplicate_message(
                source_channel=channel_id,
                media_hash=save_data.get('media_hash'),
                combined_media_hash=save_data.get('combined_media_hash'),
                content=save_data.get('content'),
                message_time=save_data.get('created_at'),
                visual_hashes=visual_hashes
            )
            
            if is_duplicate:
                logger.info(f"æ£€æµ‹åˆ°é‡å¤æ¶ˆæ¯ï¼ˆ{dup_type}ï¼‰ï¼ŒåŸå§‹æ¶ˆæ¯ID: {orig_id}")
                return {
                    'is_duplicate': True,
                    'original_id': orig_id,
                    'type': dup_type,
                    'reason': f"{dup_type}é‡å¤"
                }
                
            return None
            
        except Exception as e:
            logger.error(f"é‡å¤æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    async def _is_duplicate(self, save_data: dict, channel_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ¶ˆæ¯"""
        try:
            # è§£æè§†è§‰å“ˆå¸Œ
            visual_hashes = None
            if save_data.get('visual_hash'):
                try:
                    visual_hashes = eval(save_data['visual_hash'])
                    if isinstance(visual_hashes, list) and visual_hashes:
                        visual_hashes = visual_hashes[0]
                except:
                    pass
            
            is_duplicate, orig_id, dup_type = await self.duplicate_detector.is_duplicate_message(
                source_channel=channel_id,
                media_hash=save_data.get('media_hash'),
                combined_media_hash=save_data.get('combined_media_hash'),
                content=save_data.get('content'),
                message_time=save_data.get('created_at'),
                visual_hashes=visual_hashes
            )
            
            if is_duplicate:
                logger.info(f"æ£€æµ‹åˆ°é‡å¤æ¶ˆæ¯ï¼ˆ{dup_type}ï¼‰ï¼ŒåŸå§‹æ¶ˆæ¯ID: {orig_id}")
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"é‡å¤æ£€æµ‹å¤±è´¥: {e}")
            return False
    
    async def _cleanup_media_files(self, save_data: dict):
        """æ¸…ç†åª’ä½“æ–‡ä»¶"""
        try:
            # æ¸…ç†å•ä¸ªåª’ä½“æ–‡ä»¶
            if save_data.get('media_url') and os.path.exists(save_data['media_url']):
                await media_handler.cleanup_file(save_data['media_url'])
            
            # æ¸…ç†ç»„åˆæ¶ˆæ¯çš„åª’ä½“æ–‡ä»¶
            if save_data.get('media_group'):
                for media_item in save_data['media_group']:
                    file_path = media_item.get('file_path')
                    if file_path and os.path.exists(file_path):
                        await media_handler.cleanup_file(file_path)
                        
        except Exception as e:
            logger.error(f"æ¸…ç†åª’ä½“æ–‡ä»¶å¤±è´¥: {e}")
    
    async def _should_forward_to_review(self, is_history: bool) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥è½¬å‘æ¶ˆæ¯åˆ°å®¡æ ¸ç¾¤
        
        Args:
            is_history: æ˜¯å¦ä¸ºå†å²æ¶ˆæ¯
            
        Returns:
            æ˜¯å¦åº”è¯¥è½¬å‘åˆ°å®¡æ ¸ç¾¤
        """
        try:
            from app.services.config_manager import config_manager
            
            # è·å–é…ç½®ï¼šæ˜¯å¦å¯ç”¨å®¡æ ¸ç¾¤è½¬å‘
            enable_review = await config_manager.get_config('review.enable_forward_to_group')
            if enable_review is False:
                return False
            
            # å¯¹äºå®æ—¶æ¶ˆæ¯ï¼Œé»˜è®¤è½¬å‘
            if not is_history:
                return True
            
            # å¯¹äºå†å²æ¶ˆæ¯ï¼Œæ£€æŸ¥ä¸“é—¨çš„é…ç½®
            forward_history = await config_manager.get_config('review.forward_history_messages')
            return forward_history if forward_history is not None else False
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥è½¬å‘é…ç½®å¤±è´¥: {e}")
            # å‡ºé”™æ—¶çš„é»˜è®¤è¡Œä¸ºï¼šå®æ—¶æ¶ˆæ¯è½¬å‘ï¼Œå†å²æ¶ˆæ¯ä¸è½¬å‘
            return not is_history
    
    async def _should_forward_history(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è½¬å‘å†å²æ¶ˆæ¯åˆ°å®¡æ ¸ç¾¤ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        return await self._should_forward_to_review(is_history=True)
    
    async def _forward_to_review(self, db_message: Message):
        """è½¬å‘æ¶ˆæ¯åˆ°å®¡æ ¸ç¾¤"""
        try:
            # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¼•ç”¨
            from app.telegram.message_forwarder import message_forwarder
            from app.telegram.bot import telegram_bot
            
            if telegram_bot and telegram_bot.client:
                await message_forwarder.forward_to_review(telegram_bot.client, db_message)
            else:
                logger.warning("Telegramå®¢æˆ·ç«¯æœªè¿æ¥ï¼Œæ— æ³•è½¬å‘åˆ°å®¡æ ¸ç¾¤")
                
        except Exception as e:
            logger.error(f"è½¬å‘åˆ°å®¡æ ¸ç¾¤å¤±è´¥: {e}")
    
    def _should_reject_pure_ad(self, is_ad: bool, filter_reason: str, filtered_content: str, 
                              content: str, media_info: dict, ocr_result: dict) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥å®Œå…¨æ‹’ç»çº¯å¹¿å‘Šæ¶ˆæ¯
        
        Args:
            is_ad: æ˜¯å¦è¢«åˆ¤å®šä¸ºå¹¿å‘Š
            filter_reason: è¿‡æ»¤åŸå› 
            filtered_content: è¿‡æ»¤åçš„å†…å®¹
            content: åŸå§‹å†…å®¹
            media_info: åª’ä½“ä¿¡æ¯
            ocr_result: OCRè¯†åˆ«ç»“æœ
            
        Returns:
            (æ˜¯å¦æ‹’ç», æ‹’ç»åŸå› )
        """
        import re
        
        # é«˜å±å¹¿å‘Šå…³é”®è¯ï¼ˆèµŒåšã€è‰²æƒ…ã€è¯ˆéª—ï¼‰
        HIGH_RISK_AD_KEYWORDS = [
            # èµŒåšå¹³å°ç›¸å…³
            r'(?:é“‚è±|åšè±|Y3|AG|BBIN).*(?:å¨±ä¹|å¨›æ¨‚|å›½é™…|åœ‹éš›|å¹³å°)',
            r'(?:USDT|æ³°è¾¾å¸|è™šæ‹Ÿå¸|åŠ å¯†è´§å¸).*(?:å¨±ä¹åŸ|å¨›æ¨‚åŸ|å¹³å°|å……å€¼|ææ¬¾)',
            r'(?:åšå½©|èµŒåœº|è³­å ´|æ£‹ç‰Œ|ä½“è‚²|é«”è‚²|çœŸäºº|ç”µå­).*(?:å¹³å°|å®˜ç½‘|å®˜ç¶²|å¨±ä¹åŸ)',
            r'(?:é¦–å……|é¦–å­˜|äºŒå­˜|ä¸‰å­˜).*(?:è¿”æ°´|ä¼˜æƒ |å„ªæƒ |èµ é€|è´ˆé€)',
            r'(?:æ—¥å‡º|æ—¥å…¥|æœˆå…¥|æ—¥èµš|æ—¥è³º).*[0-9]+.*[ä¸‡è¬uU]',
            r'(?:å®åŠ›|å¯¦åŠ›|ä¿¡èª‰|ä¿¡è­½).*(?:Uç›˜|Uç›¤|USDT|å‡ºæ¬¾)',
            r'(?:åƒä¸‡|åƒè¬|å·¨æ¬¾|å·¨é¢|å¤§é¢).*(?:æ— å¿§|ç„¡æ†‚|ç§’åˆ°|ææ¬¾)',
            r'777.*(?:è€è™æœº|è€è™æ©Ÿ|slots|æ¸¸æˆ|éŠæˆ²)',
            
            # è‰²æƒ…ç›¸å…³
            r'(?:ä¸Šçº¿|ä¸Šç·š).*(?:ç¦åˆ©|å…«å¤§|å¦¹å¦¹)',
            r'(?:æ°¸ä¹…|å…è´¹|å…è²»).*(?:é€|é¢†å–|é ˜å–|çœ‹ç‰‡)',
            r'(?:å¹¸è¿|å¹¸é‹).*(?:å•|å–®).*(?:å¥–|ç)',
            
            # è¯ˆéª—ç›¸å…³
            r'(?:ä¸€ä¸ªæœˆ|ä¸€å€‹æœˆ).*(?:å¥”é©°|å¥”é¦³|å®é©¬|å¯¶é¦¬)',
            r'(?:ä¸‰ä¸ªæœˆ|ä¸‰å€‹æœˆ).*(?:å¥—æˆ¿|æˆ¿å­)',
            r'(?:æ±½è½¦|æ±½è»Š).*(?:è¿åœ|é•åœ).*(?:æ‹ç…§|ä¸€å¼ |ä¸€å¼µ).*[0-9]+',
            r'(?:æƒ³åŠŸæˆåå°±|èƒ†å­å¤§|è†½å­å¤§).*(?:ç°è‰²|çœ‹æˆ‘)',
            
            # ç‰¹å®šå¹³å°æ ‡è¯†
            r'(?:å®˜æ–¹|å®¢æœ).*(?:QQ|qq|å¾®ä¿¡|WeChat|wechat).*[0-9]+',
            r'(?:æ³¨å†Œ|è¨»å†Š|ç™»å½•|ç™»éŒ„).*(?:å°±é€|å³é€|ç«‹å³é€)',
        ]
        
        # æå–OCRæ–‡å­—å†…å®¹
        ocr_texts = []
        if ocr_result:
            # ä»OCRç»“æœä¸­æå–æ‰€æœ‰æ–‡å­—
            if ocr_result.get('texts'):
                ocr_texts.extend(ocr_result['texts'])
            
            # ä»äºŒç»´ç ä¸­æå–æ–‡å­—å†…å®¹  
            if ocr_result.get('qr_codes'):
                for qr in ocr_result['qr_codes']:
                    if qr.get('data'):
                        ocr_texts.append(qr['data'])
        
        # åˆå¹¶æ‰€æœ‰éœ€è¦æ£€æŸ¥çš„æ–‡æœ¬
        all_text_to_check = content
        if ocr_texts:
            all_text_to_check += " " + " ".join(ocr_texts)
        
        # ä¼˜å…ˆçº§1ï¼šOCRæ£€æµ‹åˆ°é«˜åˆ†å¹¿å‘Šå†…å®¹ - ç›´æ¥æ‹’ç»
        if ocr_result and ocr_result.get('ad_score', 0) >= 50:
            return True, f"å›¾ç‰‡å¹¿å‘Šå†…å®¹è‡ªåŠ¨æ‹’ç»ï¼ˆOCRåˆ†æ•°:{ocr_result.get('ad_score', 0)}ï¼‰"
        
        # ä¼˜å…ˆçº§2ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«é«˜å±èµŒåšå…³é”®è¯
        for pattern in HIGH_RISK_AD_KEYWORDS:
            if re.search(pattern, all_text_to_check, re.IGNORECASE):
                # å¦‚æœè¿˜æœ‰åª’ä½“æ–‡ä»¶ï¼Œæ›´ä¸¥æ ¼
                if media_info:
                    return True, "é«˜é£é™©å¹¿å‘Šè‡ªåŠ¨æ‹’ç»ï¼ˆèµŒåš/è‰²æƒ…/è¯ˆéª—+åª’ä½“ï¼‰"
                # ä»…æ–‡å­—ä¹Ÿå¯èƒ½æ‹’ç»
                elif len(filtered_content.strip()) < 20:  # è¿‡æ»¤åå†…å®¹å¾ˆå°‘
                    return True, "é«˜é£é™©å¹¿å‘Šè‡ªåŠ¨æ‹’ç»ï¼ˆèµŒåš/è‰²æƒ…/è¯ˆéª—å†…å®¹ï¼‰"
        
        # ä¼˜å…ˆçº§3ï¼šçº¯åª’ä½“æ¶ˆæ¯ä¸”OCRæ£€æµ‹åˆ°å¹¿å‘Š
        if not content.strip() and media_info and ocr_result:
            if ocr_result.get('ad_score', 0) >= 30:
                return True, "çº¯åª’ä½“å¹¿å‘Šè‡ªåŠ¨æ‹’ç»ï¼ˆæ— æ–‡å­—å†…å®¹ï¼ŒOCRæ£€æµ‹ä¸ºå¹¿å‘Šï¼‰"
        
        # ä¼˜å…ˆçº§4ï¼šæ–‡æœ¬è¢«å®Œå…¨è¿‡æ»¤ä¸”æœ‰åª’ä½“
        if not filtered_content.strip() and media_info:
            # å¦‚æœOCRä¹Ÿæ£€æµ‹åˆ°å¹¿å‘Šå†…å®¹
            if ocr_result and ocr_result.get('ad_score', 0) >= 30:
                return True, "çº¯å¹¿å‘Šåª’ä½“è‡ªåŠ¨æ‹’ç»ï¼ˆæ–‡å­—+åª’ä½“éƒ½æ˜¯å¹¿å‘Šï¼‰"
            
            # å¦‚æœåŸæ–‡æœ¬è¿‡æ»¤æ‰äº†è¶…è¿‡95%çš„å†…å®¹
            if len(content) > 10 and len(filtered_content) < len(content) * 0.05:
                return True, "ç–‘ä¼¼çº¯å¹¿å‘Šè‡ªåŠ¨æ‹’ç»ï¼ˆæ–‡æœ¬è¿‡æ»¤è¶…95%ï¼‰"
        
        # ä¼˜å…ˆçº§5ï¼šæ•´æ¡æ¶ˆæ¯éƒ½æ˜¯å¹¿å‘Šæ–‡æœ¬çš„å¤„ç†
        if "æ•´æ¡æ¶ˆæ¯éƒ½æ˜¯å¹¿å‘Š" in filter_reason or "é«˜é£é™©å¹¿å‘Š" in filter_reason:
            # æ²¡æœ‰åª’ä½“çš„çº¯æ–‡å­—å¹¿å‘Šï¼Œç›´æ¥æ‹’ç»
            if not media_info:
                return True, "çº¯æ–‡å­—å¹¿å‘Šè‡ªåŠ¨æ‹’ç»"
            # æœ‰åª’ä½“ä¸”OCRä¹Ÿæ˜¯å¹¿å‘Šï¼Œæ‹’ç»
            elif ocr_result and ocr_result.get('ad_score', 0) >= 30:
                return True, "çº¯å¹¿å‘Šæ¶ˆæ¯è‡ªåŠ¨æ‹’ç»ï¼ˆæ–‡å­—+åª’ä½“éƒ½æ˜¯å¹¿å‘Šï¼‰"
        
        return False, ""
    
    async def _broadcast_new_message(self, db_message: Message):
        """å¹¿æ’­æ–°æ¶ˆæ¯åˆ°WebSocketå®¢æˆ·ç«¯"""
        try:
            # ç›´æ¥ä½¿ç”¨websocket_managerï¼Œé¿å…ä¾èµ–telegram_bot
            from app.api.websocket import websocket_manager
            
            # å‡†å¤‡æ¶ˆæ¯æ•°æ®ï¼ˆç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µï¼‰
            message_data = {
                "id": db_message.id,
                "message_id": db_message.message_id,  # æ·»åŠ message_idå­—æ®µ
                "source_channel": db_message.source_channel,
                "content": db_message.content,
                "filtered_content": db_message.filtered_content,
                "media_type": db_message.media_type,
                "media_url": db_message.media_url,
                "is_ad": db_message.is_ad,
                "status": db_message.status,
                "created_at": format_for_api(db_message.created_at),
                "is_combined": db_message.is_combined,
                "media_group": db_message.media_group if db_message.is_combined else None,
                "combined_messages": db_message.combined_messages if db_message.is_combined else None
            }
            
            # å¹¿æ’­æ¶ˆæ¯
            await websocket_manager.broadcast_new_message(message_data)
            logger.info(f"âœ… æˆåŠŸå¹¿æ’­æ–°æ¶ˆæ¯ ID:{db_message.id} åˆ° {len(websocket_manager.active_connections)} ä¸ªWebSocketè¿æ¥")
            
        except ImportError as e:
            logger.error(f"å¯¼å…¥WebSocketç®¡ç†å™¨å¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"å¹¿æ’­æ¶ˆæ¯å¤±è´¥: {e}")

# å¯¼å…¥hashlibï¼ˆç”¨äºç»„åˆåª’ä½“å“ˆå¸Œï¼‰
import hashlib

# å…¨å±€å®ä¾‹
unified_processor = UnifiedMessageProcessor()