"""
ç»Ÿä¸€çš„æ¶ˆæ¯å¤„ç†å™¨
å°†å®æ—¶æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯çš„å¤„ç†æµç¨‹ç»Ÿä¸€ï¼Œç¡®ä¿ä¸€è‡´æ€§å’Œå¯ç»´æŠ¤æ€§
"""
import logging
import os
from typing import Optional, Dict, Any, Tuple
from datetime import datetime
from app.utils.timezone import get_current_time, parse_telegram_time, format_for_api
from telethon.tl.types import Message as TLMessage

from app.core.database import AsyncSessionLocal, Message
from app.services.content_filter import ContentFilter
from app.services.media_handler import media_handler
from app.services.message_grouper import message_grouper
from app.services.duplicate_detector import DuplicateDetector
from app.services.message_processor import MessageProcessor
from app.core.config import db_settings

logger = logging.getLogger(__name__)

class UnifiedMessageProcessor:
    """ç»Ÿä¸€çš„æ¶ˆæ¯å¤„ç†å™¨ - å¤„ç†æ‰€æœ‰æ¥æºçš„æ¶ˆæ¯"""
    
    def __init__(self):
        self.content_filter = ContentFilter()
        self.duplicate_detector = DuplicateDetector()
        self.message_processor = MessageProcessor()
        
    async def process_telegram_message(
        self, 
        message: TLMessage, 
        channel_id: str, 
        is_history: bool = False
    ) -> Optional[Message]:
        """
        ç»Ÿä¸€çš„æ¶ˆæ¯å¤„ç†å…¥å£
        
        Args:
            message: Telegramæ¶ˆæ¯å¯¹è±¡
            channel_id: é¢‘é“IDï¼ˆå·²æ ¼å¼åŒ–ï¼‰
            is_history: æ˜¯å¦ä¸ºå†å²æ¶ˆæ¯
            
        Returns:
            å¤„ç†åçš„æ•°æ®åº“æ¶ˆæ¯å¯¹è±¡ï¼Œå¦‚æœæ¶ˆæ¯è¢«è¿‡æ»¤åˆ™è¿”å›None
        """
        try:
            # æ­¥éª¤1: é¦–å…ˆæå–åŸå§‹å†…å®¹å¹¶ä¿å­˜
            original_content = await self._extract_original_content(message)
            
            # æ­¥éª¤2: é€šç”¨å¤„ç†ï¼ˆæå–å†…å®¹ã€ä¸‹è½½åª’ä½“ã€è¿‡æ»¤å¹¿å‘Šï¼‰
            processed_data = await self._common_message_processing(message, channel_id, is_history)
            if not processed_data:
                return None  # æ¶ˆæ¯è¢«è¿‡æ»¤
            
            # ç¡®ä¿åŸå§‹å†…å®¹è¢«ä¿ç•™
            processed_data['original_content'] = original_content
            
            # æ­¥éª¤3: ç»„åˆæ¶ˆæ¯æ£€æµ‹
            combined_message = await message_grouper.process_message(
                message, 
                channel_id, 
                processed_data.get('media_info'),
                filtered_content=processed_data['filtered_content'],
                is_ad=processed_data['is_ad'],
                is_batch=is_history  # å†å²æ¶ˆæ¯ä½¿ç”¨æ‰¹é‡æ¨¡å¼
            )
            
            # å¦‚æœè¿”å›Noneï¼Œè¯´æ˜æ¶ˆæ¯æ­£åœ¨ç­‰å¾…ç»„åˆ
            if combined_message is None:
                logger.debug(f"æ¶ˆæ¯ {message.id} æ­£åœ¨ç­‰å¾…ç»„åˆ")
                return None
            
            # æ­¥éª¤4: å‡†å¤‡ä¿å­˜æ•°æ®
            save_data = await self._prepare_save_data(
                combined_message, 
                channel_id, 
                processed_data,
                is_history
            )
            
            # æ­¥éª¤5: å»é‡æ£€æµ‹
            duplicate_info = await self._check_duplicate_with_details(save_data, channel_id)
            if duplicate_info:
                logger.info(f"{'å†å²' if is_history else 'å®æ—¶'}æ¶ˆæ¯è¢«å»é‡æ£€æµ‹æ‹’ç»: {duplicate_info['reason']}")
                # ä¿å­˜è¢«å»é‡æ‹’ç»çš„æ¶ˆæ¯åˆ°æ•°æ®åº“ï¼ŒçŠ¶æ€ä¸ºrejected
                save_data['status'] = 'rejected'
                save_data['reject_reason'] = f"å»é‡æ£€æµ‹: {duplicate_info['reason']} (åŸæ¶ˆæ¯ID: {duplicate_info.get('original_id', 'N/A')})"
                save_data['filter_reason'] = duplicate_info['reason']
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                db_message = await self.message_processor.process_new_message(save_data)
                if db_message:
                    logger.info(f"å»é‡æ‹’ç»çš„æ¶ˆæ¯å·²ä¿å­˜åˆ°æ•°æ®åº“ï¼ŒID: {db_message.id}")
                
                # æ¸…ç†åª’ä½“æ–‡ä»¶ï¼ˆå¦‚æœä¸æƒ³ä¿ç•™çš„è¯ï¼‰
                # await self._cleanup_media_files(save_data)
                return db_message
            
            # æ­¥éª¤6: ä¿å­˜åˆ°æ•°æ®åº“
            db_message = await self.message_processor.process_new_message(save_data)
            
            if not db_message:
                logger.info(f"æ¶ˆæ¯ä¿å­˜å¤±è´¥æˆ–è¢«æ‹’ç»")
                await self._cleanup_media_files(save_data)
                return None
            
            # æ­¥éª¤7: è½¬å‘åˆ°å®¡æ ¸ç¾¤ï¼ˆæ ¹æ®é…ç½®å†³å®šï¼‰
            if await self._should_forward_to_review(is_history):
                await self._forward_to_review(db_message)
            
            # æ­¥éª¤8: å¹¿æ’­åˆ°WebSocketï¼ˆæ‰€æœ‰æ–°æ¶ˆæ¯éƒ½å¹¿æ’­ï¼Œè®©webç«¯èƒ½çœ‹åˆ°ï¼‰
            # ä¸å†åŒºåˆ†æ˜¯å¦å†å²æ¶ˆæ¯ï¼Œæ‰€æœ‰æˆåŠŸä¿å­˜çš„æ¶ˆæ¯éƒ½å¹¿æ’­åˆ°webç«¯
            await self._broadcast_new_message(db_message)
            
            return db_message
            
        except Exception as e:
            logger.error(f"ç»Ÿä¸€æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            # æ¸…ç†å¯èƒ½å·²ä¸‹è½½çš„åª’ä½“
            if 'processed_data' in locals() and processed_data:
                media_info = processed_data.get('media_info')
                if media_info and media_info.get('file_path'):
                    await media_handler.cleanup_file(media_info['file_path'])
            return None
    
    async def _extract_original_content(self, message: TLMessage) -> str:
        """
        æå–æ¶ˆæ¯çš„åŸå§‹å†…å®¹ï¼Œç¡®ä¿ä¸ä¸¢å¤±ä»»ä½•æ–‡æœ¬
        
        Args:
            message: Telegramæ¶ˆæ¯å¯¹è±¡
            
        Returns:
            åŸå§‹å†…å®¹å­—ç¬¦ä¸²
        """
        # å°è¯•å¤šç§æ–¹å¼æå–å†…å®¹
        content = ""
        
        # 1. ä¼˜å…ˆä½¿ç”¨textå±æ€§
        if hasattr(message, 'text') and message.text:
            content = message.text
        # 2. å°è¯•raw_text
        elif hasattr(message, 'raw_text') and message.raw_text:
            content = message.raw_text
        # 3. å°è¯•messageå±æ€§
        elif hasattr(message, 'message') and message.message:
            content = message.message
        # 4. å¯¹äºåª’ä½“æ¶ˆæ¯ï¼Œå°è¯•caption
        elif hasattr(message, 'media') and message.media:
            if hasattr(message, 'caption') and message.caption:
                content = message.caption
        
        # è®°å½•åŸå§‹å†…å®¹æå–æƒ…å†µ
        if content:
            logger.info(f"ğŸ“ æå–åˆ°åŸå§‹å†…å®¹: {len(content)} å­—ç¬¦")
            logger.debug(f"åŸå§‹å†…å®¹å‰100å­—ç¬¦: {content[:100]}...")
        else:
            logger.debug(f"ğŸ“ æ¶ˆæ¯æ— æ–‡æœ¬å†…å®¹ï¼ˆçº¯åª’ä½“ï¼‰")
        
        return content
    
    async def _common_message_processing(
        self, 
        message: TLMessage, 
        channel_id: str, 
        is_history: bool
    ) -> Optional[Dict[str, Any]]:
        """
        é€šç”¨æ¶ˆæ¯å¤„ç†é€»è¾‘
        æå–å†…å®¹ã€ä¸‹è½½åª’ä½“ã€è¿‡æ»¤å¹¿å‘Š
        """
        try:
            # æå–æ¶ˆæ¯å†…å®¹
            content = message.text or message.raw_text or message.message or ""
            
            # å¯¹äºåª’ä½“æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰caption
            if not content and message.media:
                if hasattr(message, 'caption'):
                    content = message.caption or ""
                elif hasattr(message, 'raw_text'):
                    content = message.raw_text or ""
            
            # å†æ¬¡å°è¯•è·å–
            if not content and hasattr(message, 'message') and message.message:
                content = message.message
                logger.debug(f"ğŸ“ ä»messageå±æ€§æå–åˆ°å†…å®¹")
            
            # è®°å½•å†…å®¹æå–ç»“æœ
            if content:
                logger.info(f"ğŸ“ æå–åˆ°æ¶ˆæ¯å†…å®¹: {content[:100]}...")
            else:
                logger.debug(f"ğŸ“ æ¶ˆæ¯æ— æ–‡æœ¬å†…å®¹ï¼ˆçº¯åª’ä½“ï¼‰")
            
            # å¤„ç†åª’ä½“
            media_info = None
            if message.media:
                media_info = await self._process_media(message, channel_id)
            
            # å‡†å¤‡åª’ä½“æ–‡ä»¶åˆ—è¡¨ç”¨äºOCRå¤„ç†
            media_files = []
            if media_info and media_info.get('file_path'):
                media_files.append(media_info['file_path'])
            
            # æå–æ¶ˆæ¯å®ä½“ï¼ˆåŒ…æ‹¬éšè—é“¾æ¥ï¼‰
            from app.services.structural_ad_detector import structural_detector
            entities = structural_detector.extract_entity_data(message)
            
            # ç§»é™¤éšè—é“¾æ¥ï¼ˆæ ¹æ®é…ç½®ï¼‰
            removed_hidden_links = []
            from app.services.config_manager import config_manager
            hidden_link_action = await config_manager.get_config('filter.hidden_link_action')
            if hidden_link_action == 'remove' or hidden_link_action is None:  # é»˜è®¤ç§»é™¤
                clean_entities, removed_hidden_links = structural_detector.remove_hidden_links(message)
                if removed_hidden_links:
                    logger.info(f"ğŸ”— ç§»é™¤äº† {len(removed_hidden_links)} ä¸ªéšè—é“¾æ¥")
            
            # å†…å®¹è¿‡æ»¤ï¼ˆæ™ºèƒ½å»å°¾éƒ¨ + ç»“æ„åŒ–å¹¿å‘Šæ£€æµ‹ + AIå¹¿å‘Šæ£€æµ‹ + OCRå›¾ç‰‡æ–‡å­—æå–ï¼‰
            is_ad, filtered_content, filter_reason, ocr_result = await self.content_filter.filter_message(
                content, 
                channel_id=channel_id,
                message_obj=message,  # ä¼ é€’æ¶ˆæ¯å¯¹è±¡ç”¨äºç»“æ„åŒ–æ£€æµ‹
                media_files=media_files  # ä¼ é€’åª’ä½“æ–‡ä»¶ç”¨äºOCRå¤„ç†
            )
            
            # è®°å½•è¿‡æ»¤æ•ˆæœ
            if content != filtered_content:
                original_len = len(content)
                filtered_len = len(filtered_content)
                logger.info(f"ğŸ“ å†…å®¹è¿‡æ»¤: {original_len} -> {filtered_len} å­—ç¬¦ (å‡å°‘ {original_len - filtered_len})")
            
            if is_ad:
                logger.info(f"ğŸš« æ£€æµ‹åˆ°å¹¿å‘Š: {filter_reason}")
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥å®Œå…¨æ‹’ç»çº¯å¹¿å‘Šæ¶ˆæ¯
                should_reject, reject_reason = self._should_reject_pure_ad(
                    is_ad, filter_reason, filtered_content, content, media_info, ocr_result
                )
                
                if should_reject:
                    logger.warning(f"ğŸš¨ æ‹’ç»çº¯å¹¿å‘Šæ¶ˆæ¯: {reject_reason}")
                    # æ¸…ç†åª’ä½“æ–‡ä»¶
                    if media_info and media_info.get('file_path'):
                        await media_handler.cleanup_file(media_info['file_path'])
                    return None
                
                # å¦‚æœé…ç½®äº†è‡ªåŠ¨è¿‡æ»¤å¹¿å‘Šï¼Œç›´æ¥è¿”å›None
                if await db_settings.get_auto_filter_ads():
                    logger.info(f"è‡ªåŠ¨è¿‡æ»¤å¹¿å‘Šæ¶ˆæ¯")
                    if media_info and media_info.get('file_path'):
                        await media_handler.cleanup_file(media_info['file_path'])
                    return None
            
            # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
            # å¦‚æœæ—¢æ²¡æœ‰åª’ä½“ï¼Œfiltered_contentåˆä¸ºç©ºï¼Œåˆ™æ‹’ç»è¿™æ¡æ¶ˆæ¯
            if not media_info and not filtered_content:
                logger.warning(f"âŒ æ¶ˆæ¯æ—¢æ— åª’ä½“åˆæ— æœ‰æ•ˆå†…å®¹ï¼Œæ‹’ç»å¤„ç† (åŸå†…å®¹é•¿åº¦: {len(content)})")
                return None
            
            return {
                'content': content,
                'filtered_content': filtered_content,
                'is_ad': is_ad,
                'filter_reason': filter_reason,
                'media_info': media_info,
                'ocr_result': ocr_result,  # åŒ…å«OCRæå–ç»“æœ
                'entities': entities,  # æ‰€æœ‰å®ä½“ä¿¡æ¯
                'removed_hidden_links': removed_hidden_links  # è¢«ç§»é™¤çš„éšè—é“¾æ¥
            }
            
        except Exception as e:
            logger.error(f"é€šç”¨æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            return None
    
    async def _process_media(self, message: TLMessage, channel_id: str) -> Optional[Dict]:
        """å¤„ç†åª’ä½“ä¸‹è½½"""
        try:
            media_type = None
            if hasattr(message.media, 'photo'):
                media_type = "photo"
                timeout = 30.0
            elif hasattr(message.media, 'document'):
                media_type = "document"
                document = message.media.document
                mime_type = document.mime_type or ""
                timeout = 120.0 if mime_type.startswith("video/") else 60.0
            else:
                return None
            
            # è·å–Telegramå®¢æˆ·ç«¯
            from app.telegram.bot import telegram_bot
            if not telegram_bot or not telegram_bot.client:
                logger.warning("Telegramå®¢æˆ·ç«¯æœªè¿æ¥ï¼Œæ— æ³•ä¸‹è½½åª’ä½“")
                return None
            
            # ä¸‹è½½åª’ä½“ï¼ˆéœ€è¦ä¼ é€’clientå’Œmessage_idï¼‰
            media_info = await media_handler.download_media(
                telegram_bot.client,
                message, 
                message.id,
                timeout=timeout
            )
            
            if not media_info or not media_info.get('file_path'):
                logger.warning(f"åª’ä½“ä¸‹è½½å¤±è´¥æˆ–è¶…æ—¶")
                return None
            
            # è¿”å›åª’ä½“ä¿¡æ¯ï¼ˆmedia_handlerå·²ç»è®¡ç®—äº†å“ˆå¸Œå’Œè§†è§‰å“ˆå¸Œï¼‰
            return media_info
            
        except Exception as e:
            logger.error(f"åª’ä½“å¤„ç†å¤±è´¥: {e}")
            return None
    
    async def _prepare_save_data(
        self, 
        message_data: dict, 
        channel_id: str,
        processed_data: dict,
        is_history: bool
    ) -> dict:
        """å‡†å¤‡ä¿å­˜åˆ°æ•°æ®åº“çš„æ•°æ®"""
        # æå–åª’ä½“å“ˆå¸Œ
        media_hash = None
        combined_media_hash = None
        visual_hash = None
        
        if message_data.get('is_combined'):
            # ç»„åˆæ¶ˆæ¯çš„å“ˆå¸Œå¤„ç†
            if message_data.get('media_group'):
                hashes = []
                visual_hashes = []
                for media_item in message_data['media_group']:
                    if media_item.get('hash'):
                        hashes.append(media_item['hash'])
                    if media_item.get('visual_hashes'):
                        visual_hashes.append(media_item['visual_hashes'])
                
                if hashes:
                    combined_media_hash = hashlib.sha256(''.join(sorted(hashes)).encode()).hexdigest()
                if visual_hashes:
                    visual_hash = str(visual_hashes)
        else:
            # å•ç‹¬æ¶ˆæ¯çš„å“ˆå¸Œ
            media_info = processed_data.get('media_info')
            if media_info:
                media_hash = media_info.get('hash')
                if media_info.get('visual_hashes'):
                    visual_hash = str(media_info['visual_hashes'])
        
        # å¤„ç†æ—¶é—´æˆ³ï¼Œç¡®ä¿æ˜¯æ— æ—¶åŒºçš„UTC datetime
        created_at = parse_telegram_time(message_data.get('date'))
        
        # å¤„ç†OCRç»“æœ
        ocr_result = processed_data.get('ocr_result', {})
        ocr_text = None
        qr_codes = None
        ocr_ad_score = 0
        ocr_processed = False
        
        if ocr_result:
            # å°†OCRæ–‡å­—è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å­˜å‚¨
            if ocr_result.get('texts'):
                import json
                ocr_text = json.dumps(ocr_result['texts'], ensure_ascii=False)
            
            # å°†äºŒç»´ç ä¿¡æ¯è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å­˜å‚¨
            if ocr_result.get('qr_codes'):
                qr_codes = json.dumps(ocr_result['qr_codes'], ensure_ascii=False)
            
            ocr_ad_score = int(ocr_result.get('ad_score', 0))
            ocr_processed = bool(ocr_result.get('processed_files', 0) > 0)
        
        return {
            'source_channel': channel_id,
            'message_id': message_data.get('message_id', message_data.get('id')),
            'content': processed_data.get('original_content', message_data.get('content', processed_data['content'])),  # ä¼˜å…ˆä½¿ç”¨åŸå§‹å†…å®¹
            'filtered_content': message_data.get('filtered_content', processed_data['filtered_content']),
            'is_ad': message_data.get('is_ad', processed_data['is_ad']),
            'media_type': message_data.get('media_type'),
            'media_url': message_data.get('media_url'),
            'media_hash': media_hash,
            # æ–°å¢OCRç›¸å…³å­—æ®µ
            'ocr_text': ocr_text,
            'qr_codes': qr_codes,
            'ocr_ad_score': ocr_ad_score,
            'ocr_processed': ocr_processed,
            # æ–°å¢å®ä½“ç›¸å…³å­—æ®µ
            'entities': processed_data.get('entities'),
            'removed_hidden_links': processed_data.get('removed_hidden_links'),
            'combined_media_hash': combined_media_hash,
            'visual_hash': visual_hash,
            'grouped_id': str(message_data.get('grouped_id')) if message_data.get('grouped_id') else None,
            'is_combined': message_data.get('is_combined', False),
            'combined_messages': message_data.get('combined_messages'),
            'media_group': message_data.get('media_group'),
            'status': 'pending',  # æ‰€æœ‰æ¶ˆæ¯éƒ½å…ˆè®¾ä¸ºpendingçŠ¶æ€ï¼Œç­‰å¾…å®¡æ ¸
            'created_at': created_at
        }
    
    async def _check_duplicate_with_details(self, save_data: dict, channel_id: str) -> Optional[dict]:
        """æ£€æŸ¥æ˜¯å¦é‡å¤å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯"""
        try:
            # æå–è§†è§‰å“ˆå¸Œï¼ˆå¦‚æœæœ‰ï¼‰
            visual_hashes = None
            media_info = save_data.get('media_info')
            if media_info and media_info.get('visual_hashes'):
                visual_hashes = media_info['visual_hashes']
            else:
                # å…¼å®¹æ—§æ ¼å¼
                try:
                    import json
                    if save_data.get('visual_hash'):
                        visual_hashes = json.loads(save_data['visual_hash'])
                except:
                    pass
            
            is_duplicate, orig_id, dup_type = await self.duplicate_detector.is_duplicate_message(
                source_channel=channel_id,
                media_hash=save_data.get('media_hash'),
                combined_media_hash=save_data.get('combined_media_hash'),
                content=save_data.get('content'),
                message_time=save_data.get('created_at'),
                visual_hashes=visual_hashes
            )
            
            if is_duplicate:
                logger.info(f"æ£€æµ‹åˆ°é‡å¤æ¶ˆæ¯ï¼ˆ{dup_type}ï¼‰ï¼ŒåŸå§‹æ¶ˆæ¯ID: {orig_id}")
                return {
                    'is_duplicate': True,
                    'original_id': orig_id,
                    'type': dup_type,
                    'reason': f"{dup_type}é‡å¤"
                }
                
            return None
            
        except Exception as e:
            logger.error(f"é‡å¤æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    async def _is_duplicate(self, save_data: dict, channel_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ¶ˆæ¯"""
        try:
            # è§£æè§†è§‰å“ˆå¸Œ
            visual_hashes = None
            if save_data.get('visual_hash'):
                try:
                    visual_hashes = eval(save_data['visual_hash'])
                    if isinstance(visual_hashes, list) and visual_hashes:
                        visual_hashes = visual_hashes[0]
                except:
                    pass
            
            is_duplicate, orig_id, dup_type = await self.duplicate_detector.is_duplicate_message(
                source_channel=channel_id,
                media_hash=save_data.get('media_hash'),
                combined_media_hash=save_data.get('combined_media_hash'),
                content=save_data.get('content'),
                message_time=save_data.get('created_at'),
                visual_hashes=visual_hashes
            )
            
            if is_duplicate:
                logger.info(f"æ£€æµ‹åˆ°é‡å¤æ¶ˆæ¯ï¼ˆ{dup_type}ï¼‰ï¼ŒåŸå§‹æ¶ˆæ¯ID: {orig_id}")
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"é‡å¤æ£€æµ‹å¤±è´¥: {e}")
            return False
    
    async def _cleanup_media_files(self, save_data: dict):
        """æ¸…ç†åª’ä½“æ–‡ä»¶"""
        try:
            # æ¸…ç†å•ä¸ªåª’ä½“æ–‡ä»¶
            if save_data.get('media_url') and os.path.exists(save_data['media_url']):
                await media_handler.cleanup_file(save_data['media_url'])
            
            # æ¸…ç†ç»„åˆæ¶ˆæ¯çš„åª’ä½“æ–‡ä»¶
            if save_data.get('media_group'):
                for media_item in save_data['media_group']:
                    file_path = media_item.get('file_path')
                    if file_path and os.path.exists(file_path):
                        await media_handler.cleanup_file(file_path)
                        
        except Exception as e:
            logger.error(f"æ¸…ç†åª’ä½“æ–‡ä»¶å¤±è´¥: {e}")
    
    async def _should_forward_to_review(self, is_history: bool) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥è½¬å‘æ¶ˆæ¯åˆ°å®¡æ ¸ç¾¤
        
        Args:
            is_history: æ˜¯å¦ä¸ºå†å²æ¶ˆæ¯
            
        Returns:
            æ˜¯å¦åº”è¯¥è½¬å‘åˆ°å®¡æ ¸ç¾¤
        """
        try:
            from app.services.config_manager import config_manager
            
            # è·å–é…ç½®ï¼šæ˜¯å¦å¯ç”¨å®¡æ ¸ç¾¤è½¬å‘
            enable_review = await config_manager.get_config('review.enable_forward_to_group')
            if enable_review is False:
                return False
            
            # å¯¹äºå®æ—¶æ¶ˆæ¯ï¼Œé»˜è®¤è½¬å‘
            if not is_history:
                return True
            
            # å¯¹äºå†å²æ¶ˆæ¯ï¼Œæ£€æŸ¥ä¸“é—¨çš„é…ç½®
            forward_history = await config_manager.get_config('review.forward_history_messages')
            return forward_history if forward_history is not None else False
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥è½¬å‘é…ç½®å¤±è´¥: {e}")
            # å‡ºé”™æ—¶çš„é»˜è®¤è¡Œä¸ºï¼šå®æ—¶æ¶ˆæ¯è½¬å‘ï¼Œå†å²æ¶ˆæ¯ä¸è½¬å‘
            return not is_history
    
    async def _should_forward_history(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è½¬å‘å†å²æ¶ˆæ¯åˆ°å®¡æ ¸ç¾¤ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        return await self._should_forward_to_review(is_history=True)
    
    async def _forward_to_review(self, db_message: Message):
        """è½¬å‘æ¶ˆæ¯åˆ°å®¡æ ¸ç¾¤"""
        try:
            # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¼•ç”¨
            from app.telegram.message_forwarder import message_forwarder
            from app.telegram.bot import telegram_bot
            
            if telegram_bot and telegram_bot.client:
                await message_forwarder.forward_to_review(telegram_bot.client, db_message)
            else:
                logger.warning("Telegramå®¢æˆ·ç«¯æœªè¿æ¥ï¼Œæ— æ³•è½¬å‘åˆ°å®¡æ ¸ç¾¤")
                
        except Exception as e:
            logger.error(f"è½¬å‘åˆ°å®¡æ ¸ç¾¤å¤±è´¥: {e}")
    
    def _should_reject_pure_ad(self, is_ad: bool, filter_reason: str, filtered_content: str, 
                              content: str, media_info: dict, ocr_result: dict) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥å®Œå…¨æ‹’ç»çº¯å¹¿å‘Šæ¶ˆæ¯
        
        Args:
            is_ad: æ˜¯å¦è¢«åˆ¤å®šä¸ºå¹¿å‘Š
            filter_reason: è¿‡æ»¤åŸå› 
            filtered_content: è¿‡æ»¤åçš„å†…å®¹
            content: åŸå§‹å†…å®¹
            media_info: åª’ä½“ä¿¡æ¯
            ocr_result: OCRè¯†åˆ«ç»“æœ
            
        Returns:
            (æ˜¯å¦æ‹’ç», æ‹’ç»åŸå› )
        """
        import re
        
        # é«˜å±å¹¿å‘Šå…³é”®è¯
        HIGH_RISK_AD_KEYWORDS = [
            # èµŒåšç›¸å…³
            r'[Yy]3.*(?:å¨±ä¹|å¨›æ¨‚|å›½é™…|åœ‹éš›)',
            r'(?:USDT|æ³°è¾¾å¸|è™šæ‹Ÿå¸).*(?:å¨±ä¹åŸ|å¨›æ¨‚åŸ|å¹³å°)',
            r'(?:åšå½©|èµŒåœº|è³­å ´|æ£‹ç‰Œ|ä½“è‚²|é«”è‚²).*(?:å¹³å°|å®˜ç½‘|å®˜ç¶²)',
            r'(?:é¦–å……|é¦–å­˜).*(?:è¿”æ°´|ä¼˜æƒ |å„ªæƒ )',
            r'(?:æ—¥å‡º|æ—¥å…¥).*[0-9]+.*[uU]',
            r'(?:å®åŠ›|å¯¦åŠ›).*(?:Uç›˜|Uç›¤|USDT)',
            r'(?:åƒä¸‡|åƒè¬|å·¨æ¬¾).*(?:æ— å¿§|ç„¡æ†‚)',
            
            # è‰²æƒ…ç›¸å…³
            r'(?:ä¸Šçº¿|ä¸Šç·š).*(?:ç¦åˆ©|å…«å¤§)',
            r'(?:æ°¸ä¹…|å…è´¹|å…è²»).*(?:é€|é¢†å–|é ˜å–)',
            r'(?:å¹¸è¿|å¹¸é‹).*(?:å•|å–®).*(?:å¥–|ç)',
            
            # è¯ˆéª—ç›¸å…³
            r'(?:ä¸€ä¸ªæœˆ|ä¸€å€‹æœˆ).*(?:å¥”é©°|å¥”é¦³|å®é©¬|å¯¶é¦¬)',
            r'(?:ä¸‰ä¸ªæœˆ|ä¸‰å€‹æœˆ).*(?:å¥—æˆ¿|æˆ¿å­)',
            r'(?:æ±½è½¦|æ±½è»Š).*(?:è¿åœ|é•åœ).*(?:æ‹ç…§|ä¸€å¼ |ä¸€å¼µ).*[0-9]+',
            r'(?:æƒ³åŠŸæˆåå°±|èƒ†å­å¤§|è†½å­å¤§).*(?:ç°è‰²|çœ‹æˆ‘)',
        ]
        
        # æƒ…å†µ1ï¼šæ•´æ¡æ¶ˆæ¯éƒ½æ˜¯å¹¿å‘Šæ–‡æœ¬
        if "æ•´æ¡æ¶ˆæ¯éƒ½æ˜¯å¹¿å‘Š" in filter_reason:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é«˜å±å…³é”®è¯
            for pattern in HIGH_RISK_AD_KEYWORDS:
                if re.search(pattern, content, re.IGNORECASE):
                    return True, "é«˜é£é™©çº¯å¹¿å‘Šï¼ˆèµŒåš/è‰²æƒ…/è¯ˆéª—ï¼‰"
            
            # å¦‚æœæœ‰åª’ä½“æ–‡ä»¶
            if media_info:
                # å¦‚æœOCRç»“æœæ˜¾ç¤ºåª’ä½“ä¹ŸåŒ…å«å¹¿å‘Šå†…å®¹
                if ocr_result and ocr_result.get('ad_score', 0) > 0.5:
                    return True, "çº¯å¹¿å‘Šæ¶ˆæ¯ï¼ˆæ–‡æœ¬+åª’ä½“éƒ½æ˜¯å¹¿å‘Šï¼‰"
                
                # å¦‚æœOCRæå–åˆ°äº†ç±»ä¼¼çš„å¹¿å‘Šæ–‡å­—
                if ocr_result and ocr_result.get('ocr_text'):
                    ocr_text = ocr_result.get('ocr_text', '')
                    # æ£€æŸ¥OCRæ–‡å­—æ˜¯å¦åŒ…å«é«˜å±å…³é”®è¯
                    for pattern in HIGH_RISK_AD_KEYWORDS:
                        if re.search(pattern, ocr_text, re.IGNORECASE):
                            return True, "çº¯å¹¿å‘Šæ¶ˆæ¯ï¼ˆåª’ä½“å«é«˜å±å¹¿å‘Šå†…å®¹ï¼‰"
            else:
                # æ²¡æœ‰åª’ä½“ï¼Œçº¯å¹¿å‘Šæ–‡æœ¬ï¼Œç›´æ¥æ‹’ç»
                return True, "çº¯å¹¿å‘Šæ–‡æœ¬ï¼ˆæ— åª’ä½“ï¼‰"
        
        # æƒ…å†µ2ï¼šæ–‡æœ¬è¢«å®Œå…¨è¿‡æ»¤ä¸”æœ‰åª’ä½“
        if not filtered_content.strip() and media_info:
            # æ£€æŸ¥åŸæ–‡æœ¬æ˜¯å¦åŒ…å«é«˜å±å…³é”®è¯
            for pattern in HIGH_RISK_AD_KEYWORDS:
                if re.search(pattern, content, re.IGNORECASE):
                    return True, "é«˜é£é™©å¹¿å‘Šåª’ä½“ï¼ˆæ–‡æœ¬å«é«˜å±å†…å®¹ï¼‰"
            
            # æ£€æŸ¥OCRç»“æœ
            if ocr_result:
                # é«˜å¹¿å‘Šåˆ†æ•°
                if ocr_result.get('ad_score', 0) > 0.7:
                    return True, "çº¯å¹¿å‘Šåª’ä½“ï¼ˆOCRæ£€æµ‹é«˜åˆ†ï¼‰"
                
                # OCRæ–‡å­—åŒ…å«é«˜å±å…³é”®è¯
                ocr_text = ocr_result.get('ocr_text', '')
                if ocr_text:
                    for pattern in HIGH_RISK_AD_KEYWORDS:
                        if re.search(pattern, ocr_text, re.IGNORECASE):
                            return True, "çº¯å¹¿å‘Šåª’ä½“ï¼ˆOCRå«é«˜å±å†…å®¹ï¼‰"
            
            # å¦‚æœæ–‡æœ¬è¿‡æ»¤æ‰äº†è¶…è¿‡90%çš„å†…å®¹ï¼Œä¹Ÿè§†ä¸ºçº¯å¹¿å‘Š
            if len(content) > 0 and len(filtered_content) < len(content) * 0.1:
                return True, "ç–‘ä¼¼çº¯å¹¿å‘Šï¼ˆæ–‡æœ¬è¿‡æ»¤è¶…90%ï¼‰"
        
        return False, ""
    
    async def _broadcast_new_message(self, db_message: Message):
        """å¹¿æ’­æ–°æ¶ˆæ¯åˆ°WebSocketå®¢æˆ·ç«¯"""
        try:
            # ç›´æ¥ä½¿ç”¨websocket_managerï¼Œé¿å…ä¾èµ–telegram_bot
            from app.api.websocket import websocket_manager
            
            # å‡†å¤‡æ¶ˆæ¯æ•°æ®ï¼ˆç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µï¼‰
            message_data = {
                "id": db_message.id,
                "message_id": db_message.message_id,  # æ·»åŠ message_idå­—æ®µ
                "source_channel": db_message.source_channel,
                "content": db_message.content,
                "filtered_content": db_message.filtered_content,
                "media_type": db_message.media_type,
                "media_url": db_message.media_url,
                "is_ad": db_message.is_ad,
                "status": db_message.status,
                "created_at": format_for_api(db_message.created_at),
                "is_combined": db_message.is_combined,
                "media_group": db_message.media_group if db_message.is_combined else None,
                "combined_messages": db_message.combined_messages if db_message.is_combined else None
            }
            
            # å¹¿æ’­æ¶ˆæ¯
            await websocket_manager.broadcast_new_message(message_data)
            logger.info(f"âœ… æˆåŠŸå¹¿æ’­æ–°æ¶ˆæ¯ ID:{db_message.id} åˆ° {len(websocket_manager.active_connections)} ä¸ªWebSocketè¿æ¥")
            
        except ImportError as e:
            logger.error(f"å¯¼å…¥WebSocketç®¡ç†å™¨å¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"å¹¿æ’­æ¶ˆæ¯å¤±è´¥: {e}")

# å¯¼å…¥hashlibï¼ˆç”¨äºç»„åˆåª’ä½“å“ˆå¸Œï¼‰
import hashlib

# å…¨å±€å®ä¾‹
unified_processor = UnifiedMessageProcessor()