"""
æ™ºèƒ½è‡ªå­¦ä¹ ç³»ç»Ÿ
è§£å†³åŸå§‹è®­ç»ƒæœºåˆ¶çš„é—®é¢˜ï¼Œå®ç°çœŸæ­£çš„æ™ºèƒ½åŒ–å­¦ä¹ 
"""
import re
import json
import logging
import hashlib
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
from pathlib import Path
import numpy as np
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)


@dataclass
class Pattern:
    """æ¨å¹¿å†…å®¹æ¨¡å¼"""
    id: str
    structure: List[str]  # ç»“æ„æ¨¡å¼
    features: Dict[str, float]  # ç‰¹å¾å‘é‡
    confidence: float  # ç½®ä¿¡åº¦
    created_at: str
    usage_count: int = 0
    success_rate: float = 0.0
    last_used: Optional[str] = None


class FeatureExtractor:
    """
    ç‰¹å¾æå–å™¨ - ä»æ–‡æœ¬ä¸­æå–å¤šç»´åº¦ç‰¹å¾
    ä¸è®°å¿†åŸå§‹æ–‡æœ¬ï¼Œåªæå–ç‰¹å¾
    """
    
    def __init__(self):
        self.promo_keywords = {
            'è®¢é˜…', 'è®¢é–±', 'å…³æ³¨', 'é—œæ³¨', 'åŠ å…¥', 'æŠ•ç¨¿', 'çˆ†æ–™',
            'å•†åŠ¡', 'å•†å‹™', 'è”ç³»', 'è¯ç¹«', 'é¢‘é“', 'é »é“', 'å®¢æœ'
        }
        self.link_patterns = [
            r't\.me/[\w+]+',
            r'@[\w]+',
            r'https?://[\w\./]+',
        ]
    
    def extract_features(self, text: str, position_ratio: float = 1.0) -> Dict[str, float]:
        """
        æå–æ–‡æœ¬ç‰¹å¾
        
        Args:
            text: å¾…åˆ†ææ–‡æœ¬
            position_ratio: æ–‡æœ¬åœ¨æ¶ˆæ¯ä¸­çš„ä½ç½®æ¯”ä¾‹ï¼ˆ0=å¼€å¤´, 1=ç»“å°¾ï¼‰
            
        Returns:
            ç‰¹å¾å­—å…¸
        """
        if not text:
            return {}
        
        lines = text.split('\n')
        text_length = len(text)
        
        features = {
            # ç»“æ„ç‰¹å¾
            'line_count': len(lines),
            'avg_line_length': sum(len(line) for line in lines) / max(len(lines), 1),
            'empty_line_ratio': sum(1 for line in lines if not line.strip()) / max(len(lines), 1),
            
            # é“¾æ¥ç‰¹å¾
            'has_telegram_link': 1.0 if 't.me/' in text else 0.0,
            'has_username': 1.0 if '@' in text else 0.0,
            'link_count': len(re.findall(r'(?:t\.me/|@|https?://)', text)),
            'link_density': len(re.findall(r'(?:t\.me/|@|https?://)', text)) / max(text_length, 1) * 100,
            
            # è¡¨æƒ…ç¬¦å·ç‰¹å¾
            'emoji_count': len(re.findall(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿ğŸ€-ğŸ¿]', text)),
            'emoji_density': len(re.findall(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿ğŸ€-ğŸ¿]', text)) / max(text_length, 1),
            
            # å…³é”®è¯ç‰¹å¾
            'promo_keyword_count': sum(1 for kw in self.promo_keywords if kw in text),
            'promo_keyword_density': sum(1 for kw in self.promo_keywords if kw in text) / max(len(lines), 1),
            
            # æ ¼å¼ç‰¹å¾
            'has_separator': 1.0 if re.search(r'^[-=*#_~â€”]{3,}$', text, re.MULTILINE) else 0.0,
            'bold_text_ratio': text.count('**') / max(text_length, 1) * 100,
            
            # ä½ç½®ç‰¹å¾
            'position_ratio': position_ratio,
            'is_at_end': 1.0 if position_ratio > 0.8 else 0.0,
            
            # è¯­ä¹‰ç‰¹å¾
            'has_call_to_action': 1.0 if any(word in text for word in ['è®¢é˜…', 'å…³æ³¨', 'åŠ å…¥', 'ç‚¹å‡»']) else 0.0,
            'has_contact_info': 1.0 if any(word in text for word in ['è”ç³»', 'æŠ•ç¨¿', 'å®¢æœ', 'å•†åŠ¡']) else 0.0,
        }
        
        return features
    
    def extract_structure(self, text: str) -> List[str]:
        """
        æå–æ–‡æœ¬ç»“æ„æ¨¡å¼
        å°†æ–‡æœ¬è½¬æ¢ä¸ºæŠ½è±¡çš„ç»“æ„è¡¨ç¤º
        """
        lines = text.split('\n')
        structure = []
        
        for line in lines:
            line = line.strip()
            
            if not line:
                structure.append('EMPTY')
            elif '@' in line and len(line) < 50:
                structure.append('USERNAME')
            elif 't.me/' in line:
                structure.append('TELEGRAM_LINK')
            elif re.match(r'^https?://', line):
                structure.append('URL')
            elif re.match(r'^[-=*#_~â€”]{3,}$', line):
                structure.append('SEPARATOR')
            elif re.match(r'^[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿ğŸ€-ğŸ¿]{2,}', line):
                structure.append('EMOJI_LINE')
            elif any(kw in line for kw in ['è®¢é˜…', 'å…³æ³¨', 'é¢‘é“']):
                structure.append('SUBSCRIBE_TEXT')
            elif any(kw in line for kw in ['æŠ•ç¨¿', 'çˆ†æ–™', 'è”ç³»']):
                structure.append('CONTACT_TEXT')
            elif len(line) < 20:
                structure.append('SHORT_TEXT')
            else:
                structure.append('LONG_TEXT')
        
        return structure


class SampleValidator:
    """
    æ ·æœ¬éªŒè¯å™¨ - ç¡®ä¿è®­ç»ƒæ ·æœ¬çš„è´¨é‡
    """
    
    def __init__(self):
        self.news_keywords = {
            'æ”¿åºœ', 'å›½å®¶', 'æ€»ç»Ÿ', 'éƒ¨é•¿', 'è­¦æ–¹', 'æ³•é™¢',
            'äº¿', 'ä¸‡', 'ç¾å…ƒ', 'äººæ°‘å¸', 'è‚¡ç¥¨', 'ç»æµ',
            'å…¬å¸', 'ä¼ä¸š', 'é›†å›¢', 'å‘å¸ƒ', 'å®£å¸ƒ', 'è¡¨ç¤º'
        }
        self.min_sample_length = 20
        self.max_sample_length = 500
    
    def validate(self, sample: str, original_message: str, message_id: int = None) -> Dict[str, Any]:
        """
        éªŒè¯è®­ç»ƒæ ·æœ¬çš„åˆç†æ€§
        
        Args:
            sample: è®­ç»ƒæ ·æœ¬
            original_message: åŸå§‹æ¶ˆæ¯
            message_id: æ¶ˆæ¯IDï¼ˆç”¨äºé˜²æ­¢è‡ªå¼•ç”¨ï¼‰
            
        Returns:
            éªŒè¯ç»“æœ
        """
        results = {
            'is_valid': False,
            'confidence': 0.0,
            'checks': {},
            'errors': []
        }
        
        # 1. åŸºç¡€æ£€æŸ¥
        if not sample or not original_message:
            results['errors'].append("æ ·æœ¬æˆ–åŸå§‹æ¶ˆæ¯ä¸ºç©º")
            return results
        
        # 2. é•¿åº¦æ£€æŸ¥
        results['checks']['length_valid'] = self.min_sample_length <= len(sample) <= self.max_sample_length
        if not results['checks']['length_valid']:
            results['errors'].append(f"æ ·æœ¬é•¿åº¦ä¸åˆç†: {len(sample)}")
        
        # 3. æ¨å¹¿å†…å®¹æ£€æŸ¥
        results['checks']['is_promotional'] = self._check_promotional_content(sample)
        if not results['checks']['is_promotional']:
            results['errors'].append("æ ·æœ¬ä¸åŒ…å«æ¨å¹¿ç‰¹å¾")
        
        # 4. éæ­£æ–‡å†…å®¹æ£€æŸ¥
        results['checks']['not_news_content'] = self._check_not_news_content(sample)
        if not results['checks']['not_news_content']:
            results['errors'].append("æ ·æœ¬åŒ…å«æ–°é—»æ­£æ–‡å†…å®¹")
        
        # 5. ä½ç½®åˆç†æ€§æ£€æŸ¥
        results['checks']['position_valid'] = self._check_position_validity(sample, original_message)
        if not results['checks']['position_valid']:
            results['errors'].append("æ ·æœ¬ä¸åœ¨æ¶ˆæ¯å°¾éƒ¨")
        
        # 6. è‡ªå¼•ç”¨æ£€æŸ¥
        if message_id:
            results['checks']['no_self_reference'] = self._check_no_self_reference(sample, message_id)
            if not results['checks']['no_self_reference']:
                results['errors'].append("ä¸èƒ½ç”¨æ¶ˆæ¯è‡ªå·±çš„å†…å®¹ä½œä¸ºè®­ç»ƒæ ·æœ¬")
        
        # è®¡ç®—ç½®ä¿¡åº¦
        passed_checks = sum(1 for v in results['checks'].values() if v)
        total_checks = len(results['checks'])
        results['confidence'] = passed_checks / max(total_checks, 1)
        
        # åˆ¤æ–­æ˜¯å¦æœ‰æ•ˆ
        results['is_valid'] = (
            results['confidence'] >= 0.7 and
            results['checks'].get('is_promotional', False) and
            results['checks'].get('position_valid', False)
        )
        
        return results
    
    def _check_promotional_content(self, sample: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨å¹¿ç‰¹å¾"""
        promo_indicators = [
            '@',  # Telegramç”¨æˆ·å
            't.me/',  # Telegramé“¾æ¥
            'è®¢é˜…', 'è¨‚é–±', 'å…³æ³¨', 'é—œæ³¨',
            'é¢‘é“', 'é »é“', 'æŠ•ç¨¿', 'çˆ†æ–™',
            'è”ç³»', 'è¯ç¹«', 'å•†åŠ¡', 'å•†å‹™'
        ]
        
        # è‡³å°‘åŒ…å«2ä¸ªæ¨å¹¿ç‰¹å¾
        indicator_count = sum(1 for indicator in promo_indicators if indicator in sample)
        return indicator_count >= 2
    
    def _check_not_news_content(self, sample: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸åŒ…å«æ–°é—»æ­£æ–‡"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šæ–°é—»å…³é”®è¯
        news_word_count = sum(1 for keyword in self.news_keywords if keyword in sample)
        
        # å¦‚æœåŒ…å«è¶…è¿‡3ä¸ªæ–°é—»å…³é”®è¯ï¼Œå¯èƒ½æ˜¯æ­£æ–‡
        if news_word_count > 3:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥æœŸã€é‡‘é¢ç­‰
        if re.search(r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥', sample):
            return False
        if re.search(r'\d+[äº¿ä¸‡]', sample):
            return False
        
        return True
    
    def _check_position_validity(self, sample: str, original: str) -> bool:
        """æ£€æŸ¥ä½ç½®åˆç†æ€§"""
        if sample not in original:
            return False
        
        # æ‰¾åˆ°æ ·æœ¬åœ¨åŸæ–‡ä¸­çš„ä½ç½®
        position = original.rfind(sample)
        if position == -1:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ¶ˆæ¯æœ«å°¾é™„è¿‘
        after_content = original[position + len(sample):].strip()
        
        # åé¢çš„å†…å®¹ä¸åº”è¯¥å¤ªå¤š
        return len(after_content) < 100
    
    def _check_no_self_reference(self, sample: str, message_id: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦è‡ªå¼•ç”¨"""
        # è¿™é‡Œéœ€è¦æŸ¥è¯¢æ•°æ®åº“ï¼Œæ£€æŸ¥æ ·æœ¬æ˜¯å¦æ¥è‡ªåŒä¸€æ¡æ¶ˆæ¯
        # æš‚æ—¶è¿”å›Trueï¼Œå®é™…å®ç°æ—¶éœ€è¦æŸ¥è¯¢æ•°æ®åº“
        return True


class PatternLearner:
    """
    æ¨¡å¼å­¦ä¹ å™¨ - å­¦ä¹ æ¨å¹¿å†…å®¹çš„æ¨¡å¼ï¼Œè€Œä¸æ˜¯è®°å¿†æ–‡æœ¬
    """
    
    def __init__(self, storage_path: str = "data/learned_patterns.json"):
        self.storage_path = Path(storage_path)
        self.patterns: List[Pattern] = []
        self.feature_extractor = FeatureExtractor()
        self.load_patterns()
    
    def learn_from_sample(self, sample: str, confidence: float = 0.5) -> Optional[str]:
        """
        ä»æ ·æœ¬ä¸­å­¦ä¹ æ¨¡å¼
        
        Args:
            sample: è®­ç»ƒæ ·æœ¬
            confidence: åˆå§‹ç½®ä¿¡åº¦
            
        Returns:
            æ¨¡å¼ID
        """
        # æå–ç‰¹å¾
        features = self.feature_extractor.extract_features(sample)
        structure = self.feature_extractor.extract_structure(sample)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼æ¨¡å¼
        if self._is_duplicate_pattern(structure, features):
            logger.info("æ¨¡å¼å·²å­˜åœ¨ï¼Œè·³è¿‡å­¦ä¹ ")
            return None
        
        # åˆ›å»ºæ–°æ¨¡å¼
        pattern = Pattern(
            id=self._generate_pattern_id(structure),
            structure=structure,
            features=features,
            confidence=confidence,
            created_at=datetime.now().isoformat(),
            usage_count=0,
            success_rate=0.0
        )
        
        self.patterns.append(pattern)
        self.save_patterns()
        
        logger.info(f"å­¦ä¹ äº†æ–°æ¨¡å¼: {pattern.id}")
        return pattern.id
    
    def match_pattern(self, text: str, position_ratio: float = 1.0) -> Tuple[Optional[Pattern], float]:
        """
        åŒ¹é…æ–‡æœ¬ä¸å·²å­¦ä¹ çš„æ¨¡å¼
        
        Args:
            text: å¾…åŒ¹é…æ–‡æœ¬
            position_ratio: æ–‡æœ¬åœ¨æ¶ˆæ¯ä¸­çš„ä½ç½®
            
        Returns:
            (æœ€ä½³åŒ¹é…æ¨¡å¼, åŒ¹é…å¾—åˆ†)
        """
        if not text or not self.patterns:
            return None, 0.0
        
        # æå–æ–‡æœ¬ç‰¹å¾
        text_features = self.feature_extractor.extract_features(text, position_ratio)
        text_structure = self.feature_extractor.extract_structure(text)
        
        best_pattern = None
        best_score = 0.0
        
        for pattern in self.patterns:
            # è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦
            structure_score = self._calculate_structure_similarity(text_structure, pattern.structure)
            
            # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
            feature_score = self._calculate_feature_similarity(text_features, pattern.features)
            
            # ç»¼åˆå¾—åˆ†
            total_score = (structure_score * 0.4 + feature_score * 0.6) * pattern.confidence
            
            if total_score > best_score:
                best_score = total_score
                best_pattern = pattern
        
        return best_pattern, best_score
    
    def update_pattern_performance(self, pattern_id: str, was_correct: bool):
        """æ›´æ–°æ¨¡å¼çš„æ€§èƒ½æŒ‡æ ‡"""
        for pattern in self.patterns:
            if pattern.id == pattern_id:
                pattern.usage_count += 1
                pattern.last_used = datetime.now().isoformat()
                
                # æ›´æ–°æˆåŠŸç‡
                if was_correct:
                    pattern.success_rate = (
                        (pattern.success_rate * (pattern.usage_count - 1) + 1) /
                        pattern.usage_count
                    )
                else:
                    pattern.success_rate = (
                        (pattern.success_rate * (pattern.usage_count - 1)) /
                        pattern.usage_count
                    )
                
                # è°ƒæ•´ç½®ä¿¡åº¦
                if pattern.usage_count >= 10:
                    pattern.confidence = min(1.0, pattern.success_rate * 1.2)
                
                self.save_patterns()
                break
    
    def _is_duplicate_pattern(self, structure: List[str], features: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‡å¤æ¨¡å¼"""
        for pattern in self.patterns:
            # ç»“æ„å®Œå…¨ç›¸åŒ
            if pattern.structure == structure:
                # ç‰¹å¾ç›¸ä¼¼åº¦è¶…è¿‡90%
                similarity = self._calculate_feature_similarity(features, pattern.features)
                if similarity > 0.9:
                    return True
        return False
    
    def _calculate_structure_similarity(self, struct1: List[str], struct2: List[str]) -> float:
        """è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦"""
        if not struct1 or not struct2:
            return 0.0
        
        # ä½¿ç”¨æœ€é•¿å…¬å…±å­åºåˆ—ç®—æ³•
        m, n = len(struct1), len(struct2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if struct1[i-1] == struct2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        lcs_length = dp[m][n]
        return lcs_length / max(m, n)
    
    def _calculate_feature_similarity(self, feat1: Dict, feat2: Dict) -> float:
        """è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦"""
        if not feat1 or not feat2:
            return 0.0
        
        # è·å–å…±åŒç‰¹å¾
        common_keys = set(feat1.keys()) & set(feat2.keys())
        if not common_keys:
            return 0.0
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = sum(feat1[k] * feat2[k] for k in common_keys)
        norm1 = sum(feat1[k]**2 for k in common_keys) ** 0.5
        norm2 = sum(feat2[k]**2 for k in common_keys) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _generate_pattern_id(self, structure: List[str]) -> str:
        """ç”Ÿæˆæ¨¡å¼ID"""
        structure_str = '-'.join(structure[:5])  # ä½¿ç”¨å‰5ä¸ªç»“æ„å…ƒç´ 
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        hash_suffix = hashlib.md5(str(structure).encode()).hexdigest()[:6]
        return f"pattern_{timestamp}_{hash_suffix}"
    
    def save_patterns(self):
        """ä¿å­˜æ¨¡å¼åˆ°æ–‡ä»¶"""
        try:
            self.storage_path.parent.mkdir(parents=True, exist_ok=True)
            
            patterns_data = [asdict(p) for p in self.patterns]
            
            with open(self.storage_path, 'w', encoding='utf-8') as f:
                json.dump(patterns_data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ä¿å­˜äº† {len(self.patterns)} ä¸ªæ¨¡å¼")
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å¼å¤±è´¥: {e}")
    
    def load_patterns(self):
        """ä»æ–‡ä»¶åŠ è½½æ¨¡å¼"""
        try:
            if self.storage_path.exists():
                with open(self.storage_path, 'r', encoding='utf-8') as f:
                    patterns_data = json.load(f)
                
                self.patterns = [Pattern(**p) for p in patterns_data]
                logger.info(f"åŠ è½½äº† {len(self.patterns)} ä¸ªæ¨¡å¼")
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å¼å¤±è´¥: {e}")
            self.patterns = []


class IntelligentFilterEngine:
    """
    æ™ºèƒ½è¿‡æ»¤å¼•æ“ - ä½¿ç”¨å­¦ä¹ åˆ°çš„æ¨¡å¼è¿›è¡Œè¿‡æ»¤
    """
    
    def __init__(self):
        self.pattern_learner = PatternLearner()
        self.feature_extractor = FeatureExtractor()
        self.min_confidence_threshold = 0.5  # é™ä½é˜ˆå€¼æé«˜æ•æ„Ÿåº¦
        self.max_filter_ratio = 0.45  # æœ€å¤šè¿‡æ»¤45%çš„å†…å®¹ï¼Œå…è®¸æ›´å¤§å°¾éƒ¨
    
    def filter_message(self, message: str) -> Tuple[str, bool, Optional[str]]:
        """
        æ™ºèƒ½è¿‡æ»¤æ¶ˆæ¯
        
        Args:
            message: åŸå§‹æ¶ˆæ¯
            
        Returns:
            (è¿‡æ»¤åå†…å®¹, æ˜¯å¦è¿‡æ»¤äº†å†…å®¹, è¢«è¿‡æ»¤çš„éƒ¨åˆ†)
        """
        if not message:
            return message, False, None
        
        lines = message.split('\n')
        best_split_point = -1
        best_score = 0.0
        best_tail = None
        exact_match_found = False
        
        # ä»åå‘å‰æ‰«æï¼Œå¯»æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
        for i in range(len(lines) - 1, max(0, len(lines) - 20), -1):
            # è·å–å€™é€‰å°¾éƒ¨
            tail_candidate = '\n'.join(lines[i:])
            
            # 0. æ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨åŒ¹é…çš„è®­ç»ƒæ ·æœ¬ï¼ˆç›´æ¥è¿‡æ»¤ï¼Œä¸å—é˜ˆå€¼é™åˆ¶ï¼‰
            if self._has_exact_match(tail_candidate):
                logger.info(f"æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„è®­ç»ƒæ ·æœ¬ï¼Œç›´æ¥è¿‡æ»¤")
                if self._is_safe_to_filter(message, i, tail_candidate):
                    best_split_point = i
                    best_tail = tail_candidate
                    exact_match_found = True
                    best_score = 1.0  # å®Œå…¨åŒ¹é…ç»™äºˆæœ€é«˜åˆ†æ•°
                    break  # æ‰¾åˆ°å®Œå…¨åŒ¹é…å°±åœæ­¢æ‰«æ
            
            # è®¡ç®—ä½ç½®æ¯”ä¾‹
            position_ratio = i / len(lines)
            
            # åŒ¹é…æ¨¡å¼ï¼ˆä»…å½“æ²¡æœ‰å®Œå…¨åŒ¹é…æ—¶æ‰ä½¿ç”¨é˜ˆå€¼ï¼‰
            pattern, score = self.pattern_learner.match_pattern(tail_candidate, position_ratio)
            
            # é«˜ç›¸ä¼¼åº¦ç›´æ¥é€šè¿‡ï¼ˆä¸å—é˜ˆå€¼é™åˆ¶ï¼‰
            if score > 0.9:  # 90%ä»¥ä¸Šç›¸ä¼¼åº¦
                if self._is_safe_to_filter(message, i, tail_candidate):
                    best_score = score
                    best_split_point = i
                    best_tail = tail_candidate
                    exact_match_found = True
                    break
            
            # éƒ¨åˆ†åŒ¹é…æƒ…å†µä½¿ç”¨é˜ˆå€¼åˆ¤æ–­
            if score > self.min_confidence_threshold and score > best_score:
                # å®‰å…¨æ£€æŸ¥
                if self._is_safe_to_filter(message, i, tail_candidate):
                    best_score = score
                    best_split_point = i
                    best_tail = tail_candidate
        
        # åº”ç”¨è¿‡æ»¤
        if best_split_point > 0 and best_tail:
            filtered = '\n'.join(lines[:best_split_point]).rstrip()
            
            # å®Œå…¨åŒ¹é…æˆ–é«˜ç›¸ä¼¼åº¦æƒ…å†µè·³è¿‡æ¯”ä¾‹æ£€æŸ¥
            if not exact_match_found:
                # æœ€ç»ˆå®‰å…¨æ£€æŸ¥ï¼ˆä»…å¯¹éƒ¨åˆ†åŒ¹é…æƒ…å†µï¼‰
                filter_ratio = 1 - len(filtered) / len(message)
                if filter_ratio > self.max_filter_ratio:
                    logger.warning(f"è¿‡æ»¤æ¯”ä¾‹è¿‡å¤§ ({filter_ratio:.1%})ï¼Œå–æ¶ˆè¿‡æ»¤")
                    return message, False, None
            
            logger.info(f"æˆåŠŸè¿‡æ»¤å°¾éƒ¨ï¼Œç½®ä¿¡åº¦: {best_score:.2f}ï¼Œå®Œå…¨åŒ¹é…: {exact_match_found}")
            return filtered, True, best_tail
        
        return message, False, None
    
    def _has_exact_match(self, text: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨åŒ¹é…çš„è®­ç»ƒæ ·æœ¬
        
        Args:
            text: å¾…æ£€æŸ¥çš„æ–‡æœ¬
            
        Returns:
            æ˜¯å¦æœ‰å®Œå…¨åŒ¹é…
        """
        if not text:
            return False
            
        text_stripped = text.strip()
        
        # æ£€æŸ¥æ‰€æœ‰å·²å­¦ä¹ çš„æ¨¡å¼ä¸­æ˜¯å¦æœ‰åŸå§‹è®­ç»ƒæ•°æ®
        # è¿™é‡Œéœ€è¦è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®ï¼Œæš‚æ—¶ä½¿ç”¨pattern learnerçš„æ–¹å¼
        try:
            from app.core.training_config import TrainingDataConfig
            import json
            
            # æ£€æŸ¥å°¾éƒ¨è¿‡æ»¤æ ·æœ¬
            tail_file = TrainingDataConfig.TAIL_FILTER_SAMPLES_FILE
            if tail_file.exists():
                with open(tail_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    samples = data.get('samples', data) if isinstance(data, dict) else data
                    
                    for sample in samples:
                        if isinstance(sample, dict) and sample.get('tail_part'):
                            sample_text = sample['tail_part'].strip()
                        elif isinstance(sample, str):
                            sample_text = sample.strip()
                        else:
                            continue
                            
                        # å®Œå…¨åŒ¹é…æˆ–é«˜åº¦é‡åˆ
                        if text_stripped == sample_text:
                            return True
                        elif sample_text in text_stripped or text_stripped in sample_text:
                            # è®¡ç®—é‡åˆåº¦
                            shorter = min(len(text_stripped), len(sample_text))
                            longer = max(len(text_stripped), len(sample_text))
                            if shorter >= longer * 0.8:  # 80%ä»¥ä¸Šé‡åˆ
                                return True
                                
        except Exception as e:
            logger.warning(f"æ£€æŸ¥å®Œå…¨åŒ¹é…æ—¶å‡ºé”™: {e}")
        
        return False
    
    def _is_safe_to_filter(self, message: str, split_point: int, tail: str) -> bool:
        """
        å®‰å…¨æ£€æŸ¥ï¼Œç¡®ä¿ä¸ä¼šç ´åæ­£æ–‡
        """
        lines = message.split('\n')
        
        # æ£€æŸ¥å‰©ä½™å†…å®¹é•¿åº¦
        remaining = '\n'.join(lines[:split_point])
        if len(remaining) < 50:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦ä¿¡æ¯
        important_patterns = [
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',  # æ—¥æœŸ
            r'\d+[äº¿ä¸‡]',  # é‡‘é¢
            r'[^ï¼Œã€‚ï¼ï¼Ÿ]{50,}',  # é•¿å¥å­ï¼ˆå¯èƒ½æ˜¯æ­£æ–‡ï¼‰
        ]
        
        for pattern in important_patterns:
            if re.search(pattern, tail):
                # å°¾éƒ¨åŒ…å«é‡è¦ä¿¡æ¯ï¼Œå¯èƒ½ä¸å®‰å…¨
                return False
        
        return True


class IntelligentLearningSystem:
    """
    æ™ºèƒ½å­¦ä¹ ç³»ç»Ÿä¸»ç±» - æ•´åˆæ‰€æœ‰ç»„ä»¶
    """
    
    def __init__(self):
        self.feature_extractor = FeatureExtractor()
        self.validator = SampleValidator()
        self.pattern_learner = PatternLearner()
        self.filter_engine = IntelligentFilterEngine()
        
        # å­¦ä¹ ç»Ÿè®¡
        self.stats = {
            'samples_processed': 0,
            'samples_accepted': 0,
            'samples_rejected': 0,
            'patterns_learned': 0
        }
    
    def add_training_sample(self, tail_part: str = None, original_content: str = None, message_id: int = None, sample: str = None, original_message: str = None) -> Dict[str, Any]:
        """
        æ·»åŠ è®­ç»ƒæ ·æœ¬ï¼ˆæ”¯æŒæ–°æ—§æ¥å£ï¼‰
        
        Args:
            tail_part: å°¾éƒ¨å†…å®¹ï¼ˆæ–°æ¥å£ï¼‰
            original_content: åŸå§‹æ¶ˆæ¯å†…å®¹ï¼ˆå¯é€‰ï¼‰
            message_id: æ¶ˆæ¯ID
            sample: è®­ç»ƒæ ·æœ¬ï¼ˆæ—§æ¥å£å…¼å®¹ï¼‰
            original_message: åŸå§‹æ¶ˆæ¯ï¼ˆæ—§æ¥å£å…¼å®¹ï¼‰
            
        Returns:
            å¤„ç†ç»“æœ
        """
        result = {
            'success': False,
            'message': '',
            'pattern_id': None,
            'validation': None
        }
        
        # å…¼å®¹æ—§æ¥å£
        if sample is not None:
            tail_part = sample
        if original_message is not None:
            original_content = original_message
        
        if not tail_part:
            result['message'] = "å°¾éƒ¨å†…å®¹ä¸èƒ½ä¸ºç©º"
            return result
        
        # éªŒè¯æ ·æœ¬ï¼ˆåŸå§‹å†…å®¹å¯ä»¥ä¸ºç©ºï¼‰
        validation = self.validator.validate(tail_part, original_content, message_id)
        result['validation'] = validation
        
        self.stats['samples_processed'] += 1
        
        if not validation['is_valid']:
            self.stats['samples_rejected'] += 1
            result['message'] = f"æ ·æœ¬éªŒè¯å¤±è´¥: {', '.join(validation['errors'])}"
            logger.warning(result['message'])
            return result
        
        # å­¦ä¹ æ¨¡å¼
        pattern_id = self.pattern_learner.learn_from_sample(tail_part, validation['confidence'])
        
        if pattern_id:
            self.stats['samples_accepted'] += 1
            self.stats['patterns_learned'] += 1
            result['success'] = True
            result['pattern_id'] = pattern_id
            result['message'] = f"æˆåŠŸå­¦ä¹ æ–°æ¨¡å¼: {pattern_id}"
            logger.info(result['message'])
        else:
            result['message'] = "æ¨¡å¼å·²å­˜åœ¨ï¼Œæœªå­¦ä¹ æ–°å†…å®¹"
            
        return result
    
    def filter_message(self, message: str) -> Tuple[str, bool, Optional[str]]:
        """
        ä½¿ç”¨æ™ºèƒ½è¿‡æ»¤å¼•æ“è¿‡æ»¤æ¶ˆæ¯
        """
        return self.filter_engine.filter_message(message)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        return {
            'learning_stats': self.stats,
            'pattern_count': len(self.pattern_learner.patterns),
            'patterns': [
                {
                    'id': p.id,
                    'confidence': p.confidence,
                    'usage_count': p.usage_count,
                    'success_rate': p.success_rate
                }
                for p in self.pattern_learner.patterns[:10]  # åªè¿”å›å‰10ä¸ª
            ]
        }


# åˆ›å»ºå…¨å±€å®ä¾‹
intelligent_learning_system = IntelligentLearningSystem()