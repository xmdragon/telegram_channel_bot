"""
å¹¿å‘Šè®­ç»ƒç›¸å…³API
"""
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List, Dict, Optional
from datetime import datetime
import json
import hashlib
from pathlib import Path
import logging

from app.core.database import get_db
from app.services.adaptive_learning import adaptive_learning
from app.core.training_config import TrainingDataConfig

logger = logging.getLogger(__name__)
router = APIRouter()

# æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨é›†ä¸­é…ç½®ï¼‰
SEPARATOR_PATTERNS_FILE = TrainingDataConfig.SEPARATOR_PATTERNS_FILE
TAIL_AD_SAMPLES_FILE = TrainingDataConfig.TAIL_AD_SAMPLES_FILE

# ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
TrainingDataConfig.ensure_directories()


@router.get("/separator-patterns")
async def get_separator_patterns():
    """è·å–åˆ†éš”ç¬¦æ¨¡å¼åˆ—è¡¨"""
    try:
        if SEPARATOR_PATTERNS_FILE.exists():
            with open(SEPARATOR_PATTERNS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return {"patterns": data.get("patterns", [])}
        else:
            # è¿”å›é»˜è®¤æ¨¡å¼
            default_patterns = [
                {"regex": "â”{10,}", "description": "æ¨ªçº¿åˆ†éš”ç¬¦ï¼ˆ10ä¸ªä»¥ä¸Šï¼‰"},
                {"regex": "â•{10,}", "description": "åŒçº¿åˆ†éš”ç¬¦"},
                {"regex": "â”€{10,}", "description": "ç»†çº¿åˆ†éš”ç¬¦"},
                {"regex": "â–¬{10,}", "description": "ç²—çº¿åˆ†éš”ç¬¦"},
                {"regex": "-{20,}", "description": "çŸ­æ¨ªçº¿ï¼ˆ20ä¸ªä»¥ä¸Šï¼‰"},
                {"regex": "={20,}", "description": "ç­‰å·çº¿"},
                {"regex": "\\*{20,}", "description": "æ˜Ÿå·çº¿"}
            ]
            return {"patterns": default_patterns}
    except Exception as e:
        logger.error(f"è·å–åˆ†éš”ç¬¦æ¨¡å¼å¤±è´¥: {e}")
        return {"patterns": []}


@router.post("/separator-patterns")
async def save_separator_patterns(request: dict):
    """ä¿å­˜åˆ†éš”ç¬¦æ¨¡å¼"""
    try:
        patterns = request.get("patterns", [])
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(SEPARATOR_PATTERNS_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                "patterns": patterns,
                "updated_at": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        
        # æ›´æ–°smart_tail_filterçš„æ¨¡å¼
        from app.services.smart_tail_filter import smart_tail_filter
        smart_tail_filter.separator_patterns = [p['regex'] for p in patterns if p.get('regex')]
        
        logger.info(f"ä¿å­˜äº† {len(patterns)} ä¸ªåˆ†éš”ç¬¦æ¨¡å¼")
        return {"success": True, "message": "åˆ†éš”ç¬¦æ¨¡å¼å·²ä¿å­˜"}
    except Exception as e:
        logger.error(f"ä¿å­˜åˆ†éš”ç¬¦æ¨¡å¼å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


@router.get("/tail-ad-samples")
async def get_tail_ad_samples():
    """è·å–å°¾éƒ¨å¹¿å‘Šè®­ç»ƒæ ·æœ¬"""
    try:
        if TAIL_AD_SAMPLES_FILE.exists():
            with open(TAIL_AD_SAMPLES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                samples = data.get("samples", [])
                
                # æ·»åŠ IDå¦‚æœæ²¡æœ‰
                for i, sample in enumerate(samples):
                    if 'id' not in sample:
                        sample['id'] = i + 1
                
                return {"samples": samples}
        else:
            return {"samples": []}
    except Exception as e:
        logger.error(f"è·å–å°¾éƒ¨å¹¿å‘Šæ ·æœ¬å¤±è´¥: {e}")
        return {"samples": []}


@router.post("/tail-ad-samples")
async def add_tail_ad_sample(request: dict):
    """æ·»åŠ å°¾éƒ¨å¹¿å‘Šè®­ç»ƒæ ·æœ¬"""
    logger.info(f"ğŸ“¥ æ”¶åˆ°å°¾éƒ¨æ•°æ®æäº¤è¯·æ±‚ - è¯·æ±‚æ•°æ®é”®: {list(request.keys()) if request else 'None'}")
    try:
        # æå–å‚æ•°
        description = request.get("description", "")
        content = request.get("content", "")
        separator = request.get("separator", "")
        normal_part = request.get("normalPart", "")
        ad_part = request.get("adPart", "")
        
        logger.debug(f"æå–çš„å‚æ•° - å†…å®¹é•¿åº¦: {len(content) if content else 0}, åˆ†éš”ç¬¦: '{separator[:20]}...', æè¿°: '{description[:30]}...'")
        logger.debug(f"æ­£å¸¸éƒ¨åˆ†é•¿åº¦: {len(normal_part) if normal_part else 0}, å¹¿å‘Šéƒ¨åˆ†é•¿åº¦: {len(ad_part) if ad_part else 0}")
        
        if not content or not separator:
            logger.warning("âŒ å‚æ•°éªŒè¯å¤±è´¥ - å†…å®¹æˆ–åˆ†éš”ç¬¦ä¸ºç©º")
            return {"success": False, "error": "å†…å®¹å’Œåˆ†éš”ç¬¦ä¸èƒ½ä¸ºç©º"}
        
        # åŠ è½½ç°æœ‰æ ·æœ¬
        samples = []
        if TAIL_AD_SAMPLES_FILE.exists():
            logger.debug(f"åŠ è½½ç°æœ‰æ ·æœ¬æ–‡ä»¶: {TAIL_AD_SAMPLES_FILE}")
            with open(TAIL_AD_SAMPLES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                samples = data.get("samples", [])
                logger.debug(f"å½“å‰æ ·æœ¬æ•°é‡: {len(samples)}")
        else:
            logger.debug("æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶")
        
        # ç”ŸæˆID
        new_id = max([s.get('id', 0) for s in samples], default=0) + 1
        
        # åˆ›å»ºæ–°æ ·æœ¬
        new_sample = {
            "id": new_id,
            "description": description,
            "content": content,
            "separator": separator,
            "normal_part": normal_part,
            "ad_part": ad_part,
            "content_hash": hashlib.md5(content.encode()).hexdigest(),
            "created_at": datetime.now().isoformat()
        }
        
        # æ£€æŸ¥é‡å¤
        for sample in samples:
            if sample.get("content_hash") == new_sample["content_hash"]:
                logger.warning(f"âŒ æ£€æµ‹åˆ°é‡å¤æ ·æœ¬ - hash: {new_sample['content_hash'][:8]}...")
                return {"success": False, "error": "æ ·æœ¬å·²å­˜åœ¨"}
        
        # æ·»åŠ æ ·æœ¬
        samples.append(new_sample)
        logger.debug(f"â• æ·»åŠ æ–°æ ·æœ¬ - ID: {new_id}, æ€»æ•°é‡: {len(samples)}")
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        logger.debug(f"ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶: {TAIL_AD_SAMPLES_FILE}")
        with open(TAIL_AD_SAMPLES_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                "samples": samples,
                "updated_at": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        logger.debug("âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ")
        
        # åŒæ—¶æ·»åŠ åˆ°å¹¿å‘Šæ ·æœ¬åº“ç”¨äºAIå­¦ä¹ 
        if ad_part:
            logger.debug(f"æ·»åŠ å¹¿å‘Šéƒ¨åˆ†åˆ°AIå­¦ä¹ åº“ - é•¿åº¦: {len(ad_part)}")
            await adaptive_learning._add_ad_sample(ad_part)
        else:
            logger.warning("å¹¿å‘Šéƒ¨åˆ†ä¸ºç©ºï¼Œè·³è¿‡AIå­¦ä¹ åº“æ·»åŠ ")
        
        logger.info(f"âœ… æˆåŠŸæ·»åŠ æ–°çš„å°¾éƒ¨å¹¿å‘Šæ ·æœ¬: ID={new_id}, å†…å®¹é•¿åº¦={len(content)}, å¹¿å‘Šé•¿åº¦={len(ad_part)}")
        return {"success": True, "message": "æ ·æœ¬å·²æ·»åŠ ", "id": new_id}
        
    except Exception as e:
        logger.error(f"âŒ æ·»åŠ å°¾éƒ¨å¹¿å‘Šæ ·æœ¬å¤±è´¥: {e}", exc_info=True)
        return {"success": False, "error": str(e)}


@router.delete("/tail-ad-samples/{sample_id}")
async def delete_tail_ad_sample(sample_id: int):
    """åˆ é™¤å°¾éƒ¨å¹¿å‘Šè®­ç»ƒæ ·æœ¬"""
    try:
        # åŠ è½½æ ·æœ¬
        if not TAIL_AD_SAMPLES_FILE.exists():
            return {"success": False, "error": "æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨"}
        
        with open(TAIL_AD_SAMPLES_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            samples = data.get("samples", [])
        
        # æŸ¥æ‰¾å¹¶åˆ é™¤
        original_count = len(samples)
        samples = [s for s in samples if s.get('id') != sample_id]
        
        if len(samples) == original_count:
            return {"success": False, "error": "æ ·æœ¬ä¸å­˜åœ¨"}
        
        # ä¿å­˜
        with open(TAIL_AD_SAMPLES_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                "samples": samples,
                "updated_at": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"åˆ é™¤å°¾éƒ¨å¹¿å‘Šæ ·æœ¬: {sample_id}")
        return {"success": True, "message": "æ ·æœ¬å·²åˆ é™¤"}
        
    except Exception as e:
        logger.error(f"åˆ é™¤å°¾éƒ¨å¹¿å‘Šæ ·æœ¬å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


@router.get("/learning-stats")
async def get_learning_stats():
    """è·å–å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = adaptive_learning.get_learning_stats()
        return {"success": True, "stats": stats}
    except Exception as e:
        logger.error(f"è·å–å­¦ä¹ ç»Ÿè®¡å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


@router.post("/feedback")
async def record_feedback(request: dict):
    """è®°å½•ç”¨æˆ·åé¦ˆç”¨äºå­¦ä¹ """
    try:
        message_id = request.get("message_id")
        action = request.get("action")  # 'approved', 'rejected', 'edited'
        reviewer = request.get("reviewer", "Webç”¨æˆ·")
        
        if not message_id or not action:
            return {"success": False, "error": "å‚æ•°ä¸å®Œæ•´"}
        
        # è®°å½•åé¦ˆ
        await adaptive_learning.learn_from_user_action(message_id, action, reviewer)
        
        return {"success": True, "message": "åé¦ˆå·²è®°å½•"}
        
    except Exception as e:
        logger.error(f"è®°å½•åé¦ˆå¤±è´¥: {e}")
        return {"success": False, "error": str(e)}