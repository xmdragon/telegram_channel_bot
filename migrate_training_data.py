#!/usr/bin/env python3
"""
è®­ç»ƒæ•°æ®è¿ç§»è„šæœ¬
å°†æ—§çš„è®­ç»ƒæ ·æœ¬è¿ç§»åˆ°æ–°çš„æ™ºèƒ½å­¦ä¹ ç³»ç»Ÿ
"""
import json
import logging
from pathlib import Path
from datetime import datetime
import asyncio
from sqlalchemy import select

from app.services.intelligent_learning_system import intelligent_learning_system
from app.core.database import AsyncSessionLocal, Message

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TrainingDataMigrator:
    """è®­ç»ƒæ•°æ®è¿ç§»å™¨"""
    
    def __init__(self):
        self.old_samples_file = Path("data/tail_filter_samples.json")
        self.backup_dir = Path("data/migration_backup")
        self.backup_dir.mkdir(exist_ok=True)
        
        self.stats = {
            'total_samples': 0,
            'valid_samples': 0,
            'invalid_samples': 0,
            'patterns_learned': 0,
            'errors': []
        }
    
    async def migrate(self):
        """æ‰§è¡Œè¿ç§»"""
        logger.info("å¼€å§‹è®­ç»ƒæ•°æ®è¿ç§»...")
        
        # 1. å¤‡ä»½ç°æœ‰æ•°æ®
        self._backup_existing_data()
        
        # 2. åŠ è½½æ—§æ ·æœ¬
        old_samples = self._load_old_samples()
        if not old_samples:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æ—§çš„è®­ç»ƒæ ·æœ¬")
            return
        
        self.stats['total_samples'] = len(old_samples)
        logger.info(f"æ‰¾åˆ° {len(old_samples)} ä¸ªæ—§æ ·æœ¬")
        
        # 3. éªŒè¯å¹¶è¿ç§»æ¯ä¸ªæ ·æœ¬
        for i, sample in enumerate(old_samples, 1):
            logger.info(f"å¤„ç†æ ·æœ¬ {i}/{len(old_samples)}")
            await self._process_sample(sample)
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        self._generate_report()
    
    def _backup_existing_data(self):
        """å¤‡ä»½ç°æœ‰æ•°æ®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å¤‡ä»½æ—§çš„è®­ç»ƒæ ·æœ¬æ–‡ä»¶
        if self.old_samples_file.exists():
            backup_path = self.backup_dir / f"tail_filter_samples_{timestamp}.json"
            with open(self.old_samples_file, 'r') as src:
                with open(backup_path, 'w') as dst:
                    dst.write(src.read())
            logger.info(f"å¤‡ä»½æ—§æ ·æœ¬æ–‡ä»¶åˆ°: {backup_path}")
        
        # å¤‡ä»½å­¦ä¹ çš„æ¨¡å¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        patterns_file = Path("data/learned_patterns.json")
        if patterns_file.exists():
            backup_path = self.backup_dir / f"learned_patterns_{timestamp}.json"
            with open(patterns_file, 'r') as src:
                with open(backup_path, 'w') as dst:
                    dst.write(src.read())
            logger.info(f"å¤‡ä»½æ¨¡å¼æ–‡ä»¶åˆ°: {backup_path}")
    
    def _load_old_samples(self):
        """åŠ è½½æ—§çš„è®­ç»ƒæ ·æœ¬"""
        if not self.old_samples_file.exists():
            return []
        
        try:
            with open(self.old_samples_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('samples', [])
        except Exception as e:
            logger.error(f"åŠ è½½æ—§æ ·æœ¬å¤±è´¥: {e}")
            return []
    
    async def _process_sample(self, sample_data):
        """å¤„ç†å•ä¸ªæ ·æœ¬"""
        try:
            tail_part = sample_data.get('tail_part', '')
            message_id = sample_data.get('message_id')
            
            if not tail_part:
                logger.warning(f"æ ·æœ¬ {sample_data.get('id')} æ²¡æœ‰å°¾éƒ¨å†…å®¹ï¼Œè·³è¿‡")
                self.stats['invalid_samples'] += 1
                return
            
            # å¦‚æœæœ‰æ¶ˆæ¯IDï¼Œå°è¯•è·å–åŸå§‹æ¶ˆæ¯
            original_message = await self._get_original_message(message_id)
            
            if not original_message:
                # å¦‚æœæ²¡æœ‰åŸå§‹æ¶ˆæ¯ï¼Œæ„é€ ä¸€ä¸ªæ¨¡æ‹Ÿçš„
                original_message = self._construct_mock_message(tail_part)
            
            # éªŒè¯æ ·æœ¬è´¨é‡
            validation_result = self._validate_sample(tail_part, original_message, message_id)
            
            if validation_result['is_valid']:
                # ä½¿ç”¨æ™ºèƒ½å­¦ä¹ ç³»ç»Ÿå­¦ä¹ 
                result = intelligent_learning_system.add_training_sample(
                    tail_part, 
                    original_message,
                    message_id
                )
                
                if result['success']:
                    self.stats['valid_samples'] += 1
                    self.stats['patterns_learned'] += 1
                    logger.info(f"æˆåŠŸè¿ç§»æ ·æœ¬: {result['message']}")
                else:
                    self.stats['invalid_samples'] += 1
                    logger.warning(f"æ ·æœ¬éªŒè¯é€šè¿‡ä½†å­¦ä¹ å¤±è´¥: {result['message']}")
            else:
                self.stats['invalid_samples'] += 1
                reasons = validation_result.get('errors', ['æœªçŸ¥åŸå› '])
                logger.warning(f"æ ·æœ¬æ— æ•ˆ: {', '.join(reasons)}")
                self.stats['errors'].append({
                    'sample_id': sample_data.get('id'),
                    'reasons': reasons
                })
        
        except Exception as e:
            logger.error(f"å¤„ç†æ ·æœ¬æ—¶å‡ºé”™: {e}")
            self.stats['errors'].append({
                'sample_id': sample_data.get('id'),
                'error': str(e)
            })
    
    async def _get_original_message(self, message_id):
        """è·å–åŸå§‹æ¶ˆæ¯"""
        if not message_id:
            return None
        
        try:
            async with AsyncSessionLocal() as db:
                result = await db.execute(
                    select(Message).where(Message.id == message_id)
                )
                message = result.scalar_one_or_none()
                if message:
                    return message.content
        except Exception as e:
            logger.debug(f"è·å–æ¶ˆæ¯ {message_id} å¤±è´¥: {e}")
        
        return None
    
    def _construct_mock_message(self, tail_part):
        """æ„é€ æ¨¡æ‹Ÿæ¶ˆæ¯"""
        # æ„é€ ä¸€ä¸ªåŒ…å«å°¾éƒ¨çš„æ¨¡æ‹Ÿæ¶ˆæ¯
        mock_content = """
è¿™æ˜¯ä¸€æ¡æ¨¡æ‹Ÿçš„æ–°é—»æ¶ˆæ¯å†…å®¹ã€‚
åŒ…å«ä¸€äº›æ­£æ–‡å†…å®¹ç”¨äºéªŒè¯ã€‚
æ–°é—»äº‹ä»¶æè¿°ç­‰ç­‰ã€‚

""" + tail_part
        return mock_content
    
    def _validate_sample(self, tail_part, original_message, message_id):
        """éªŒè¯æ ·æœ¬è´¨é‡"""
        # ä½¿ç”¨æ™ºèƒ½å­¦ä¹ ç³»ç»Ÿçš„éªŒè¯å™¨
        from app.services.intelligent_learning_system import SampleValidator
        validator = SampleValidator()
        
        # ç‰¹æ®Šæ£€æŸ¥ï¼šé˜²æ­¢è‡ªå¼•ç”¨
        if message_id and original_message:
            # æ£€æŸ¥æ ·æœ¬æ˜¯å¦åŒ…å«æ­£æ–‡å†…å®¹
            # å°¾éƒ¨åº”è¯¥åªåŒ…å«æ¨å¹¿å†…å®¹ï¼Œä¸åº”è¯¥åŒ…å«æ–°é—»æ­£æ–‡
            news_keywords = ['é©¬äº‘', 'é˜¿é‡Œ', 'è…¾è®¯', 'æ”¿åºœ', 'å›½å®¶', 'äº¿', 'ä¸‡']
            contains_news = sum(1 for kw in news_keywords if kw in tail_part)
            
            if contains_news > 2:
                return {
                    'is_valid': False,
                    'errors': ['æ ·æœ¬åŒ…å«è¿‡å¤šæ–°é—»å…³é”®è¯ï¼Œå¯èƒ½æ˜¯æ­£æ–‡å†…å®¹']
                }
        
        return validator.validate(tail_part, original_message, message_id)
    
    def _generate_report(self):
        """ç”Ÿæˆè¿ç§»æŠ¥å‘Š"""
        report = {
            'migration_time': datetime.now().isoformat(),
            'statistics': self.stats,
            'success_rate': self.stats['valid_samples'] / max(self.stats['total_samples'], 1),
            'system_stats': intelligent_learning_system.get_statistics()
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.backup_dir / f"migration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“Š è®­ç»ƒæ•°æ®è¿ç§»æŠ¥å‘Š")
        print("=" * 60)
        print(f"å¤„ç†æ ·æœ¬æ€»æ•°: {self.stats['total_samples']}")
        print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {self.stats['valid_samples']}")
        print(f"æ— æ•ˆæ ·æœ¬æ•°: {self.stats['invalid_samples']}")
        print(f"å­¦ä¹ çš„æ¨¡å¼æ•°: {self.stats['patterns_learned']}")
        print(f"æˆåŠŸç‡: {report['success_rate']:.1%}")
        
        if self.stats['errors']:
            print(f"\nâš ï¸ å‘ç° {len(self.stats['errors'])} ä¸ªé”™è¯¯")
            for error in self.stats['errors'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  - æ ·æœ¬ {error.get('sample_id')}: {error.get('reasons', error.get('error'))}")
        
        print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        print("=" * 60)


async def main():
    """ä¸»å‡½æ•°"""
    migrator = TrainingDataMigrator()
    await migrator.migrate()


if __name__ == "__main__":
    asyncio.run(main())