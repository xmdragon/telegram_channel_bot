#!/usr/bin/env python3
"""
è®­ç»ƒæ•°æ®æ¢å¤å·¥å…·

è¿™ä¸ªè„šæœ¬æä¾›äº†å¼ºå¤§çš„è®­ç»ƒæ•°æ®æ¢å¤åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤æŸåçš„æ–‡ä»¶
2. ä»å¤‡ä»½ä¸­æ¢å¤æ•°æ®
3. åˆå¹¶å¤šä¸ªå¤‡ä»½æ–‡ä»¶
4. æ•°æ®å®Œæ•´æ€§éªŒè¯
5. ç´§æ€¥æ¢å¤æ¨¡å¼

ä½¿ç”¨æ–¹æ³•:
  python3 recover_training_data.py --check                    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
  python3 recover_training_data.py --auto-recover            # è‡ªåŠ¨æ¢å¤
  python3 recover_training_data.py --restore backup.json     # ä»æŒ‡å®šå¤‡ä»½æ¢å¤
  python3 recover_training_data.py --merge-backups           # åˆå¹¶å¤šä¸ªå¤‡ä»½
  python3 recover_training_data.py --emergency               # ç´§æ€¥æ¢å¤æ¨¡å¼

ä½œè€…: Claude Code
ç‰ˆæœ¬: 2.0
"""

import os
import sys
import json
import argparse
import logging
import hashlib
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('recovery.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class TrainingDataRecovery:
    """è®­ç»ƒæ•°æ®æ¢å¤å·¥å…·ç±»"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.backup_dir = Path("data/backups")
        self.main_data_file = Path("data/manual_training_data.json")
        self.history_file = Path("data/training_history.json")
        self.recovery_log = []
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.data_dir.mkdir(exist_ok=True)
        self.backup_dir.mkdir(exist_ok=True)
    
    def log_operation(self, operation: str, details: str = "", success: bool = True):
        """è®°å½•æ¢å¤æ“ä½œ"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "operation": operation,
            "details": details,
            "success": success
        }
        self.recovery_log.append(log_entry)
        
        if success:
            logger.info(f"âœ… {operation}: {details}")
        else:
            logger.error(f"âŒ {operation}: {details}")
    
    def calculate_file_hash(self, file_path: Path) -> Optional[str]:
        """è®¡ç®—æ–‡ä»¶SHA256å“ˆå¸Œ"""
        if not file_path.exists():
            return None
        try:
            with open(file_path, 'rb') as f:
                return hashlib.sha256(f.read()).hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return None
    
    def verify_json_integrity(self, file_path: Path) -> Tuple[bool, str]:
        """éªŒè¯JSONæ–‡ä»¶å®Œæ•´æ€§"""
        if not file_path.exists():
            return False, "æ–‡ä»¶ä¸å­˜åœ¨"
        
        if file_path.stat().st_size == 0:
            return False, "æ–‡ä»¶ä¸ºç©º"
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # éªŒè¯åŸºæœ¬ç»“æ„
            if file_path.name.startswith('manual_training_data'):
                if not isinstance(data, dict) or 'channels' not in data:
                    return False, "è®­ç»ƒæ•°æ®ç»“æ„æ— æ•ˆ"
                
                # éªŒè¯channelsç»“æ„
                channels = data['channels']
                if not isinstance(channels, dict):
                    return False, "channelså­—æ®µæ— æ•ˆ"
                
                for channel_id, channel_data in channels.items():
                    if not isinstance(channel_data, dict) or 'samples' not in channel_data:
                        return False, f"é¢‘é“ {channel_id} æ•°æ®ç»“æ„æ— æ•ˆ"
            
            elif file_path.name.startswith('training_history'):
                if not isinstance(data, dict) or 'history' not in data or 'stats' not in data:
                    return False, "å†å²æ•°æ®ç»“æ„æ— æ•ˆ"
            
            return True, "JSONæ ¼å¼æ­£ç¡®"
            
        except json.JSONDecodeError as e:
            return False, f"JSONè§£æé”™è¯¯: {e}"
        except Exception as e:
            return False, f"éªŒè¯å¤±è´¥: {e}"
    
    def get_file_info(self, file_path: Path) -> Dict:
        """è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯"""
        info = {
            "path": str(file_path),
            "exists": file_path.exists(),
            "size": 0,
            "valid": False,
            "error": "",
            "hash": None,
            "modified": None,
            "content_summary": {}
        }
        
        if not file_path.exists():
            info["error"] = "æ–‡ä»¶ä¸å­˜åœ¨"
            return info
        
        try:
            stat = file_path.stat()
            info["size"] = stat.st_size
            info["modified"] = datetime.fromtimestamp(stat.st_mtime).isoformat()
            info["hash"] = self.calculate_file_hash(file_path)
            
            valid, error = self.verify_json_integrity(file_path)
            info["valid"] = valid
            info["error"] = error
            
            # å¦‚æœæ–‡ä»¶æœ‰æ•ˆï¼Œè·å–å†…å®¹æ‘˜è¦
            if valid:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    if 'channels' in data:
                        info["content_summary"] = {
                            "type": "training_data",
                            "channels_count": len(data["channels"]),
                            "total_samples": sum(len(c.get("samples", [])) for c in data["channels"].values()),
                            "last_updated": data.get("updated_at", "unknown")
                        }
                    elif 'history' in data:
                        info["content_summary"] = {
                            "type": "training_history", 
                            "history_count": len(data.get("history", [])),
                            "total_samples": data.get("stats", {}).get("total_samples", 0)
                        }
                except:
                    pass
            
        except Exception as e:
            info["error"] = f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}"
        
        return info
    
    def check_data_integrity(self) -> Dict:
        """æ£€æŸ¥æ‰€æœ‰æ•°æ®æ–‡ä»¶çš„å®Œæ•´æ€§"""
        logger.info("ğŸ” å¼€å§‹æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "main_files": {},
            "backup_files": [],
            "summary": {
                "healthy_files": 0,
                "corrupted_files": 0,
                "missing_files": 0,
                "total_backups": 0,
                "valid_backups": 0
            }
        }
        
        # æ£€æŸ¥ä¸»è¦æ–‡ä»¶
        for file_path in [self.main_data_file, self.history_file]:
            info = self.get_file_info(file_path)
            report["main_files"][file_path.name] = info
            
            if not info["exists"]:
                report["summary"]["missing_files"] += 1
            elif info["valid"]:
                report["summary"]["healthy_files"] += 1
            else:
                report["summary"]["corrupted_files"] += 1
        
        # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶
        backup_files = list(self.backup_dir.glob("*.json"))
        report["summary"]["total_backups"] = len(backup_files)
        
        for backup_file in sorted(backup_files, key=lambda f: f.stat().st_mtime, reverse=True):
            info = self.get_file_info(backup_file)
            report["backup_files"].append(info)
            
            if info["valid"]:
                report["summary"]["valid_backups"] += 1
        
        # è®°å½•æ£€æŸ¥ç»“æœ
        healthy = report["summary"]["healthy_files"]
        corrupted = report["summary"]["corrupted_files"]
        missing = report["summary"]["missing_files"]
        valid_backups = report["summary"]["valid_backups"]
        
        self.log_operation(
            "æ•°æ®å®Œæ•´æ€§æ£€æŸ¥",
            f"å¥åº·: {healthy}, æŸå: {corrupted}, ä¸¢å¤±: {missing}, æœ‰æ•ˆå¤‡ä»½: {valid_backups}"
        )
        
        return report
    
    def find_best_backup(self, file_type: str) -> Optional[Path]:
        """æŸ¥æ‰¾æœ€ä½³å¤‡ä»½æ–‡ä»¶"""
        pattern = f"*{file_type}*.json"
        backup_files = list(self.backup_dir.glob(pattern))
        
        if not backup_files:
            return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œä¼˜å…ˆé€‰æ‹©æœ€æ–°çš„æœ‰æ•ˆå¤‡ä»½
        for backup_file in sorted(backup_files, key=lambda f: f.stat().st_mtime, reverse=True):
            valid, _ = self.verify_json_integrity(backup_file)
            if valid:
                logger.info(f"æ‰¾åˆ°æœ€ä½³å¤‡ä»½: {backup_file}")
                return backup_file
        
        logger.warning(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„ {file_type} å¤‡ä»½æ–‡ä»¶")
        return None
    
    def create_backup(self, source_file: Path, backup_name: str) -> Optional[Path]:
        """åˆ›å»ºå¤‡ä»½æ–‡ä»¶"""
        if not source_file.exists() or source_file.stat().st_size == 0:
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        backup_file = self.backup_dir / f"{backup_name}_{timestamp}.json"
        
        try:
            shutil.copy2(source_file, backup_file)
            self.log_operation("åˆ›å»ºå¤‡ä»½", f"{source_file} -> {backup_file}")
            return backup_file
        except Exception as e:
            self.log_operation("åˆ›å»ºå¤‡ä»½", f"å¤±è´¥: {e}", False)
            return None
    
    def restore_from_backup(self, target_file: Path, backup_file: Path) -> bool:
        """ä»å¤‡ä»½æ¢å¤æ–‡ä»¶"""
        try:
            # éªŒè¯å¤‡ä»½æ–‡ä»¶
            valid, error = self.verify_json_integrity(backup_file)
            if not valid:
                self.log_operation("æ¢å¤éªŒè¯", f"å¤‡ä»½æ— æ•ˆ: {error}", False)
                return False
            
            # å¤‡ä»½å½“å‰æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ä¸”æœ‰æ•ˆï¼‰
            if target_file.exists() and target_file.stat().st_size > 0:
                backup_current = self.create_backup(target_file, f"{target_file.stem}_before_restore")
                if backup_current:
                    self.log_operation("æ¢å¤å‰å¤‡ä»½", f"å·²å¤‡ä»½å½“å‰æ–‡ä»¶: {backup_current}")
            
            # æ‰§è¡Œæ¢å¤
            shutil.copy2(backup_file, target_file)
            
            # éªŒè¯æ¢å¤ç»“æœ
            valid, error = self.verify_json_integrity(target_file)
            if valid:
                self.log_operation("æ–‡ä»¶æ¢å¤", f"{backup_file} -> {target_file}")
                return True
            else:
                self.log_operation("æ¢å¤éªŒè¯", f"æ¢å¤åéªŒè¯å¤±è´¥: {error}", False)
                return False
                
        except Exception as e:
            self.log_operation("æ–‡ä»¶æ¢å¤", f"å¤±è´¥: {e}", False)
            return False
    
    def auto_recover(self) -> bool:
        """è‡ªåŠ¨æ¢å¤æŸåçš„æ–‡ä»¶"""
        logger.info("ğŸš‘ å¼€å§‹è‡ªåŠ¨æ¢å¤...")
        
        integrity_report = self.check_data_integrity()
        recovery_needed = False
        recovery_success = True
        
        # æ¢å¤ä¸»æ•°æ®æ–‡ä»¶
        main_file_info = integrity_report["main_files"].get("manual_training_data.json", {})
        if not main_file_info.get("valid", False):
            recovery_needed = True
            best_backup = self.find_best_backup("manual_training_data")
            if best_backup:
                if not self.restore_from_backup(self.main_data_file, best_backup):
                    recovery_success = False
            else:
                self.log_operation("è‡ªåŠ¨æ¢å¤", "æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®å¤‡ä»½", False)
                recovery_success = False
        
        # æ¢å¤å†å²æ–‡ä»¶
        history_file_info = integrity_report["main_files"].get("training_history.json", {})
        if not history_file_info.get("valid", False):
            recovery_needed = True
            best_backup = self.find_best_backup("training_history")
            if best_backup:
                if not self.restore_from_backup(self.history_file, best_backup):
                    recovery_success = False
            else:
                self.log_operation("è‡ªåŠ¨æ¢å¤", "æœªæ‰¾åˆ°å†å²æ•°æ®å¤‡ä»½", False)
                recovery_success = False
        
        if not recovery_needed:
            self.log_operation("è‡ªåŠ¨æ¢å¤", "æ‰€æœ‰æ–‡ä»¶çŠ¶æ€æ­£å¸¸ï¼Œæ— éœ€æ¢å¤")
        else:
            result_msg = "è‡ªåŠ¨æ¢å¤å®Œæˆ" if recovery_success else "è‡ªåŠ¨æ¢å¤éƒ¨åˆ†å¤±è´¥"
            self.log_operation("è‡ªåŠ¨æ¢å¤", result_msg, recovery_success)
        
        return recovery_success
    
    def merge_backups(self) -> bool:
        """åˆå¹¶å¤šä¸ªå¤‡ä»½æ–‡ä»¶ï¼Œåˆ›å»ºæœ€å®Œæ•´çš„æ•°æ®é›†"""
        logger.info("ğŸ”„ å¼€å§‹åˆå¹¶å¤‡ä»½æ–‡ä»¶...")
        
        # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®å¤‡ä»½
        training_backups = []
        for backup_file in self.backup_dir.glob("*manual_training_data*.json"):
            valid, _ = self.verify_json_integrity(backup_file)
            if valid:
                training_backups.append(backup_file)
        
        if len(training_backups) < 2:
            self.log_operation("åˆå¹¶å¤‡ä»½", "å¯ç”¨å¤‡ä»½å°‘äº2ä¸ªï¼Œæ— éœ€åˆå¹¶")
            return True
        
        try:
            # åˆå¹¶æ‰€æœ‰è®­ç»ƒæ•°æ®
            merged_data = {
                "channels": {},
                "updated_at": datetime.now().isoformat(),
                "version": "2.0_merged",
                "merged_from": []
            }
            
            for backup_file in training_backups:
                try:
                    with open(backup_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    merged_data["merged_from"].append({
                        "file": backup_file.name,
                        "timestamp": data.get("updated_at", "unknown")
                    })
                    
                    # åˆå¹¶é¢‘é“æ•°æ®
                    for channel_id, channel_data in data.get("channels", {}).items():
                        if channel_id not in merged_data["channels"]:
                            merged_data["channels"][channel_id] = channel_data
                        else:
                            # åˆå¹¶æ ·æœ¬æ•°æ®ï¼Œå»é‡
                            existing_samples = merged_data["channels"][channel_id].get("samples", [])
                            new_samples = channel_data.get("samples", [])
                            
                            # ä½¿ç”¨å“ˆå¸Œå»é‡
                            existing_hashes = set()
                            for sample in existing_samples:
                                sample_hash = hashlib.sha256(f"{sample.get('original', '')}|{sample.get('tail', '')}".encode()).hexdigest()
                                existing_hashes.add(sample_hash)
                            
                            for sample in new_samples:
                                sample_hash = hashlib.sha256(f"{sample.get('original', '')}|{sample.get('tail', '')}".encode()).hexdigest()
                                if sample_hash not in existing_hashes:
                                    existing_samples.append(sample)
                                    existing_hashes.add(sample_hash)
                            
                            merged_data["channels"][channel_id]["samples"] = existing_samples
                            merged_data["channels"][channel_id]["sample_count"] = len(existing_samples)
                            
                except Exception as e:
                    logger.error(f"å¤„ç†å¤‡ä»½æ–‡ä»¶å¤±è´¥ {backup_file}: {e}")
            
            # ä¿å­˜åˆå¹¶ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            merged_file = self.backup_dir / f"merged_training_data_{timestamp}.json"
            
            with open(merged_file, 'w', encoding='utf-8') as f:
                json.dump(merged_data, f, ensure_ascii=False, indent=2)
            
            total_channels = len(merged_data["channels"])
            total_samples = sum(len(c.get("samples", [])) for c in merged_data["channels"].values())
            
            self.log_operation(
                "åˆå¹¶å¤‡ä»½",
                f"æˆåŠŸåˆå¹¶ {len(training_backups)} ä¸ªå¤‡ä»½ï¼ŒåŒ…å« {total_channels} ä¸ªé¢‘é“ï¼Œ{total_samples} ä¸ªæ ·æœ¬"
            )
            
            logger.info(f"åˆå¹¶ç»“æœå·²ä¿å­˜åˆ°: {merged_file}")
            return True
            
        except Exception as e:
            self.log_operation("åˆå¹¶å¤‡ä»½", f"å¤±è´¥: {e}", False)
            return False
    
    def emergency_recovery(self) -> bool:
        """ç´§æ€¥æ¢å¤æ¨¡å¼ï¼Œå°è¯•æ‰€æœ‰å¯èƒ½çš„æ¢å¤æ–¹æ³•"""
        logger.info("ğŸ†˜ å¯åŠ¨ç´§æ€¥æ¢å¤æ¨¡å¼...")
        
        recovery_success = True
        
        # æ­¥éª¤1: æ£€æŸ¥å®Œæ•´æ€§
        integrity_report = self.check_data_integrity()
        
        # æ­¥éª¤2: å°è¯•è‡ªåŠ¨æ¢å¤
        if not self.auto_recover():
            recovery_success = False
        
        # æ­¥éª¤3: åˆå¹¶å¤‡ä»½
        try:
            self.merge_backups()
        except Exception as e:
            logger.error(f"åˆå¹¶å¤‡ä»½å¤±è´¥: {e}")
            recovery_success = False
        
        # æ­¥éª¤4: åˆ›å»ºç´§æ€¥å¤‡ä»½
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        emergency_report = {
            "emergency_recovery": {
                "timestamp": timestamp,
                "integrity_report": integrity_report,
                "recovery_log": self.recovery_log,
                "success": recovery_success
            }
        }
        
        emergency_report_file = self.backup_dir / f"emergency_recovery_report_{timestamp}.json"
        try:
            with open(emergency_report_file, 'w', encoding='utf-8') as f:
                json.dump(emergency_report, f, ensure_ascii=False, indent=2)
            logger.info(f"ç´§æ€¥æ¢å¤æŠ¥å‘Šå·²ä¿å­˜åˆ°: {emergency_report_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜ç´§æ€¥æ¢å¤æŠ¥å‘Šå¤±è´¥: {e}")
        
        result_msg = "ç´§æ€¥æ¢å¤å®Œæˆ" if recovery_success else "ç´§æ€¥æ¢å¤é‡åˆ°é—®é¢˜"
        self.log_operation("ç´§æ€¥æ¢å¤", result_msg, recovery_success)
        
        return recovery_success
    
    def save_recovery_log(self):
        """ä¿å­˜æ¢å¤æ—¥å¿—"""
        if not self.recovery_log:
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.backup_dir / f"recovery_log_{timestamp}.json"
        
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "recovery_session": {
                        "timestamp": timestamp,
                        "operations": self.recovery_log
                    }
                }, f, ensure_ascii=False, indent=2)
            logger.info(f"æ¢å¤æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ¢å¤æ—¥å¿—å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è®­ç»ƒæ•°æ®æ¢å¤å·¥å…·")
    parser.add_argument("--check", action="store_true", help="æ£€æŸ¥æ•°æ®å®Œæ•´æ€§")
    parser.add_argument("--auto-recover", action="store_true", help="è‡ªåŠ¨æ¢å¤æŸåçš„æ–‡ä»¶")
    parser.add_argument("--restore", metavar="BACKUP_FILE", help="ä»æŒ‡å®šå¤‡ä»½æ–‡ä»¶æ¢å¤")
    parser.add_argument("--merge-backups", action="store_true", help="åˆå¹¶å¤šä¸ªå¤‡ä»½æ–‡ä»¶")
    parser.add_argument("--emergency", action="store_true", help="ç´§æ€¥æ¢å¤æ¨¡å¼")
    parser.add_argument("--target", choices=["training", "history", "both"], default="both",
                       help="æŒ‡å®šæ¢å¤ç›®æ ‡ç±»å‹")
    
    args = parser.parse_args()
    
    if not any([args.check, args.auto_recover, args.restore, args.merge_backups, args.emergency]):
        parser.print_help()
        return
    
    recovery = TrainingDataRecovery()
    
    try:
        if args.check:
            print("ğŸ“Š æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š")
            print("=" * 50)
            report = recovery.check_data_integrity()
            
            # æ‰“å°ä¸»è¦æ–‡ä»¶çŠ¶æ€
            for filename, info in report["main_files"].items():
                status = "âœ… å¥åº·" if info["valid"] else "âŒ æŸå" if info["exists"] else "âš ï¸ ä¸¢å¤±"
                print(f"{filename}: {status}")
                if info.get("content_summary"):
                    summary = info["content_summary"]
                    if summary.get("type") == "training_data":
                        print(f"  ğŸ“ é¢‘é“æ•°: {summary.get('channels_count', 0)}")
                        print(f"  ğŸ“ æ ·æœ¬æ•°: {summary.get('total_samples', 0)}")
                    elif summary.get("type") == "training_history":
                        print(f"  ğŸ“œ å†å²è®°å½•: {summary.get('history_count', 0)}")
                print()
            
            # æ‰“å°å¤‡ä»½ç»Ÿè®¡
            summary = report["summary"]
            print(f"ğŸ“¦ å¤‡ä»½æ–‡ä»¶ç»Ÿè®¡:")
            print(f"  æ€»æ•°: {summary['total_backups']}")
            print(f"  æœ‰æ•ˆ: {summary['valid_backups']}")
            print(f"  æ— æ•ˆ: {summary['total_backups'] - summary['valid_backups']}")
        
        elif args.auto_recover:
            success = recovery.auto_recover()
            if success:
                print("âœ… è‡ªåŠ¨æ¢å¤å®Œæˆ")
            else:
                print("âŒ è‡ªåŠ¨æ¢å¤å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
                sys.exit(1)
        
        elif args.restore:
            backup_file = Path("data/backups") / args.restore
            if not backup_file.exists():
                print(f"âŒ å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
                sys.exit(1)
            
            # ç¡®å®šç›®æ ‡æ–‡ä»¶
            targets = []
            if args.target in ["training", "both"]:
                targets.append((recovery.main_data_file, "è®­ç»ƒæ•°æ®"))
            if args.target in ["history", "both"]:
                targets.append((recovery.history_file, "å†å²æ•°æ®"))
            
            success = True
            for target_file, description in targets:
                if recovery.restore_from_backup(target_file, backup_file):
                    print(f"âœ… {description}æ¢å¤æˆåŠŸ")
                else:
                    print(f"âŒ {description}æ¢å¤å¤±è´¥")
                    success = False
            
            if not success:
                sys.exit(1)
        
        elif args.merge_backups:
            if recovery.merge_backups():
                print("âœ… å¤‡ä»½åˆå¹¶å®Œæˆ")
            else:
                print("âŒ å¤‡ä»½åˆå¹¶å¤±è´¥")
                sys.exit(1)
        
        elif args.emergency:
            print("ğŸ†˜ å¯åŠ¨ç´§æ€¥æ¢å¤æ¨¡å¼...")
            if recovery.emergency_recovery():
                print("âœ… ç´§æ€¥æ¢å¤å®Œæˆ")
            else:
                print("âš ï¸ ç´§æ€¥æ¢å¤å®Œæˆï¼Œä½†å‘ç°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
        
    except KeyboardInterrupt:
        print("\nâ¸ï¸ æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
    finally:
        # ä¿å­˜æ¢å¤æ—¥å¿—
        recovery.save_recovery_log()

if __name__ == "__main__":
    main()